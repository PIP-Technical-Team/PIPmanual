[["index.html", "PIP Manual Welcome! Team", " PIP Manual DECIS Poverty GP 2021-08-30 Welcome! These are the internal guidelines for the technical process of the PIP project. Visit the Github repository for this book. Team The PIP technical team is composed of . "],["intro.html", "Chapter 1 Introduction 1.1 Ojectives 1.2 Technical requirements", " Chapter 1 Introduction NOTE:Andres, finish chapter 1.1 Ojectives This book explains several things, An overview of the project from a technical perspective. The interaction between R packages developed to manage the data and do the calculations. The different types of data in the PIP project and the interaction between them. How the poverty calculator, the table maker, and the Statistics Online (SOL) platform are updated. Technical standalone procedure necessary for the execution of some parts of the project. 1.2 Technical requirements You need to make sure the bookdown package is installed in your computer install.packages(&quot;bookdown&quot;) # or the development version devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. Make sure to install the latest version of PIP R packages by typing the following ipkg &lt;- utils::installed.packages()[,1] pip_install &lt;- function(pkg, ipkg) { if (isFALSE(pkg %in% ipkg)) { gitcall &lt;- paste0(&quot;PIP-Technical-Team/&quot;, pkg) remotes::install_github(gitcall, dependencies = TRUE) TRUE } else { FALSE } } pkgs &lt;- c(&quot;pipload&quot;, &quot;pipaux&quot;, &quot;wbpip&quot;, &quot;piptb&quot;, &quot;pipdm&quot;, &quot;pipapi&quot;) purrr::walk(pkgs, pip_install, ipkg = ipkg) "],["internal-workflow.html", "Chapter 2 Internal Workflow 2.1 The Github group 2.2 Overview 2.3 Data acquisition 2.4 Data preparation 2.5 Pre-computed indicators 2.6 API feeding 2.7 Packages interaction", " Chapter 2 Internal Workflow This chapter explains the internal technical workflow of the PIP project. Many of the different steps of the workflow are explained in more detail in other chapters of this book. This chapter, however, presents an overview of the workflow and its components. 2.1 The Github group The technical code of the PIP project is organized in Git repositories in the Github group /PIP-Technical-Team. You need to be granted collaborator status in order to contribute to any of the repositories in the group. Also, many of the repositories do not play a direct role in the PIP technical workflow. Some of them are intended for documenting parts of the workflow or for testing purposes. For example, the repository of this book/PIPmanualis not part of the workflow of PIP, since it is not necessary for any estimation. Yet, you need get familiar with all the repositories in case you need to make a contribution in any of them. In this chapter, however, we will focus on understanding the repositories that affect directly the PIP workflow. First, we will see the overview of the workflow. It is an overview because each bucket of its buckets is a workflow in itself. Yet, it is important to have the overview clear in order to understand how and where all the pieces fall together. Then, we will unpack each of the workflow buckets to understand them in more detail. 2.2 Overview The workflow overview is mainly composed of four steps. Data acquisition Data preparation pre-computed indicators API feeding Each of the steps (or buckets) is prerequisite of the next one, so if something changes in one of the them, it is necessary to execute the subsequent steps. 2.3 Data acquisition Before understanding how the input data of PIP is acquired, we need to understand PIP data itself. The PIP is fed by two kinds of data: welfare data and auxiliary data. Welfare data refers to the data files that contain at least one welfare vector and one population expansion factor (i.e., weights) vector. These two variables are the minimum data necessary to estimate poverty and inequality measures.1 These files come in four varieties; microdata, group data, bin data, and synthetic data. The details of welfare can be found in Section 7. Regardless of the variety of the data, all welfare data in PIP are gathered from the Global Monitoring Database, GMD. For a comprehensive explanation of how the household data are selected and obtained, you may check chapter Acquiring household survey dataof the methodological PIP manual. Microdata is uploaded into the PRIMUS system by the regional teams of the Poverty Global Practice. To do so, each regional team has to follow the GMD guidelines, which are verified by the Stata command {primus}. Rarely. the {primus} command does NOT capture some potential errors in the data. More details in Section 6 As of now (August 30, 2021), Group data is divided two: historical group data and new group data. Historical group data is organized and provided by the PovcalNet team, who sends it to the Poverty GP to be included in the datalibweb system. New group data is collected by the poverty economist of the corresponding country, who shares it with her regional focal team, who shares it with PovcalNet team. This new group data is organized and tested by the PovcalNet team and then send back to the poverty GP to be included in the datalibweb system. Bin data refers to welfare data from countries for which there is no poverty economist. Most of these countries are developed countries such as Canada or Japan. The main characteristic of this data is that it is only available through the LISSY system of the LIS data center, which does not allow access to the entire microdata. Thus, we need to contract the microdata into 400 bins or quantiles. The code that gathers LIS data is available in the Github repository PovcalNet-Team/LIS_data. Finally, synthetic data, refers to simulated microdata from an statistical procedure. As of now (August 30, 2021), the these data are estimated using multiple imputation techniques, so that any calculation must take into account the imputation-id variable. The data is calculated by the poverty economist of the country and the organizes by the global team in the Poverty GP. Auxiliary data refers to all data necessary to temporally deflate and line up welfare data, with the objective of getting poverty estimates comparable over time, across countries, and, more importantly, being able to estimate regional and global estimates. Some of these data are national population, GDP, consumer price index, purchasing parity power, etc. Auxiliary data also include metadata such as time comparability or type of welfare aggregate. Since each measure of auxiliary data is acquired differently, all the details are explain in Section 8. 2.4 Data preparation This step assumes that all welfare data is properly organized in the datalibweb system and vetted in PRIMUS. In contrast to the previous global-poverty calculator system, PovcalNet, the PIP system only gathers welfare from the datalibweb server. The welfare data preparation is done using the repository /pipdp. As of now (August 30, 2021), this part of the process has been coded in Stata, given that the there is no an R version of {dataliweb}. As for the auxiliary data preparation, it is done with package {pipaux}, available in the repository /pipaux. Right now the automation of updating the auxiliary has not been implemented. Thus, it has to be done manually by typing pipaux::pip_update_all_aux() to update all measures, or use function pipaux::update_aux() to update a particular measure. 2.5 Pre-computed indicators All measures in PIP that do not depend on the value of the poverty line are pre-computed in order to make the API more efficient and responsive. Some other indicators that not depend on the poverty lines but do depend on other parameters, like the societal poverty, are not included as part of the pre-computed indicators. This step is executed in the repository /pip_ingestion_pipeline, which is a pipeline powered by the {targets} package. The process to estimate the pre-computed indicators is explained in detail in Chapter 9. The pipeline makes use of two R packages, {wbpip} and {pipdm}. The former is publicly available and contains all the technical and methodological procedures to estimate poverty and inequality measures at the country, regional, and global level. The latter, makes use of {wbpip} to execute the calculations and put the resulting data in order, ready to be ingested by the PIP API. 2.6 API feeding NOTE: Tony, please finish this section. 2.7 Packages interaction NOTE: Andres, finished this section. Population data is also necessary when working with group data. "],["folder-structure.html", "Chapter 3 Folder Structure 3.1 PIP-Data 3.2 PIP-Data_QA 3.3 PIP-Data_ExtSOL 3.4 PIP_Data_Testing 3.5 PIP_Data_Vintage 3.6 pip_ingestion_pipeline", " Chapter 3 Folder Structure The PIP root directory (Sys.getenv(\"PIP_ROOT_DIR\")) contains the following main folders. Folder Explanation PIP-Data N/A PIP-Data_QA QA directory for survey and AUX data PIP-Data_ExtSOL SOL PIP-PIP_Data_Testing Testing directory for pipaux and pipdp PIP-PIP_Data_Vintage N/A pip_ingestion_pipeline Pipeline directory Details on each folder and their subfolders are given below. 3.1 PIP-Data This folder is no longer in use. This was the original PIP_Data folder. It was used for testing and development of {pipdp} and {pipaux}, but has since been replaced by PIP-Data_QA and PIP_Data_Testing. 3.2 PIP-Data_QA This is the main output directory for the {pipdp} and {pipaux} packages. It contains all the survey and auxiliary data needed to run the Poverty Calculator and Table Maker pipelines. Note that the contents of this folder is for QA and production purposes. Please use the PIP_Data_Testing directory, or your own personal testing directory, if you intend to test code changes in {pipdp} or {pipaux}. 3.2.1 _aux The _aux folder contains the auxiliary data used by the Poverty Calculator Pipeline and the {pipdp} and {pipapi} packages. Please note the following: The file sna/NAS special_2021-01-14.xlsx is currently hardcoded in {pipaux}. If the contents of the file changes this package might need to be updated. Some other National Accounts special cases (e.g. BLZ, VEN) are manually hardcoded in pipaux. Beware of this when updating GDP/PCE. The file weo/weo_&lt;YYYY-MM-DD&gt;.xls needs to be manually downloaded from IMF, opened and then re-saved as weo_&lt;YYYY-MM-DD&gt;.xls. The grouped data means currently come from the PovcalNet Masterfile. This should be changed when PovcalNet goes out of production. An explanation of each subfolder is given below. Folder Measure Usage Source countries PIP country list pipapi pipaux country_list WDI country list pipaux, PC pipeline pipaux cp Country profiles pipai pipaux cpi CPI PC pipeline pipaux dlw DLW repository pipdp pipdp gdm Grouped data means PC pipeline pipaux gdp Gross Domestic Product PC pipeline pipaux maddison Maddison Project Data pipaux pipaux indicators Indicators master pipapi, PC pipeline Manual, pipaux pce Private consumption PC pipeline pipaux pfw Price Framework pipdp, PC pipeline pipaux pl Poverty Lines pipapi pipaux pop Population PC pipeline pipaux ppp Purchasing Power Parity PC pipeline pipaux regions Regions pipapi pipaux sna Special National Account cases pipaux Manual weo World Economic Outlook (GDP) pipaux IMF, pipaux The data in the folders countries, regions, pl, cp and indicators are loaded into the Poverty Calculator pipeline, but that they are not used for any calculations or modified in any way, when being parsed through the pipeline. They are thus only listed with {pipapi} as their usage. In contrast the measures CPI, GDP, PCE, POP and PPP are both used in the pre-calculations and transformed before being saved as pipeline outputs. So even though these measures are also available in the PIP PC API, the files at this stage only have the Poverty Calculator pipeline as their use case. 3.2.2 _inventory The _inventory folder contains the PIP inventory file created by pipload::pip_update_inventory(). It is important to update this file if the survey data has been updated. 3.2.3 Country data The folders with country codes as names, e.g AGO, contain survey data for each country. This is created by {pipdp}. The folder structure within each country folder follows roughly the same convention as used by DLW. There will be one data file for each survey, based on DLWs GMD module and the grouped data in the PovcalNet-drive. These are labelled with PC in their filenames. Additionally, if there is survey data available from DLWs ALL module there will also be a dataset for the Table Maker, labelled with TB. 3.3 PIP-Data_ExtSOL This folder is used for SOL. Minh: Could you provide details? 3.4 PIP_Data_Testing This folder is used as a testing directory for development of the {pipdp} and {pipaux} packges. 3.5 PIP_Data_Vintage This folder is currently not in use. 3.6 pip_ingestion_pipeline This is the main output directory for the both the Poverty Calculator and Table Maker pipelines. 3.6.1 pc_data Folder Explanation _targets Targets storage cache Cleaned survey data output PC pipline output directory validation Validation directory 3.6.1.1 _targets This folder contains the objects that are cached by {targets} when the Poverty Calculator Pipeline is run. It is located on the shared network drive so that the latest cached objects are available to all team members. Please note however that a shared _targets folder does entail some risks. It is very important that only one person runs the pipeline at a time. Concurrent runs against the same _targets store will wreak havoc. And needless to say; if you are developing new features or customising the pipeline use a different _targets store. (We should probably have a custom DEV or testing folder for this, similar to PIP_Data_Testing.) 3.6.1.2 cache The survey data output of {pipdp} cannot be directly used as input to the Poverty Calculator Pipeline. This is because the data needs to be cleaned before running the necessary pre-calculations for the PIP database. The cleaning currently includes removal of rows with missing welfare and weight values, standardising of grouped data, and restructuring of datasets with both income and consumption based welfare values in the same survey. Please refer to {wbpip} and {pipdm} for the latest cleaning procedures. In order to avoid doing this cleaning every time the pipeline runs an intermediate version of the survey data is stored in the cache/clean_survey_data/ folder. Please make sure this folder is up-to-date before running the _targets.R pipeline. Note: This is not exactly true. We need to solve the issue with the double caching of survey data. But the essence of what we want to accomplish is to avoid cleaning and checking the survey data in every pipeline run.. 3.6.1.3 output This folder contains the outputs of the Poverty Calculator Pipeline. This is seperated into three subfolders; aux containing the auxiliary data, estimations containing the survey, interpolated and distributional pre-calculated estimates, and survey_data containing the cleaned survey data. 3.6.1.4 validation TBD Andres: Could you write this section?. 3.6.2 tb_data TBD Andres: Could you write this section?. "],["joining-data.html", "Chapter 4 Joining Data 4.1 The Price FrameWork (pfw) data 4.2 Special data cases", " Chapter 4 Joining Data Since the PIP project is comprised of several databases at very different domain levels, this chapter provides all the guidelines to join data frames correctly. The first thing to understand is that the reporting measures in PIP (e.g., poverty and inequality) are uniquely identified by four variables: Country, year, domain, and welfare type. Country refers to independent economies that conduct independent household surveys. For instance, China and Taiwan are treated as two different economies by the World Bank, and hence by PIP, even though under some criteria some people think that Taiwan is part of China. Year refers to reporting year rather than the actual calendar years over which the survey was conducted. Some household surveys like India 2011/2012 are conducted over the of two calendar year, but the welfare aggregate is deflated to the reporting year, 2011. Domain refers to the smallest geographical disaggregation for which it is possible to deflate and line up to PPP values the welfare aggregate of a household survey. The criteria to determine the reporting domain of a household survey is still under consideration, but ideally it such for which there is CPI, PPP, and population auxiliary data, as well as a household survey representative at that level. There are some exceptions to this criterion like China or the Philippines, but this cases explained in detailed in Section 4.2. As of today (August 30, 2021), most country/years are reported at the national domain and few are reported at the urban/rural domain. However, the PIP technical infrastructure has been designed to incorporate other domain levels if, at some point in time, it is the case. Finally, the welfare type specifies whether the welfare aggregate is based on income or in consumption. For the latter case, though some households surveys capture expenditure instead, they are still considered consumption-based surveys. The challenge of joining different data frames in PIP is that these four variables that uniquely identify the reporting measures are not available on any of the PIP data fileswith the exception of the cache files that we discuss below. This challenge is easily addressed by having a clear understanding of the Price FrameWork (pfw) data frame. This file does not only contain valuable metadata, but it could also be considered as the anchor among all PIP data. 4.1 The Price FrameWork (pfw) data As always, this file can be loaded by typing, library(data.table) pfw &lt;- pipload::pip_load_aux(&quot;pfw&quot;) joyn::is_id(pfw, by = c(&quot;country_code&quot;, &quot;surveyid_year&quot;, &quot;survey_acronym&quot;)) #&gt; copies n percent #&gt; 1: 1 1968 100% #&gt; 2: total 1968 100% #&gt; [1] TRUE First of all, notice that pfw is uniquely identified by country code, survey year, and survey acronym. The reason for this is that pfw aims at providing a link between every single household survey and all the other auxiliary data. Since welfare data is stored following the naming convention of the International Household Survey Network (IHSN), data is stored according to country, survey year, acronym of the survey, and vintage control of master and alternative versions. The vintage control of the master and alternative version of the data is not relevant for joining data because PIP uses, by default, the most recent version. Keep in mind that PIP estimates are reported at the country, year, domain, and welfare type level, but the last two of these are not found either in the survey ID nor as unique identifiers of the pfw. To solve this problem, the pfw data makes use of the variables welfare_type, aaa_domain, and aaa_domain_var. As the name suggests, welfare_type indicates the main welfare aggregate type (i.e, income or consumption ) of the variable welfare in the GMD datasets that correspond to the survey ID formed by concatenating variables country_code, surveyid_year, and survey_acronym. For example, the welfare_type of the welfare variable in the datasets of COL_2018_GEIH_V01_M_V03_A_GMD is income. pfw[ country_code == &quot;COL&quot; &amp; surveyid_year == 2018 &amp; survey_acronym == &quot;GEIH&quot;, # Not necessary since it is the only one unique(welfare_type)] #&gt; [1] &quot;income&quot; The prefix aaa in variables aaa_domain and aaa_domain_var refers to the identification code of any of the auxiliary data. Thus, you will find a gdp_domain, cpi_domain, ppp_domain and several others. All aaa_domain variables contain the lower level of geographical disaggregation of the corresponding aaa auxiliary data. There are only three possible levels of disaagregation, domain value meaning 1 national 2 urban/rural 3 subnational As of now, no survey or auxiliary data is broken down at level 3 (i.e., subnational), but it is important to know that the PIP internal code takes that possibility into account for future cases. Depending on the country, the domain level of each auxiliary data might be different. In Indonesia, for instance, the CPI domain is national, whereas the PPP domain is urban/rural. pfw[ country_code == &quot;IDN&quot; &amp; surveyid_year == 2018, .(cpi = unique(cpi_domain), ppp = unique(ppp_domain))] #&gt; cpi ppp #&gt; 1: 1 2 Finally, and this is really important, variables aaa_domain_var contains the name of variable in the GMD dataset that uniquely identify the household survey in the corresponding aaa auxiliary data. In other words, aaa_domain_var contains the name of the variable in GMD that must be used as key to join GMD to aaa. You may ask, does the name of the variable in the aaa auxiliary data have the same variable name in the GMD data specified in aaa_domain_var? No, it does not. Since the domain level to identify observations in the aaa auxiliary data is unique, there is only one variable in auxiliary data used to merge any welfare data, aaa_data_level. Since all this process is a little cumbersome, the {pipdp} Stata package, during the process of cleaning GMD databases to PIP databases, creates as many aaa_data_level variables as needed in order to make the join of welfare data and auxiliary data simpler. You can see the lines of code that create these variables in this section of the file pipdp_md_clean.ado.2 4.1.1 Joining data example Lets see the case of Indonesia above. The pfw says that the CPI domain is national and the PPP domain is urban/rural. That means that the welfare data join to each of these auxiliary data with two different variables, domains &lt;- pfw[ country_code == &quot;IDN&quot; &amp; surveyid_year == 2018, .(cpi = unique(cpi_domain_var), ppp = unique(ppp_domain_var))][] This says that the name of the variable in the welfare data to join PPP data is called uban, but there is not seem to be a variable name in GMD to join the CPI data. When the name of the variable is missing, it indicates that the welfare data is not split by any variable to merge CPI data. That is, it is at the national level. ccode &lt;- &quot;CHN&quot; cpi &lt;- pipload::pip_load_aux(&quot;cpi&quot;) ppp &lt;- pipload::pip_load_aux(&quot;ppp&quot;) CHN &lt;- pipload::pip_load_data(country = ccode, year = 2015) dt &lt;- joyn::merge(CHN, cpi, by = c(&quot;country_code&quot;, &quot;survey_year&quot;, &quot;survey_acronym&quot;, &quot;cpi_data_level&quot;), match_type = &quot;m:1&quot;, keep = &quot;left&quot;) #&gt; report n percent #&gt; 1: x &amp; y 40 100% #&gt; 2: total 40 100% 4.2 Special data cases NOTE: Andres. Add this section You can find more information about the conversion from GMD to PIP databases in Section 7 "],["pfw-chapter.html", "Chapter 5 Price FrameWork (pfw) data frame 5.1 Variables", " Chapter 5 Price FrameWork (pfw) data frame NOTE:Minh: Could you please add (or ask someone to add) a short explanation for each variable? Just follow the same line of explanations as the ones already done. The price framework data frame (pfw) is the most important source of technical metadata of the PIP project, which makes pertinent the have of a separate chapter for it. The general explanation of the structure of the pfw can be found in Section 4.1. This chapter focuses more on the use of each of the variables. Yet, it is worh repeating a few things already mentioned in Section 4.1. pfw &lt;- pipload::pip_load_aux(&quot;pfw&quot;) 5.1 Variables blah blah ID vars Variables of identification are wb_region_code, pcn_region_code, country_code, ctryname, year, surveyid_year, survey_acronym. The main difference between wb_region_code and pcn_region_code is that the former only include geographical regions internally used by the world bank, whereas the latter has an additional category, OHI, for Other High Income countries. janitor::tabyl(pfw, wb_region_code, pcn_region_code) #&gt; wb_region_code EAP ECA LAC MNA OHI SAS SSA #&gt; EAP 144 0 0 0 31 0 0 #&gt; ECA 0 583 0 0 389 0 0 #&gt; LAC 0 0 432 0 0 0 0 #&gt; MNA 0 0 0 65 26 0 0 #&gt; NAC 0 0 0 0 49 0 0 #&gt; SAR 0 0 0 0 0 48 0 #&gt; SSA 0 0 0 0 0 0 201 The difference between year and surveyid_year is that Minh, could you please explain here? altname Alternative survey name for some surveys. head(pfw[altname != &quot;&quot;, c(&quot;altname&quot;, &quot;survey_acronym&quot;)]) #&gt; altname survey_acronym #&gt; 1: FSM_2000_PHC_v01_M CPH #&gt; 2: IDN_2002_SUSENAS-FEB_v01_M SUSENAS #&gt; 3: IDN_2003_SUSENAS-FEB_v01_M SUSENAS #&gt; 4: IDN_2004_SUSENAS-FEB_v01_M SUSENAS #&gt; 5: IDN_2005_SUSENAS-FEB_v01_M SUSENAS #&gt; 6: IDN_2006_SUSENAS-FEB_v01_M SUSENAS wbint_link blah blah wbext_link blah blah alt_link blah blah surv_title blah blah surv_producer blah blah survey_coverage This variable represent the househol survey coverage. This is different from the disaggregation data level. janitor::tabyl(pfw, survey_coverage) #&gt; survey_coverage n percent #&gt; national 1916 0.973577 #&gt; rural 1 0.000508 #&gt; urban 51 0.025915 welfare_type This variable contains the welfare type of the main welfare aggregate variable in a survey in case it has more than one. The welfare type of alternative welfare aggregates is found in variable oth_welfare1_type. janitor::tabyl(pfw, welfare_type) #&gt; welfare_type n percent #&gt; consumption 807 0.41 #&gt; income 1161 0.59 use_imputed Whether the welfare aggregate has been imputed. There are just few countries with this kind of data. pfw[use_imputed == 1, unique(country_code)] #&gt; [1] &quot;SOM&quot; &quot;SSD&quot; &quot;ZWE&quot; use_microdata Whether the welfare aggregate vector used in PIP is directly extracted from microdata without any sort of aggregation. Mos of the countries have this kind of data. Below you can see those that do not. pfw[use_microdata != 1, unique(country_code)] #&gt; [1] &quot;AUS&quot; &quot;CHN&quot; &quot;IDN&quot; &quot;JPN&quot; &quot;KOR&quot; &quot;MNG&quot; &quot;MYS&quot; &quot;PHL&quot; &quot;THA&quot; &quot;TWN&quot; &quot;ARM&quot; &quot;AUT&quot; &quot;BEL&quot; &quot;BGR&quot; &quot;BLR&quot; &quot;CHE&quot; #&gt; [17] &quot;CZE&quot; &quot;DEU&quot; &quot;DNK&quot; &quot;ESP&quot; &quot;EST&quot; &quot;FIN&quot; &quot;FRA&quot; &quot;GBR&quot; &quot;GEO&quot; &quot;GRC&quot; &quot;HRV&quot; &quot;HUN&quot; &quot;IRL&quot; &quot;ITA&quot; &quot;KAZ&quot; &quot;LTU&quot; #&gt; [33] &quot;LUX&quot; &quot;LVA&quot; &quot;MKD&quot; &quot;NLD&quot; &quot;NOR&quot; &quot;POL&quot; &quot;ROU&quot; &quot;RUS&quot; &quot;SVK&quot; &quot;SVN&quot; &quot;SWE&quot; &quot;TUR&quot; &quot;UKR&quot; &quot;ARG&quot; &quot;BOL&quot; &quot;COL&quot; #&gt; [49] &quot;CRI&quot; &quot;DOM&quot; &quot;ECU&quot; &quot;GTM&quot; &quot;GUY&quot; &quot;HND&quot; &quot;JAM&quot; &quot;LCA&quot; &quot;PAN&quot; &quot;SLV&quot; &quot;TTO&quot; &quot;URY&quot; &quot;VEN&quot; &quot;ARE&quot; &quot;DZA&quot; &quot;EGY&quot; #&gt; [65] &quot;IRN&quot; &quot;ISR&quot; &quot;JOR&quot; &quot;MAR&quot; &quot;TUN&quot; &quot;BGD&quot; &quot;IND&quot; &quot;LKA&quot; &quot;NPL&quot; &quot;BDI&quot; &quot;BWA&quot; &quot;CAF&quot; &quot;ETH&quot; &quot;GHA&quot; &quot;GIN&quot; &quot;GNB&quot; #&gt; [81] &quot;KEN&quot; &quot;LSO&quot; &quot;MDG&quot; &quot;MRT&quot; &quot;NAM&quot; &quot;NER&quot; &quot;NGA&quot; &quot;RWA&quot; &quot;SEN&quot; &quot;SLE&quot; &quot;SWZ&quot; &quot;UGA&quot; &quot;ZAF&quot; &quot;ZMB&quot; &quot;ZWE&quot; &quot;CAN&quot; #&gt; [97] &quot;USA&quot; use_bin Whether the welfare aggregate was aggregated to 400 bins from microdata before being incorporated to the PIP repository. This is the case of houshold surveys only available in the Luxembourg Data Center (LIS). Countries with bin data is considered micro data for technical purposes. pfw[use_bin == 1, unique(country_code)] #&gt; [1] &quot;AUS&quot; &quot;JPN&quot; &quot;KOR&quot; &quot;TWN&quot; &quot;AUT&quot; &quot;BEL&quot; &quot;CHE&quot; &quot;CZE&quot; &quot;DEU&quot; &quot;DNK&quot; &quot;ESP&quot; &quot;FIN&quot; &quot;FRA&quot; &quot;GBR&quot; &quot;GRC&quot; &quot;HUN&quot; #&gt; [17] &quot;IRL&quot; &quot;ITA&quot; &quot;LUX&quot; &quot;NLD&quot; &quot;NOR&quot; &quot;POL&quot; &quot;ROU&quot; &quot;SVK&quot; &quot;SVN&quot; &quot;SWE&quot; &quot;ISR&quot; &quot;CAN&quot; &quot;USA&quot; use_groupdata Whether welfare aggregate comes from grouped data. Information about this type of data is available in (Datt 1998; Krause 2013; Villaseñor and Arnold 1989). The following countries have this kind of data. pfw[use_groupdata == 1, unique(country_code)] #&gt; [1] &quot;CHN&quot; &quot;IDN&quot; &quot;MNG&quot; &quot;MYS&quot; &quot;PHL&quot; &quot;THA&quot; &quot;ARM&quot; &quot;BGR&quot; &quot;BLR&quot; &quot;CZE&quot; &quot;EST&quot; &quot;GEO&quot; &quot;HRV&quot; &quot;HUN&quot; &quot;KAZ&quot; &quot;LTU&quot; #&gt; [17] &quot;LVA&quot; &quot;MKD&quot; &quot;POL&quot; &quot;ROU&quot; &quot;RUS&quot; &quot;SVN&quot; &quot;TUR&quot; &quot;UKR&quot; &quot;ARG&quot; &quot;BOL&quot; &quot;COL&quot; &quot;CRI&quot; &quot;DOM&quot; &quot;ECU&quot; &quot;GTM&quot; &quot;GUY&quot; #&gt; [33] &quot;HND&quot; &quot;JAM&quot; &quot;LCA&quot; &quot;PAN&quot; &quot;SLV&quot; &quot;TTO&quot; &quot;URY&quot; &quot;VEN&quot; &quot;ARE&quot; &quot;DZA&quot; &quot;EGY&quot; &quot;IRN&quot; &quot;JOR&quot; &quot;MAR&quot; &quot;TUN&quot; &quot;BGD&quot; #&gt; [49] &quot;IND&quot; &quot;LKA&quot; &quot;NPL&quot; &quot;BDI&quot; &quot;BWA&quot; &quot;CAF&quot; &quot;ETH&quot; &quot;GHA&quot; &quot;GIN&quot; &quot;GNB&quot; &quot;KEN&quot; &quot;LSO&quot; &quot;MDG&quot; &quot;MRT&quot; &quot;NAM&quot; &quot;NER&quot; #&gt; [65] &quot;NGA&quot; &quot;RWA&quot; &quot;SEN&quot; &quot;SLE&quot; &quot;SWZ&quot; &quot;UGA&quot; &quot;ZAF&quot; &quot;ZMB&quot; &quot;ZWE&quot; reporting_year blah blah survey_comparability blah blah comp_note blah blah preferable blah blah show_portal blah blah fieldwork_range blah blah survey_year blah blah newref blah blah ref_year_des blah blah wf_baseprice blah blah wf_baseprice_note blah blah wf_baseprice_des blah blah wf_spatial_des blah blah wf_spatial_var blah blah cpi_replication blah blah cpi_domain blah blah cpi_domain_var blah blah wf_currency_des blah blah ppp_replication blah blah ppp_domain blah blah ppp_domain_var blah blah wf_add_temp_des blah blah wf_add_temp_var blah blah wf_add_spatial_des blah blah wf_add_spatial_var blah blah tosplit blah blah tosplit_var blah blah inpovcal blah blah oth_welfare1_type blah blah oth_welfare1_var blah blah gdp_domain blah blah pce_domain blah blah pop_domain blah blah Note blah blah pfw_id blah blah References "],["primus.html", "Chapter 6 PRIMUS 6.1 Interacting with PRIMUS 6.2 Understanding PRIMUS 6.3 Checking PRIMUS estimates 6.4 Confirming and approving data in PRIMUS", " Chapter 6 PRIMUS According to the description in the Stata repository worldbank/primus, The PRIMUS system is designed to facilitate this process of generating internal estimates of the World Banks poverty indicators and reduce the time needed for resolving discrepancies. It is a workflow management platform for the submission, review and approval of poverty estimates and a tracking facility to capture the inputs and results of the estimation process for future reference and audits. As such, PRIMUS is the platform used by the PovcalNet team to approve the adoption of new survey data into the PovcalNet system. 6.1 Interacting with PRIMUS The interaction with PRIMUS is done through different systems, so it is best to begin by clarifying terms. Website platform PRIMUS can be accessed by typing primus/ in your browser. As long as youre connected to the intranet it should work fine. However, if you have any issues connecting to the platform, please send an email to Minh Cong Nguyen, requesting access. Each database uploaded into PRIMUS gets a unique transaction ID. This ID is important because it is not unique to a dataset but unique to the transaction (or vintage of the data). That is, if one particular dataset is uploaded more than once, it will get two different transaction IDs. When talking to the Poverty GP, you better refer to the transaction ID rather than the survey (or at least both) because, though you may be talking about the same country/year, you are actually talking about two different transactions. See for instance Brazil 2013. Stata command The Poverty GP maintains the Stata repository worldbank/primus from which you can download the command primus. Right now, this is the official place from which you can access this command. From now on, each time we refer to the command, we use primus, whereas when we refer to the website, we use PRIMUS. Please, make sure you have it properly installed in your computer, by following the instruction section ??. Basically, you need to install first the github Stata command by E. F. Haghish net install github, from(&quot;https://haghish.github.io/github/&quot;) Now, you can install primus by just typing the following in Stata github install worldbank/primus In case this does not work, follow instructions in section ?? for alternative methods. Corrections to primus Stata command The primus command is maintained by the Poverty GP, so we have no control over modifications or improvements. The best you can do in case you need to fix or modify something in this command is to fork the repository, clone the forked repo into your computer, check out a new branch, make any modification, and generate a pull request to the master branch of the original repository. Once you have done that, make sure to send an email with your suggestions for improvement to Ani Rudra Silwal, copying to the D4G Central Team (Nobuo Yoshida and Minh Cong Nguyen). 6.2 Understanding PRIMUS Each time a database is uploaded into PRIMUS, it is assigned a transaction ID. During the uploading process (or right after it has finished), the three partiesDECDG, DECRG, or the Poverty GPevaluate the quality of the new or corrected data and approve them or reject them in the system. Depending on the decision of all the parties, each transaction will take one of three possible status, pending, approved, or rejected. As of today (2020-11-20), there is no one who represents DECRG. So, the approving process might be different and it will need to be changed in the PRIMUS system. Please check. The transaction ID is pending when at least one of the three parties (DECDG, DECRG, or the Poverty GP) has not approved it in the system. You can click on the check box PENDING in the PRIMUS website to see which surveys have such a status, or you can use the primus command list this, qui primus query, overallstatus(PENDING) list transaction_id country year date_modified in 1/`=min(10, _N)&#39; +----------------------------------------------+ | transaction_id country year | |----------------------------------------------| 1. | TRN-000327173-EAP-IDN-QR48Q IDN 2017 | 2. | TRN-000327173-ECA-DEU-YJYVZ DEU 1995 | 3. | TRN-000327173-ECA-DEU-2P4DR DEU 2002 | 4. | TRN-000327173-ECA-DEU-LJN8R DEU 2003 | 5. | TRN-000327173-ECA-DEU-ZSN9J DEU 2005 | |----------------------------------------------| 6. | TRN-000327173-ECA-DEU-UBS7M DEU 2008 | 7. | TRN-000327173-ECA-DEU-41TOU DEU 2009 | 8. | TRN-000327173-EAP-AUS-KKZ2E AUS 2004 | +----------------------------------------------+ Notice that the overall status of a transaction is independent from survey ID. Thus, it is possible to find several transactions for the same country and year. Indonesia 2017, for instance, has three transactions, two of them rejected and one of them pending. qui primus query, country(IDN) year(2017) list transaction_id overall_status date_modified in 1/`=min(10, _N)&#39; +--------------------------------------------------+ | transaction_id date_modified | |--------------------------------------------------| 1. | TRN-000104674-EAP-IDN-8R9IF 23may2018 15:28:47 | 2. | TRN-000327173-EAP-IDN-TYA1A 23may2018 23:57:27 | 3. | TRN-000327173-EAP-IDN-QR48Q 24may2018 00:27:33 | +--------------------------------------------------+ A transaction is rejected when at least one of the three parties rejected the database. Finally, a transaction is approved only when all three parties have approved it into the system. We recommend you understand the basic functionality of the primus command by reading the help file (type help primus in Stata). 6.3 Checking PRIMUS estimates The real first step to check the quality of the recently uploaded data into PRIMUS is to download the basic estimates of each data and compare them with our own. There is no need to calculate and compare all the estimates available in PRIMUS but the mean in PPP, the poverty headcount, and the Gini index. The primus command allows us to download the estimates of each transaction, but it has to be done one by one. Fortunately, the pcn command downloads all the estimates of pending transactions for us and properly stores them in the folder p:\\01.PovcalNet\\03.QA\\02.PRIMUS\\pending\\   . You only need to type, pcn primus pending, down(estimates) In addition, pcn checks the date for which youre downloading the estimates and keeps only those transactions that have been uploaded for the next spring or annual-meetings release. For instance, assume that today, 2020-11-20, you want to see the estimates of pending transactions in PRIMUS. Since annual meetings take place around September, pcn assumes you are interested in the estimates for the Spring-meetings release, around March next year. Thus, it will filter the results from primus, keeping only those transactions that were uploaded from November 2020. Now it is likely that the PRIMUS system has not been opened for uploading new data in November, as it usually opens around December and July. Thus, it is likely that you will find and error saying There is no pending data in PRIMUS for the combination of country/years selected. You can load the recently-downloaded estimates by typing, pcn primus pending, load(estimates) Now, you have to check whether the new estimates make sense. Once way to that is to follow this do-file, p:\\01.PovcalNet\\03.QA\\02.PRIMUS\\pending\\2020_SM\\estimates\\checks\\comparisons_wrk_data.do. You do NOT need to check the estimates with the working data (wrk) as it is suggested in the do-file above. The PovcalNet System is now fully integrated with the datalibweb system, so the CPI, PPP, and microdata will be always the same. The best you can do at this stage is to make sure the estimates in PRIMUS make sense at the country level. 6.4 Confirming and approving data in PRIMUS Once you have checked that the estimates of pending transactions make sense, you need to approve them. As explained in section 6.2, the approval on PRIMUS requires the consent of three parties. The PovcalNet team had the responsibility to approve on behalf or two of them, DECDG and DECRG. This process can easily done with the code below, which can be found in this file, p:\\01.PovcalNet\\03.QA\\02.PRIMUS\\pending\\2020_SM\\approve\\primus_approve.do. /*================================================== 0: Program set up ==================================================*/ version 14 drop _all *---------- Modify this local excl = &quot;BRA SOM SSD&quot; // countries to exclude local excl = &quot;&quot; // countries to exclude /*================================================== Load data ==================================================*/ primus query, overalls(pending) //------------Cut off date local filtdate = &quot;2019-12-01&quot; // filter date (december last year) local filtdate = &quot;2020-02-18&quot; // filter date (surveys uploaded by Minh) keep if date_modified &gt;= clock(&quot;`filtdate&#39;&quot;, &quot;YMD&quot;) //------------Select username if (lower(&quot;`c(username)&#39;&quot;) == &quot;wb424681&quot;) { local dep = &quot;povcalnet&quot; } else if (lower(&quot;`c(username)&#39;&quot;) == &quot;wb384996&quot;) { local dep = &quot;decdg&quot; } else { noi disp in red &quot;you don&#39;t have rights to run this code&quot; break } tab `dep&#39; keep if `dep&#39; == &quot;PENDING&quot; if (&quot;`excl&#39;&quot; != &quot;&quot;) { local excl: subinstr local excl &quot; &quot; &quot;|&quot;, all drop if regexm(&quot;`country&#39;&quot;, &quot;`excl&#39;&quot;) } /*================================================= Approve (Do NOT modify) ==================================================*/ local n = _N preserve qui foreach i of numlist 1/`n&#39; { restore, preserve local country = country[`i&#39;] local year = year[`i&#39;] local id = transaction_id[`i&#39;] noi disp in y &quot;primus action, tranxid(`id&#39;) decision(approved)&quot; cap noi primus action, tranxid(`id&#39;) decision(approved) if (_rc) noi disp &quot;problem with `id&#39;&quot; } Basically, this is what you need to do with this file. Modify local excl in case you do not want to approve one or several countries. Modify local filtdate in which you select the date from which you want to approve transactions. Make sure at least two people approve. One on behalf of povcalnet (which is the alias used for DECRG) and another on behalf of decdg. PRIMUS has a double-confirmation process, so you need to confirm and then approve the transaction. For that, you only need to change the option decision() from approved to confirmed. For some unknown reason, the PRIMUS system did not accept the approval of some transactions. If this happens again, you need to talk to Minh Cong Nguyen, so he can do the approval manually. "],["welfare-data.html", "Chapter 7 Welfare data 7.1 From Raw to GMD 7.2 From GMD to PIP ({pipdp} package) 7.3 Survey ID nomenclature 7.4 Survey CACHE ID nomenclature", " Chapter 7 Welfare data Welfare data refers to any data file that contains the welfare vector that underlies the poverty and inequality measures in each country/year/welfare type. PIP makes use of four different data kinds to store the welfare vectors: microdata, bin data, group data, and synthetic data. Each of these is collected, formatted, and stored in a particular way. 7.1 From Raw to GMD 7.1.1 Microdata Minh: Could you please add an explanation here? 7.1.2 Group Data Minh: Could you please explain who gets the original group data and how it is formatted? We still need to develop the code to format original group data to PIP structure. Right now, we are still using Povcalnets code. 7.1.3 Bin Data Some of the countries that are not available in DataLibWeb can be found in the repository of the LIS Cross-National Data Center (hereafter, LIS). Currently, PovcalNet uses LIS data for 8 high-income economies: AUS, CAN, DEU, ISR, JPN, KOR, TWN &amp; USA, plus the Pre-EUSILC years (generally before 2002) of European Economies. LIS datasets cannot be downloaded in full; however, they provide a remote-execution system, LISSY, that allows us interact with their microdata without having access to the individual records. We have developed a set of Stata do-files to interact with LISSY and aggregate the welfare distribution of our countries of interest to 400 bins. Then, these data is organized locally and shared with the Poverty GP to be included in DataLibWeb as a collection independent from GPWG. The LIS_data repository In order to work with the LIS data you need to clone the repository PovcalNet-Team/LIS_data. You will find in there three folders, 00.LIS_output, 01.programs, and 02.data. Interacting with LISSY Opening an account in LISSY: To interact with the LIS data you need to first register here, by first completing the LIS microdata User Registration Form, and then submitting it through the same website using your institutional e-mail account. Within a couple of days, you will receive an e-mail from LIS containing your username and password. You do not get to choose your own username or password. LISdatacenter creates both for you and those wont change in time. Make sure to save that e-mail and record that information for your future log-ins. Also, know that LISSY passwords expire each year on December 31st. While your password wont change, it must be renewedhere after January 1st. Interacting with LISSY: To get acquainted with LISSYs interface, coding structure, database naming and variables available, and learn how to compute estimates within LISSY we highly recommend taking some time to review the tutorials and self-teaching materials. However, in order to update the 400 bins twice a year, Stata codes have been previously written, so you simply need to follow the 5 steps in the next section. Getting the 400 bins from LISSY 1. log in Go to LIS main page, scroll down and click on the lock icon 2. Provide info Feed the three drop-down menus on top of the platform with the following information: Project: LIS Package: Stata Subject: (Choose a name Ex: Bins #1 - Dec 2020) The LISSY platform cannot run the code for ALL surveys available at once. If you attempt to do so, your project will stop and you will receive an e-mail containing the text: ##################################################### Your job has been killed and will be not executed ##################################################### To avoid this, we need to run the code in groups of 5 to 6 countries, depending on the amount of years in each of them. Currently, LIS has data for 52 countries (26 of them are in the EUSILC project, and 26 do not) which usually take approximately 10 rounds of this process. 3. Add do-file Copy and paste the entire content of 01.LIS_400bins.do file into the the main large command window and update the locals in lines 23-24 with the LIS 2-letter acronyms of the group of 5-6 countries in each round. local silc &quot;at be cz&quot; local nosilc &quot;au br&quot; Remember to update the subject with each round (Ex: Bins #2 - Dec 2020) so you keep track of the number of output files. Be careful not to leave out [or repeat] any country in the process. 4 Submit. Click on the green arrow icon to submit your project. You will get an e-mail within some minutes with your output. If the system kills your project, your group of countries was probably too large. Remove one country and try again. 5. Retrieve results Copy the entire text in the output e-mail you receive for each round, open your notepad and paste. Save each round in the \\00.LIS_output folder. Save each text file with the name LISSY_Dec202@_#.txt, [where @ is the year and # the round]. Consistency with this naming format is important for the next step (02.LIS_organize_output.do file) From Text file to datalibweb structure We now need to convert the the text files generated by the LISSY system to actual data suitable for datalibweb. This structure is suggested by the International Household Survey Network (IHSN). Once the data is saved in folder 00.LIS_output you need to execute the file 02.LIS_organize_output.do. This file created to be executed in just one go. However, it could be ran in sections taking advantage of the different frames along the code. Before you execute this code, you need to ensure a few things, 1. Get rcall working in your computer The processing of the text files is not done anymore on Stata but in R. To avoid changing systems, we need to execute R code directly from Stata. In order to do this, you need to make sure to have install R in your computer and also the Stata command rcall. The do-file 02.LIS_organize_output.do will check if you have it installed and will install it for you in case it is not. However, you can run the lines below to make sure everything is working fine. Also, you can take a look at the help file of rcall to get familiar with it. cap which rcall if (_rc) { cap which github if (_rc) { net install github, from(&quot;https://haghish.github.io/github/&quot;) } github install haghish/rcall, stable } 2. Personal Drive Make sure to add your UPI to the appropriate sections it appears by typing disp lower(\"`c(username)'\") , following the example below, if (lower(&quot;`c(username)&#39;&quot;) == &quot;wbxxxxx&quot;) { local dir &quot;c:/Users/wbxxxxx/OneDrive - WBG/WorldBank/DECDG/PovcalNet Team/LIS_data&quot; } 3. Directives of the code This do-file works like an ado-file in the sense that the output depends on the value of some local macros, global update_surveynames = 1 // 1 to update survey names. global replace = 0 // 1 to replace data in memory even if it has not changed global p_drive_output_dir = 0 // 1 to use default Vintage_control folder If local update_surveynames is set to 1, the code will load the sheet LIS_survname from the the file 02.data/_aux/LIS datasets.xlsx and updated the file 02.data/_aux/LIS_survname.dta. If replace is set to 1, the code will replace any output with the same name. Otherwise, it will create a new vintage version if the two files are different. If they are not different, the code will do nothing. local p_drive_output_dir is deprecated, so you must leave it as 0. 4. Pattern of the text files When the text files with the information from LIS are stored in 00.LIS_output, they should be stored in a systematic way so that they could be loaded and processed at the same time. This can be done by specifying in a matching regular expression in local pattern. For instance, all the files downloaded in December, 2020 could by loaded and processed using the directive, local pattern = \"LISSY_Dec2020.*txt\". 5. Output When the do-file is concluded, it saves the file 02.data/create_dta_status.dta with the status of all the surveys processed. Compare new LIS data to Datalibweb inventory To identify what data is new and what data has changed with respect to the one available in datalibweb, you need to execute do-file 03.LIS_compare_dlw.do. Again, this do-file is intended to be executed in one run, but you can do it in parts taking advantage of the different frames. At the end of the execution the file 02.data/comparison_results.dta is created. This file contains three important variables wf, wt, and gn, which correspond to the ration of welfare means, weight means, and Gini coefficient between the data in datalibweb and the data in the folder, p:/01.PovcalNet/03.QA/06.LIS/03.Vintage_control. You should only send to the Poverty GP those surveys for which at least one of these three variables is different to 1. The Excel file LIS datasets.xlsx With each LIS data update performed, we must first identify from LIS the new surveys (countries and/or years) they had recently added. LIS send users e-mails informing about new datasets added, and also releases newsletters with this information. Inside the 02.data folder of your LIS_data GIT repository you will find a _aux sub folder, and the LIS datasets.xlsx file placed in there. We must manually update the tab LIS_survname tab adding new rows to the sheet. All necessary information to fill up this metadata (household size, currency, etc.) can be found in METIS. ACRONYMS: The column survey_acronym is created by us. If you come across a new survey for which an acronym has not been previously established, the rules applied in the past by the team were the following: Acronyms are created based on the ENGLISH name of the survey. (Ex: German Transfer Survey (Germany) is GTS, followed by the suffix -LIS; thus GTS-LIS. For the surveys that were Microcensus, we created the acronym MC, and for Denmarks Law Model, LM. All acronyms are created in capital letters. Finally, while the survey names in METIS are in English, some of the acronyms in parenthesis are still in the original language. In those cases we translated them to English. For instance, the survey name Household Budget Survey (BdF) (France) from METIS was changed to Household Budget Survey (HBS) (France) in the column surveyname of the excel. Prepare data for the Poverty GP Finally, the do-file 04.Append_new_LIS_bases.do prepares the data to be shared with the Poverty GP. Note that this do-file ONLY appends the 400 bins data of surveys that are new and those where welfare changed, which are identified in the previous step /comparison_results.dta as those gn != 1. Before running the code, make sure to change the output file name to the date of your update (last one saved was LIS_bins_Dec_21_2020.dta). The output is saved in P:\\01.PovcalNet\\03.QA\\06.LIS\\04.Share_with_GP. Finally, quickly prepare a short .dta file importing the metadata already created in the LIS_survname tab from the Excel, keeping ONLY the surveys of the append output you just run and send both files to Minh Cong Nguyen &lt;mnguyen3@worldbank.org&gt; from the Poverty GP. 7.1.4 Synthetic Data Minh: Could you please add an explanation here? 7.2 From GMD to PIP ({pipdp} package) The next step in the preperation of survey data for PIP is to leverage the {pipdp} package. This package retrives survey data from DLW and standardizes it into the format used by the PIP ingestion pipelines (Poverty Calculator Pipeline and Table Maker Pipeline). 7.2.1 Requirements {pipdp} depends on the following Stata packages. ssc install moremata ssc install missings You will also need to have the World Bank {datalibweb} module and the PovcalNet internal {pcn} command installed. 7.2.2 Installation # Clone repo git clone &quot;https://github.com/PIP-Technical-Team/pipdp&quot; # Edit profile.do notepad.exe C:\\ado\\personal\\profile.do # Add adopath ++ &quot;&lt;mypath&gt;\\pipdp&quot; 7.2.3 Remote server It is recommended to both use {pipdp} on the PovcalNet remote server (WBGMSDDG001). The phyiscal location of this server is much closer to the data storage so queries to Datalibweb, as well as read/write operations to the PIP shared network drive, will be much faster. The only issue with using the server is that Datalibweb will require login credentials for each new Stata session. To avoid this it is highly recommended that users apply for direct access to Datalibweb. This will grant access to the Datalibweb file storage and enable the possiblity of using the files option in all datalibweb queries. In order to use the files option you will also need to make some minor adjustments to your Datalibweb setup files. In particular the global root variable in C:/ado/personal/datalibweb/GMD.do needs to be specified correctly. This can be achived by copying datalibweb.ado and GMD.do in the _aux directory to their respective locations. Since WBGMSDDG001 is a shared server and the DLW settings thus can be reset, you will need to make sure that these settings are correct every time you use pipdp. The modified setup files will only change the behavior of Datalibweb for users that have direct access to the file storage. It will not affect other users. You could either check and copy the files manually or run the following from the command line. copy _aux/datalibweb.ado C:/ado/plus/d/datalibweb.ado copy _aux/GMD.do C:/ado/personal/datalibweb/GMD.do For details on how to connect to WBGMSDDG001 see the Remote server connection section in the PovcalNet Internal Guidelines and Protocols. 7.2.4 Usage pipdp has two main commands, pipdp group to copy and prepare grouped data files from the PocvalNet shared network drive and pipdp micro to download and prepare micro datasets from Datalibweb. For examples on how to use these commands see the Usage section in the {pipdp}README. 7.2.5 Preparing survey data for Poverty Calculator Pipeline Follow these steps when preparing survey data for the Poverty Calculator Pipeline Make sure the Price Framework file is up to date in the directory you are using. # Update PFW file PIP_DATA_DIR &lt;- pipload::create_globals(Sys.getenv(&quot;PIP_ROOT_DIR&quot;))$PIP_DATA_DIR # pipaux::pip_country_list(&#39;update&#39;, maindir = PIP_DATA_DIR) pipaux::pip_pfw(&#39;update&#39;, maindir = PIP_DATA_DIR) Make sure the Datalibweb repository file is up to date in directory you are using. Note that if you are running this code on the PovcalNet remote server you will be required to log in to DLW, regardless if you have direct access or not. This is because the underlying datalibweb, type(GMD) repo(create dlwrepo) command always sends queries over web. If you want you to avoid the login requirement you can conduct this specific step from your local machine. // Update DLW repo pipdp dlw maindir(&quot;$PIP_DATA_DIR&gt;&quot;) Make sure DLW setup files are up-to-date (needed for files option). ::Copy setup files copy _aux/datalibweb.ado C:/ado/plus/d/datalibweb.ado copy _aux/GMD.do C:/ado/personal/datalibweb/GMD.do Update surveys // Update grouped data surveys (from PCN-drive) pipdp group, countries(all) maindir(&quot;$PIP_DATA_DIR&gt;&quot;) // Download new surveys from DLW pipdp micro, countries(all) files new maindir(&quot;$PIP_DATA_DIR&gt;&quot;) Update the PIP inventory # Update PIP inventory PIP_DATA_DIR &lt;- pipload::create_globals(Sys.getenv(&quot;PIP_ROOT_DIR&quot;))$PIP_DATA_DIR pipload::pip_update_inventory(maindir = PIP_DATA_DIR) 7.3 Survey ID nomenclature All household surveys in the PIP repository are stored following the naming convention of the International Household Survey Network (IHSN) for archiving and managing data. This structure can be generalized as follows: CCC_YYYY_SSSS_ vNN_M_vNN_A_TYPE_MODULE where, CCC refers to 3 letter ISO country code YYYY refers to survey year when data collection started SSSS refers to survey acronym (e.g., LSMS, CWIQ, HBS, etc.) vNN is the version vintage of the raw/master data if followed by an M or of the alternative/adapted data if followed by an A. The first version will always be v01, and when a newer version is available it should be named sequentially (e.g. v02, v03, etc.). Note that when a new version is available, the previous ones must still be kept. TYPE refers to the collection name. In the case of PIP data, the type is precisely, PIP, but be aware that there are several other types in the datalibweb collection. For instance, the Global Monitoring Database uses GMD; the South Asia region uses SARMD; or the LAC region uses SEDLAC. MODULE refers to the module of the collection. This part of the survey ID is only available at the file level, not at the folder (i.e, survey) level. Since the folder structure is created at the survey level, there is no place in the survey ID to include the module of the survey. However, within a single survey, you may find different modules, which are specified in the name of the file. In the case of PIP, the module of the survey is divided in two: the PIP tool and the GMD module. The PIP tool could be PC, TB, or SOL (forthcoming), which stand for one of the PIP systems, Poverty Calculator, Table Baker (or Maker), and Statistics OnLine. the GMD module, refers to the original module in the GMD collection, such as module ALL, or GPWG, HIST, or BIN. For example, the most recent version of the harmonized Pakistani Household Survey of 2015, she would refer to the survey ID PAK_2015_PSLM_v01_M_v02_A_PIP. In this case, PSLM refers to the acronym of the Pakistan Social and Living Standards Measurement Survey. v01_M means that the version of the raw data has not changed since it was released and v02_A means that the most recent version of the alternative version is 02.2 7.4 Survey CACHE ID nomenclature NOTE:Andres, add explanation here Testing tachyons colors This is a test Royal blue color in line "],["auxiliary-data.html", "Chapter 8 Auxiliary data 8.1 Population 8.2 National Accounts 8.3 CPI 8.4 PPP 8.5 Price FrameWork (PFW) 8.6 Abbreviations", " Chapter 8 Auxiliary data As it was said in Section 2.3, auxiliary data is the data used to temporally deflate and line up welfare data, with the objective of getting poverty estimates comparable over time, across countries, and, more importantly, being able to estimate regional and global estimates. Yet, auxiliary data also refers to metadata with functional and qualitative information. Functional information is such that is used in internal calculations such as time comparability or surveys availability. Qualitative information is just useful information that does not affect, neither depend on, quantitative data. It is primary collected and made available for the end user. As explain in Chapter 3, all auxiliary data is stored in \"y:/PIP-Data/_aux/\". #&gt; [01;34my:/PIP-Data/_aux/[0m #&gt; +-- [01;34mcountry_list[0m #&gt; +-- [01;34mcpi[0m #&gt; +-- [01;34mdlw[0m #&gt; +-- [01;34mgdm[0m #&gt; +-- [01;34mgdp[0m #&gt; +-- [01;34mmaddison[0m #&gt; +-- [01;34mpce[0m #&gt; +-- [01;34mpfw[0m #&gt; +-- [01;34mpop[0m #&gt; +-- [01;34mppp[0m #&gt; +-- [01;34msna[0m #&gt; \\-- [01;34mweo[0m The naming convention of subfolders inside the _aux directory is useful because auxiliary data is commonly referred to in all technical processes by its convention rather than by it actual name. For instance Gross Domestic Product or Purchasing Power Parity are better known as gdp and ppp, respectively. Yet, other measures such as national population or consumption also make use of conventions. In this chapter you will learn everything related to each of the files that store auxiliary data. Notice that the chapter is structured by files rather than by measures or types of auxiliary data because you may find more than one measure in one file. The R package that manages auxiliary data is {pipaux}. As explained in Chapter 3, within the folder of each auxiliary file, you will find, at a minimum, a _vintage folder, one xxx.fst, one xxx.dta file, and one xxx_datasignature.txt , where xxx stands for the name of the file. 8.1 Population 8.1.1 Original data Everything related to population data should be placed in the folder y:\\PIP-Data\\_aux\\pop\\. hereafter (./). The population data come from one of two different sources. WDI or an internal file provided by a member of the DECDG team. Ideally, population data should be downloaded from WDI, but sometimes the most recent data available has not been uploaded yet, so it needs to be collected internally in DECDG. As of now (August 30, 2021), the DECDG focal point to provide the population data is Emi Suzuki. You just need to send her an email, and she will provide the data to you. If the data is provided by DECDG, it should be stored in the folder ./raw_data. The original excel file must be placed without modification in the folder ./raw_data/original. Then, the file is copied again one level up into the folder ./raw_data with the name population_country_yyyy-mm-dd.xlsx where yyyy-mm-dd refers to the official release date of the population data. Notice that for countries PSE, KWT and SXM, some years of population data are missing in the DECDG main file and hence in WDI. Here we complement the main file with an additional file shared by Emi to assure complete coverage. This file contains historical data and will not need to be updated every year. This additional file has the name convention population_missing_yyyy-mm-dd.xlsx and should follow the same process as the population_country file. Once all the files and their corresponding place, you can update the ./pop.fst file by typing pipaux::pip_pop_update(src = \"decdg\"). If the data comes directly from WDI, you just need to update the file ./pop.fst by typing pipaux::pip_pop_update(src = \"wdi\"). It is worth mentioning that the population codes used in WDI are SP.POP.TOTL, SP.RUR.TOTL, and SP.URB.TOTL, which are total population, rural population, and urban population, respectively. If it is the case that PIP begins using subnational population, a new set of WDI codes should be added to the R script in pipaux::pip_pop_update(). 8.1.2 Data structure Population data is loaded by typing either pipload::pip_load_aux(\"pop\") or pipaux::pip_pop(\"load\"). We highly recommend the former, as {pipload} is the intended R package for loading any PIP data. pop &lt;- pipload::pip_load_aux(&quot;pop&quot;) head(pop) #&gt; country_code year pop_data_level pop pop_domain #&gt; 1: ABW 1960 national 54211 national #&gt; 2: ABW 1960 rural 26685 urban/rural #&gt; 3: ABW 1960 urban 27526 urban/rural #&gt; 4: AFG 1960 national 8996973 national #&gt; 5: AFG 1960 rural 8241137 urban/rural #&gt; 6: AFG 1960 urban 755836 urban/rural 8.2 National Accounts National accounts account for the economic development of a country at an aggregate or macroeconomic level. These measure are thus useful to interpolate or extrapolate microeconomic measures mean welfare aggregate or poverty headcount when household surveys are not available. National accounts work as a proxy of the economic development that would have been present if household surveys were available. There are two main types of national accounts, Household Final Consumption Expenditure (HFCE) and Gross Domestic Product (GDP)both in real per capita terms. Please refer to Section 5.3 of (World Bank 2021) to understand the usage of national accounts data. 8.2.1 GDP As explained in Section 5.3 of (World Bank 2021), there are three sources of GDP data, and one more for a few particular cases. The integration of all the sources of GDP data is performed by pipaux::pip_gdp_update(), youll need to manually download and store the data from WEO and the data for the special cases. The national accounts series from WDI are GDP per capita  [series code: NY.GDP.PCAP.KD]. These series are in constant 2010 US$. The most recent version of the WEO data most be downloaded from the World Economic Outlook Databases of the IMF.org website and saved as an .xls file in &lt;maindir&gt;/_aux/weo/. The filename should be in the following structure WEO_&lt;YYYY-DD-MM&gt;.xls. Due to potential file corruption the file must be opened and re-saved before it can be updated with pip_gdp_weo(), which is an internal function fo pipaux::pip_gdp_update(). Hopefully in the future IMF will stop using an `.xls` file thats not really xls. 8.2.2 Consumption (PCE) Private Consumption Expenditure (pce) is gathered from WDI, with the exception of a few special cases. As in the case of GDP, the special cases are treated in the same way with PCE. You only need to execute the function pipaux::pip_pce_update() to update the PCE data. HFCE per capita [series code: NE.CON.PRVT.PC.KD] (Prydz et al. 2019). These series are in constant 2010 US$. 8.2.3 National Accounts, Special Cases Special national accounts are used for lining up poverty estimates in the following cases3: National accounts data are unavailable in the latest version of WDI. In such cases, national accounts data are obtained, in order of preference, from the latest version of WEO, or the latest version of MPD. For example, the entire series of GDP per capita for Taiwan, China and Somalia are missing in WDI, so WEO series are used instead. National accounts data are incomplete in the latest version of WDI. These are the cases where national accounts data are not available in WDI for some historical or recent years. In such cases, national accounts data in WDI are chained on backward or forward using growth rates from WEO or MPD, in that order. For example, GDP per capita for South Sudan (2016-2019) are based on the growth rate of GDP per capita from WEO. GDP per capita data for Liberia up to 1999 are based on the growth rate in GDP per capita from MPD. The available national accounts data from official sources (e.g. WDI, WEO, MPD) are considered to have quality issues. This is the case for Syria. Supplementary national accounts data are obtained from other sources, including research papers or national statistical offices. GDP per capita series for Syria (for 2010 through 2019) are from research papers---@kostialSyriaConflictEconomy2016Gobat (for 2011-2015) and Devadas, Elbadawi, and Loayza (2019) (for 2016-2019)and are chained on backward with growth rates in GDP per capita from WEO. See *y:/PIP-Data/_aux/sna/* for more details on how this is implemented. National accounts data need to be adjusted for the purposes of global poverty monitoring. This is the case for India. Growth rates in national accounts data for rural and urban India after 2014, precisely HFCE (or formerly PCE) per capita from WDI, are adjusted with a pass-through rate of 67%, as described in Section 5 of Castaneda Aguilar et al. (2020). See *y:/PIP-Data/_aux/sna/NAS special_2021-01-14.csv* for more details on how this is implemented. 8.3 CPI 8.3.1 Raw data Note: Minh to explain how the CPI data is collected and organized in the dlw folders. 8.3.2 Vintage control Vintage control of the CPI data comes in a similar fashion as welfare data, CPI_vXX_M_vXX_A, where vXX_M refers to the version of the master or raw data, and vXX_A refers to the alternative version. Every year, around November-December, PIP CPI data is updated with the most recent version of the IMF CPI data, which comes with information for the most recent year available and with changes/fixes/additions of previous years for each country. When this happens, the master version of the CPI ID is increased in one unit before the data is saved. As of today, the current ID is CPI_v05_M_v01_A. If data is modified during the rolling of the year, then the alternative version of the CPI ID is increased in one unit. 8.3.3 Data structure When you load CPI data using pipload::pip_load_aux(\"cpi\"), the data you get has already been cleaned for being use in the PIP workflow, and it is slightly different from the original CPI data stored in datalibweb servers. That is, the way CPI data is used and referred to datalibweb is different from the way it is used in PIP even though they both achive the same purpose. The most important variable in CPI data is, no surprisingly, cpi. This variable however, is not available in the original CPI data from dlw. The original name of this variable comes in the form cpiYYYY, where YYYY refers to the base year of the CPI, which in turn depends on the collection year of the PPP. Today, this variable is thus . The name of this variable is stored in the pipaux.cpivar object in the zzz.R file of the {pipaux} package. This will supdate the option getOption(\"pipaux.cpivar\"), guaranteing that pipaux::pip_cpi_update() uses the right variable when updating the CPI data. Another important variable in CPI dataframe is change_cpiYYYY, where YYYY stands for the base year of the CPI. Since it version control of the CPI data does not depend on the individual changes in the CPI series of each country but on the release of new data by the IMF or by additional modifications by the Poverty GP, variable change_cpiYYYY tracks changes in the CPI at the country/year/survey with respect to the previous version. This is very useful when you need to identify changes in output measures like poverty rates that depend on deflation. One possible source of difference is the CPI and this variable will help you identify whether the number of interest has change because the CPI has changed. 8.4 PPP 8.4.1 Original data Note: Minh to explain how the PPP data is collected and organized in the dlw folders. The name of the variables in the wide-format file will follow this structure, ppp_YYYY_vX_vY. Where, YYYY: refers to the ICP round. vX: refers to the version of the release. vY: refers to the adaptation of the release. So, v1 will be the original data, whereas v2 would be the first adaptation or estimates of the release. 8.4.2 Data structure PPP data is available by typing, pipload::pip_load_aux(\"ppp\"). As expected, the data you get has already been cleaned for being use in the PIP workflow, and it is slightly different from the original PPP data stored in datalibweb servers. The most important difference between the PIP data frame and the datalibweb data frame is its rectangular structure. PIP data is in long format, whereas datalibweb data in wide format. The reason for having PPP data in long format in PIP is that some countries, very few, use a different PPP year than the rest of the countries. Instead of using a different variable for the calculations of those specific countries, we use the same variable for all the countries but filter the corresponding observations for each country using metadata from the Price Framework database. The PPP data is at the country/ppp year/data_level/release version/adapation version level. Yet, several filters most always be applied before this data can be used. Ultimately, the data frame should be at the country/data_level level to used properly. As a general rule, the filter must be done by selecting the most recent release_version and the most recent adaptation_version in each year. Then you can just filter by the PPP year you want to work with. In order to make this process even easier we have created variables ppp_default and ppp_default_by_year, which dummy variables to filter data. If you keep all observations that ppp_default == 1 you will get the current PPP used for all PIP calculations. If you use ppp_default_by_year == 1, you get the default version used in each PPP year. This is useful in case you want to make comparisons between PPP releases. This two variables are created in function pipaux::pip_ppp_clean() , in particular in these lines. 8.5 Price FrameWork (PFW) blah 8.5.1 Original data asds 8.6 Abbreviations HFCE  final consumption expenditure MDP  Maddison Project Database PCE  private consumption expenditure WDI  World Development Indicators WEO  World Economic Outlook References "],["pcpipeline.html", "Chapter 9 Poverty Calculator Pipeline (pre-computed estimations) 9.1 Folder structure 9.2 Prerequisites 9.3 Structure of the _targets.R file 9.4 Understanding the pipeline 9.5 Understanding {pipdm} functions 9.6 Executing the _targets.R file 9.7 Debugging", " Chapter 9 Poverty Calculator Pipeline (pre-computed estimations) The Poverty Calculator Pipelinehereafter, and only for the rest of this chapter, pipelineis the technical procedure to calculate the pre-computed (or statitic) estimations of the PIP project. These estimations have two main purposes: To provide the user with instantaneous information about distributive measures of all the household surveys in the PIP repository that do not depend on the value of the poverty line (e.g. mean income, quantiles). Avoiding thus the need for re-computation as it was the case in PovcalNet for some of these measures. To provide the necessary inputs to the PIP API. This chapter walks you through the folder structure of the folder, the main R script, _targets.R, and the complete and partial execution of the script. Also, it provides some tips for debugging. 9.1 Folder structure The pipeline is hosted in the Github repository PIP-Technical-Team/pip_ingestion_pipeline. At the root of the repo you will find a series of files and folders. #&gt; +-- batch #&gt; +-- pip_ingestion_pipeline.Rproj #&gt; +-- R #&gt; +-- README.md #&gt; +-- renv #&gt; +-- renv.lock #&gt; +-- run.R #&gt; +-- _packages.R #&gt; \\-- _targets #&gt; \\-- _targets.R Folders R contains long R functions used during the pipeline batch is a script for timing the execution of the pipeline. This folder should probably be removed _targets is a folder for all objects created during the pipeline. You dont need to look inside as its content is managed by the targets package. renv is a folder for reproducible environment. Files _packages.R is created by targets::tar_renv(). Do not modify manually. _targets.R contains the pipeline. This is the most important file. 9.2 Prerequisites Before you start working on the pipeline, you need to make sure to have the following PIP packages. Note 1: notice that instructions below contain suffixes like @development. These specify the branch of the particular package that you need to use. Ideally, all packages should use the master branch; however, that will only be possible until the end of the development process. Note 2: if you update any of the packages developed by the PIP team, make sure you always increased the version of the package using the function usethis::use_version(). Even if the change in the package is small, you need on increased the version of the package. Otherwise, {targets} wont execute the sections of the pipeline that run the functions you changed. remotes::install_github(&quot;PIP-Technical-Team/pipdm@development&quot;) remotes::install_github(&quot;PIP-Technical-Team/pipload@development&quot;) remotes::install_github(&quot;PIP-Technical-Team/wbpip@halfmedian_spl&quot;) install.packages(&quot;joyn&quot;) In case renv is not working for you, you may need to install all the packages mentioned in the _packages.R script at the root of the folder. Also, make sure to install the most recent version of targets and tarchetypes packages. 9.3 Structure of the _targets.R file Even thought the pipeline script looks like a regular R script, it is structured in a specific way in order to make it work with the {targets} package. In fact, notice that it must be called _targets.R at the root of the project. It is highly recommended that you read the entire targets manual to fully understand how it works. We will often be referring to such manual in order to expand on any particular targets concept. Start up The first part of the pipeline sets up the environment. The process involve, loading the {targets} and {tarchetypes} packages; creating default values like directories, time stamps, survey and reference years boundaries, compression level of .fst files, etc.; executing tar_option_set() to set up an option in {targets}. packages and imports are two particularly important options to track changes in package dependencies. You can read more about it in the sections Loading and configuring R packages and Packages-based invalidation of the targets manual; attaching all the packages and functions of the project by running source('_packages.R') and source('R/_common.R'). Step 1: small functions According to the section Functions in pipelines of the targets manual, it is recommend to only use functions rather than expressions during the executions. Presumably, the reason for this is that targets track changes in functions but not in expressions. Thus, this scripts section defines small functions that are executed along the pipeline. In the section above, the scripts source('R/_common.R') loads longer functions. Yet, keep in mind that the 'R/_common.R' was used in a previous version of the pipeline before {targets} was implemented. Now, most of the function in 'R/_common.R' are included in the {pipdm} package. Step 2: preparing the data This section used to be longer in previous versions of the pipeline because it used to identify the auxiliary data, load the PIP microdata inventory, and create the cache files. It now only identifies the auxiliary data. Step 3: The actual pipeline Although better explained in the next section, the overall order of the pipeline is as follows: Load all necessary data (that is, auxiliary data and inventories), and then create any cache fie that has not been created yet. Calculate means in LCU Crate deflated survey mean (DSM) table Calculate reference year table (aka., interpolated means table) Calculate distributional stats Create output tables join survey mean table with dist table join reference year table with dist table coverage table aggregate population at the regional level table Clean and save. 9.4 Understanding the pipeline We must understand not only how the {targets} package works, but also how the targets of the Poverty Calculator Pipeline are created. For the former, you can read the targets manual. For the latter, we should start by making a distinction between the different types of targets. In {targets} terminology, there are two kinds of targets, stems and branches. Stems are unitary targets. That is, for each target there is only one single R object. Branches, on the other hand, are targets that contain several objects or subtargets inside (you can learn more about them in the chapter Dynamic branching of the targets manual). We will see the use of this type of targets when we talk about the use of cache files. Stem targets There are two ways to create stem targets: either using tar_target() or using tar_map() from the {tarchetypes} package. The tar_map() function allows to create stem targets iteratively. See for instance the creation of targets for each auxiliary data: tar_map( values = aux_tb, names = &quot;auxname&quot;, # create dynamic name tar_target( aux_dir, auxfiles, format = &quot;file&quot; ), tar_target( aux, pipload::pip_load_aux(file_to_load = aux_dir, apply_label = FALSE) ) ) tar_map() takes the values in the data frame aux_tb created in Step 2: preparing the data and creates two type of targets. First, it creates the target aux_dir that contains the paths of the auxiliary files, which are available in the column auxfiles of aux_tb. This is done by creating an internal target within tar_map() and using the argument format = \"file\". This process lets {targets} know that we will have objects that are loaded from a file and are not created inside the pc pipeline. Then, tar_map() uses the the column auxname of aux_tb to name the targets that contain the auxiliary files. Each target will be prefixed by the word aux. This is why we had to add the argument file_to_load to pipload::pip_load_aux, so we can let {targets} know that the file paths defined in target aux_dir are used to create the targets prefixed with aux, which are the actual targets. For example, if I need to use the population data frame inside the pc pipeline, Id use the target aux_pop, which had a corresponding file path in aux_dir. This way, if the original file referenced in aux_dir changes, all the targets that depend on aux_pop will be run again. Branches targets Lets think of a branch target like As explained above, branch targets are targets made of many subtargets that follow a particular pattern. Most of the targets created in the pc pipeline are branch targets because we need to execute the same procedure in every cache file. This could have been done internally in a single one, but then we would lose the tracking features of {targets}. Additionally, we could have created a stem target for every cache file, result, and output file, but that would have been both impossible to visualize and more difficult to code. Hence, branch targets is the best option. The following example illustrates how it works, # step A tar_target( cache_inventory_dir, cache_inventory_path(), format = &quot;file&quot; ), # step B tar_target( cache_inventory, { x &lt;- fst::read_fst(cache_inventory_dir, as.data.table = TRUE) }, ), # step C tar_target(cache_files, get_cache_files(cache_inventory)), # step D tar_files(cache_dir, cache_files), # step E tar_target(cache, fst::read_fst(path = cache_dir, as.data.table = TRUE), pattern = map(cache_dir), iteration = &quot;list&quot;) The code above illustrates several things. It is divided in steps, with the last step (step E) being the part of the code where the branch target is created. Yet, it is important to understand all the previous steps. In step A we create target cache_inventory_dir, which is merely the file path that contains the cache inventory. Notice that it is returned by a function and not entered directly into the target. Since it is a file path, we need to add the argument format = \"file\" to let {targets} know that it is input data. In step B we load the cache inventory file into target cache_inventory by providing the target path that we created in step A. This file has several columns. One of them contains the file path of every single cache file in the PIP network drive. That single column is extracted from the cache inventory in step C. Then, in step D, each file path is declared as input, using the convenient function tar_files(), creating thus a new target, cache_dir. Finally, we create branch target cache with all the cache files by loading each file. To do this iteratively, we parse the cache_dir target to the path argument of the function fst::read_fst() and to the pattern = map() argument of the tar_target() function. At the very end, we need to specify that the output of the iteration is stored as a list, using the argument iteration = \"list\". The basic logic of branch targets is that the vector or list to iterate through should be parsed to the functions argument and to the pattern = map() argument of the tar_target() function. It is very similar to purrr::map() Note: if we are iterating through more than one vector or list, you need to (1) separate each of them by commas in the map() part of the argument (See example code below). (2) make sure all the vectors or lists have the same length. This is why we cannot remove NULL or NA values from any target. (3) make sure you do NOT sort any of the output targets as it will loose its correspondence with other targets. # Example of creating branch target using several lists to iterate through. tar_target( name = dl_dist_stats, command = db_compute_dist_stats(dt = cache, mean = dl_mean, pop = aux_pop, cache_id = cache_ids), pattern = map(cache, dl_mean, cache_ids), iteration = &quot;list&quot; ) Creating the cache files The following code illustrates the creation of cache files: tar_target(pipeline_inventory, { x &lt;- pipdm::db_filter_inventory( dt = pip_inventory, pfw_table = aux_pfw) # Uncomment for specific countries # x &lt;- x[country_code == &#39;IDN&#39; &amp; surveyid_year == 2015] } ), tar_target(status_cache_files_creation, pipdm::create_cache_file( pipeline_inventory = pipeline_inventory, pip_data_dir = PIP_DATA_DIR, tool = &quot;PC&quot;, cache_svy_dir = CACHE_SVY_DIR, compress = FST_COMP_LVL, force = TRUE, verbose = FALSE, cpi_dt = aux_cpi, ppp_dt = aux_ppp) ) It is important to understand this part of the pc pipeline thoroughly because the cache files used to be created in Step 2: preparing the data rather than here. Now, it has not only been integrated in the pc pipeline, but it is also possible to execute the creation of cache files independently from the rest of the pipeline, by following the instructions in [Executing the _targets.R file]. The first target, pipeline_inventory is just the inner join of the pip inventory dataset and the price framework (pfw) file, to make sure we only include what the pfw says. This data set also contains a lot of useful information to create the cache files. Note that the commented line in this target would filter the pipeline inventory to have only the information for IDN, 2015. If you need to update specific cache files, you must add the proper filtering condition there. In the second target, status_cache_files_creation, you will create the cache files but notice that the returning value of the function pipdm::create_cache_file() is not the cache file per-se, but a list with the status of the creation process. If the creation of a particular file fails, it does not stop the iteration that creates all the cache files. At the end of the process, it returns a list with the creation status of each cache file. Notice that the function pipdm::create_cache_file() requires the CPI and the PPP auxiliary data. That is because the variable welfare_ppp, which is the welfare aggregate in 2011 PPP values, is added to the cache files. FInally, and more importantly, the argument force = TRUE ensures that even if the cache file already exists, it should be modified. This is important when you require additional features in the cache file from the then ones it currently has. If set to TRUE, it will replace any file in the network drive that is listed in pipeline_inventory. If set to FALSE, only the files that are in pipeline_inventory -but not in the cache folder- will be created. Use this option only when you need to add new features to all cache data, or when you are testing and only need a few surveys with the new features. 9.5 Understanding {pipdm} functions The {pipdm} package is the backbone of the pc pipeline. It is in charge of executing the functions in {wbpip} and consolidate the new DataBases. This is why many of the functions in {pipdm} are prefixed with db_. 9.5.1 Internal structure of {pipdm} functions The main objective of {pipdm} is to execute the functions in {wbpip} to do the calculations and then build the data frames. As of today (2021-08-30), the process is a little intricate. Lets take the example of estimating distributive measures in the pipeline. The image below shows that there are at least three intermediate function levels between the db_compute_dist_stats() function, which is directly executed in the pc pipeline, and the wbpip::md_compute_dist_stats(), which makes the calculations. Also, notice that the functions are very general in regards to the output. No higher level function is specific enough to retrieve only one measure, such as the Gini coefficient, or the median, or the quantiles of the distribution. If you need to add or modify one particular distributive measure, you must do it in functions inside wbpip::md_compute_dist_stats(), making sure the new output does not mess up the execution of any of the intermediate functions before the results get to db_compute_dist_stats(). This long chain of functions is inflexible and makes debugging very difficult. So, if you need to make any modification, first identify the chain of execution in each pipdm function you modify, and then make sure your changes do not affect the output format as it may break the execution chain. This is also a good example of why this structure needs to be improved. 9.5.2 Updating {pipdm} (or any other PIP package) As explained above, if you need to modify any function in pipdm or in wbpip, you need to make sure that the output does not conflict with the execution chain. Additionally, If you update any of the packages developed by the PIP team, make sure you always increased the version of the package using the function usethis::use_version(). Even if the change in the package is small, you need to increase the version of the package. Otherwise, {targets} wont execute the sections of the pipeline that run the functions you changed. Finally as explained in the Prerequisites, if you are working on a branch different than master, make sure you install that version of the package before running the pipeline. 9.6 Executing the _targets.R file The .Rprofile at the root of the directory makes sure that both {targets} and {tarchetypes} are loaded when the project is started. The whole pipeline execution might be very time consuming because it still needs to load all the data in the network drive. If you use a desktop remote connection the execution might be faster than running it locally, but it is still very time consuming. So, it is advisable to only execute the targets that are directly affected by your changes and manually check that everything looks ok. After that, you can execute the entire code confidently and leave it running overnight. In order to execute the whole pipeline, you only need to type the directive tar_make() in the console. If you want to execute only one target, then type the name of the target in the same directive, e.g., tar_make(dl_dist_stats). Keep in mind that if the inputs of prior targets to the objective target have changed, those targets will be executed first. 9.7 Debugging Debugging in targets is not easy. Yet, there are two ways to do it. The first way is provided in the chapter Debugging of the Targets Manual. It provides clear instruction on how to debug while still being in the pipeline, but it could be the case that you dont find this method flexible to dig deep enough into the problem. Alternatively, you could debug by stepping out of the pipeline a little bit and gain more flexibility, as described below. Debugging is needed in one of two cases: one, because you got an error when running the pipeline with tar_make() or, two, because your results are odd. In either case, you should probably have an ideathough not alwaysof where the problem is. If the problem is an error in the execution of the pipeline, {targets} printed messages are usually informative. Debugging stem targets Lets see a simple example. Assume the problem is in the target dt_dist_stats, which is created by executing the function db_create_dist_table of the {pipdm} package. Since the problem is in there, all the targets and inputs necessary to create dt_dist_stats should be available in the _targets/ data store. So, you can load them using tar_load() and execute the function in debugging mode. Like this, tar_load(dl_dist_stats) tar_load(svy_mean_ppp_table) tar_load(cache_inventory) debugonce(pipdm::db_create_dist_table) pipdm::db_create_dist_table( dl = dl_dist_stats, dsm_table = svy_mean_ppp_table, crr_inv = cache_inventory ) Note that you must use the :: because the environment in which {targets} runs is different from your global environment, in which you might not have attached all the libraries. Debugging branch targets The challenge debugging branch targets is that if the problem is in a specific survey, you cant access the subtarget using the survey ID, or something of that nature, because the name of the subtarget is created by {targets} using a random number. This requires a little more of work. Imagine now that the distributive measures of IDN 2015 are wrong. You see the pipeline and notice that these calculations are executed in target dl_dist_stats, which is the branch target created over all the cache files! It would look like something like this: tar_target( name = dl_dist_stats, command = db_compute_dist_stats(dt = cache, mean = dl_mean, pop = aux_pop, cache_id = cache_ids), pattern = map(cache, dl_mean, cache_ids), iteration = &quot;list&quot; ) In order to find the problem in IDN 2015, this what you could do: # Load data dt &lt;- pipload::pip_load_cache(&quot;IDN&quot;, 2015, &quot;PC&quot;) tar_load(dl_mean) tar_load(cache_ids) tar_load(aux_pop) # Extract corresponding mean and cache ID idt &lt;- which(cache_ids == unique(dt$cache_id)) cache_id &lt;- cache_ids[idt] mean_i &lt;- dl_mean[[idt]] # Esecute the function of interest debugonce(pipdm:::compute_dist_stats) ds &lt;- pipdm::db_compute_dist_stats(dt = dt, mean = mean_i, pop = aux_pop, cache_id = cache_id) First, you load all the inputs. Since target dl_mean is a relatively light object, we load it directly from the _targets/ data store. Targets cache_ids and aux_pop are data frames, not lists, so we also load them from memory. The microdata, however, is problematic because target cache, which is the one that is parsed to create the actual dl_dist_stata target, is a huge list with all the micro, grouped, and imputed data. The solution is then to load the data frame of interest, using either pipload or fst. Secondly, we need to filter the list dl_mean and the data frame cache_ids to parse only the information accepted by the pipdm::db_compute_dist_stats() function. This has to be done when debugging because in the actual target this is done iteratively in pattern = map(cache, dl_mean, cache_ids). Finally, you execute the function of interest. Notice something else. The target aux_pop is parsed as a single data frame because pipdm::db_compute_dist_stats() requires it that way. This is also one of the reasons why these functions in {pipdm} need some fixing and consistency in the format of the their inputs. "],["tmpipelilne.html", "Chapter 10 Table Maker Pipeline", " Chapter 10 Table Maker Pipeline NOTE:Andres, write this chapter when the pipeline is done "],["load.html", "Chapter 11 Load microdata and Auxiliary data 11.1 Auxiilary data 11.2 Microdata", " Chapter 11 Load microdata and Auxiliary data Make sure you have all the packages installed and loaded into memory. Given that they are hosted in Github, the code below makes sure that any package in the PIP workflow can be installed correctly. ## First specify the packages of interest packages = c(&quot;pipaux&quot;, &quot;pipload&quot;) ## Now load or install&amp;load all package.check &lt;- lapply( packages, FUN = function(x) { if (!require(x, character.only = TRUE)) { pck_name &lt;- paste0(&quot;PIP-Technical-Team/&quot;, x) devtools::install_github(pck_name) library(x, character.only = TRUE) } } ) 11.1 Auxiilary data Even though pipaux has more than 25 functions, most of its features can be executed by using only the pipaux::load_aux and pipaux::update_aux functions. 11.1.1 udpate data the main function of the pipaux package is udpate_aux. The first argument of this function is measure and it refers to the measure data to be loaded. The measures available are cpi, gdm, gdp, pce, pfw, pop, and ppp. pipaux::update_aux(measure = &quot;cpi&quot;) 11.1.2 Load data Loading auxiliary data is the job of the package pipload through the function pipload::pip_load_aux(), though pipaux also provides pipaux::load_aux() for the same purpose. Notice that, though both function do exactly the same, the loading function from pipload has the prefix pip_ to distinguish it from the one in pipaux. However, we are going to limit the work of pipaux to update auxiliary data and the work of pipload to load data. Thus, all the examples below use pipload for loading either microdata or auxiliary data. df &lt;- pipload::pip_load_aux(measure = &quot;cpi&quot;) head(df) #&gt; country_code cpi_year survey_year cpi ccf survey_acronym change_cpi2011 cpi2011 cpi_domain #&gt; 1: AGO 2000 2000 0.0339 1 HBS 0 0.0339 1 #&gt; 2: AGO 2008 2008 0.7233 1 IBEP-MICS 1 0.7233 1 #&gt; 3: AGO 2018 2018 2.9354 1 IDREA 1 2.9354 1 #&gt; 4: ALB 1996 1996 0.4445 1 EWS 1 0.4445 1 #&gt; 5: ALB 2002 2002 0.7803 1 LSMS 1 0.7803 1 #&gt; 6: ALB 2005 2005 0.8385 1 LSMS 1 0.8385 1 #&gt; cpi_domain_value cpi2011_unadj cpi2011_AM20 cpi2011_unadj_AM20 cpi2005_AM20 cpi_final_2019 #&gt; 1: 1 0.0339 0.0338 0.0338 0.0719 NA #&gt; 2: 1 0.7233 0.7233 0.7233 1.5287 NA #&gt; 3: 1 2.9354 3.0606 3.0606 NA NA #&gt; 4: 1 0.4445 0.4444 0.4444 0.5300 NA #&gt; 5: 1 0.7803 0.7803 0.7803 0.9505 NA #&gt; 6: 1 0.8385 0.8385 0.8385 1.0000 NA #&gt; cpi_data_level cpi_id #&gt; 1: national CPI_v05_M_v01_A #&gt; 2: national CPI_v05_M_v01_A #&gt; 3: national CPI_v05_M_v01_A #&gt; 4: national CPI_v05_M_v01_A #&gt; 5: national CPI_v05_M_v01_A #&gt; 6: national CPI_v05_M_v01_A 11.2 Microdata Loading PIP microdata is the most practical action in the pipload package. However, it is important to understand the logic of microdata. PIP microdata has several characteristics, There could be more than once survey for each Country/Year. This happens when there are more than one welfare variable available such as income and consumption. Some countries, like Mexico, have the two different welfare types in the same survey for the same country/year. This add a layer of complexity when the objective is to known which is default one. There are multiple version of the same harmonized survey. These version are organized in a two-type vintage control. It is possible to have a new version of the data because the Raw datathe one provided by the official NSOhas been updated, or because there has been un update in the harmonization process. Each survey could be use for more than one analytic tool in PIP (e.g., Poverty Calculator, Table Maker, or SOL). Thus, the data to be loaded depends on the tool in which it is going to be used. Thus, in order to make the process of finding and loading data efficiently, pipload is a three-step process. 11.2.1 Inventory file The inventory file resides in y:/PIP-Data/_inventory/inventory.fst. This file is a data frame with all the microdata available in the PIP structure. It has two main variables, orig and filename. The former refers to the full directory path of the database, whereas the latter is only the file name. the other variables in this data frame are derived from these two. The inventory file is used to speed up the file searching process in pipload. In previous packages, each time the user wanted to find a particular data base, it was necessary to look into the folder structure and extract the name of all the file that meet a particular criteria. This is time-consuming and inefficient. The advantage of this method though, is that, by construction, it finds all the the data available. By contrast, the inventory file method is much faster than the searching method, as it only requires to load a light file with all the data available, filter the data, and return the required information. The drawback, however, is that it needs to be kept up to date as data changes constantly. To update the inventory file, you need to use the function pip_update_inventory. If you dont provide any argument, it will update the whole inventory, which may take around 10 to 15 minthe function will warn you about it. By provide the country/ies you want to update, the process is way faster. # update one country pip_update_inventory(&quot;MEX&quot;) # Load inventory file df &lt;- pip_load_inventory() head(df[, &quot;filename&quot;]) #&gt; filename #&gt; 1: AGO_2000_HBS_V01_M_V01_A_PIP_PC-GPWG.dta #&gt; 2: AGO_2008_IBEP-MICS_V02_M_V02_A_PIP_PC-GPWG.dta #&gt; 3: AGO_2008_IBEP-MICS_V02_M_V02_A_PIP_TB-ALL.dta #&gt; 4: AGO_2018_IDREA_V01_M_V01_A_PIP_PC-GPWG.dta #&gt; 5: AGO_2018_IDREA_V01_M_V01_A_PIP_TB-ALL.dta #&gt; 6: ALB_1996_EWS_V01_M_V01_A_PIP_PC-HIST.dta 11.2.2 Finding data Every dataset in the PIP microdata repository is identified by seven variables! Country code, survey year, survey acronym, master version, alternative version, tool, and source. So giving the user the responsibility to know all the different combinations of each file is a heavy burden. Thus, the data finder, pip_find_data(), will provide the names of all the files available that meet the criteria in the arguments provided by the user. For instance, if the use wants to know the all the file available for Paraguay, we could type, pip_find_data(country = &quot;PRY&quot;)[[&quot;filename&quot;]] #&gt; [1] &quot;PRY_1990_EH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; &quot;PRY_1995_EH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; #&gt; [3] &quot;PRY_1997_EIH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_1999_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; #&gt; [5] &quot;PRY_2001_EIH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2001_EIH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [7] &quot;PRY_2002_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2002_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [9] &quot;PRY_2003_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2003_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [11] &quot;PRY_2004_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2004_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [13] &quot;PRY_2005_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2005_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [15] &quot;PRY_2006_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2006_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [17] &quot;PRY_2007_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2007_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [19] &quot;PRY_2008_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2008_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [21] &quot;PRY_2009_EPH_V01_M_V06_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2009_EPH_V01_M_V06_A_PIP_TB-ALL.dta&quot; #&gt; [23] &quot;PRY_2010_EPH_V01_M_V06_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2010_EPH_V01_M_V06_A_PIP_TB-ALL.dta&quot; #&gt; [25] &quot;PRY_2011_EPH_V01_M_V07_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2011_EPH_V01_M_V07_A_PIP_TB-ALL.dta&quot; #&gt; [27] &quot;PRY_2012_EPH_V01_M_V04_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2012_EPH_V01_M_V04_A_PIP_TB-ALL.dta&quot; #&gt; [29] &quot;PRY_2013_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2013_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [31] &quot;PRY_2014_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2014_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [33] &quot;PRY_2015_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2015_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [35] &quot;PRY_2016_EPH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2016_EPH_V01_M_V02_A_PIP_TB-ALL.dta&quot; #&gt; [37] &quot;PRY_2017_EPH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2017_EPH_V01_M_V02_A_PIP_TB-ALL.dta&quot; #&gt; [39] &quot;PRY_2018_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2018_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [41] &quot;PRY_2019_EPH_V01_M_V01_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2019_EPH_V01_M_V01_A_PIP_TB-ALL.dta&quot; Yet, if the user need to be more precise in its request, she can add information to the different arguments of the function. For example, this is data available in 2012, pip_find_data(country = &quot;PRY&quot;, year = 2012)[[&quot;filename&quot;]] #&gt; [1] &quot;PRY_2012_EPH_V01_M_V04_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2012_EPH_V01_M_V04_A_PIP_TB-ALL.dta&quot; 11.2.3 Loading data Function pip_load_data takes care of loading the data. The very first instruction within pip_load_data is to find the data avialable in the repository by using pip_load_inventory(). The difference however is two-fold. First, pip_load_data will load the default and/or most recent version of the country/year combination available. Second, it gives the user the possibility to load different datasets in either list or dataframe form. For instance, if the user wants to load the Paraguay data in 2014 and 2015 used in the Poverty Calculator tool, she may type, df &lt;- pip_load_data(country = &quot;PRY&quot;, year = c(2014, 2015), tool = &quot;PC&quot;) janitor::tabyl(df, survey_id) #&gt; survey_id n percent #&gt; PRY_2014_EPH_V01_M_V03_A_PIP_PC-GPWG 20109 0.395 #&gt; PRY_2015_EPH_V01_M_V03_A_PIP_PC-GPWG 30737 0.605 "],["pc-docker.html", "Chapter 12 Docker Container 12.1 Introduction 12.2 Prerequisites 12.3 Docker on World Bank laptops 12.4 Create volume 12.5 Build image 12.6 Run container 12.7 Debugging 12.8 Security 12.9 Base image options 12.10 Using R on Linux 12.11 Resources", " Chapter 12 Docker Container 12.1 Introduction This chapter explains how to develop and run the Docker container for the Poverty Calculator API on local World Bank laptops. For DEV, QA and Production deployments please see the chapter on Deploying to Azure. For the Table Maker API please refer to TBD. 12.2 Prerequisites Local admin account Docker Desktop for Windows WSL 2 (recommended) Visual Studio Code (recommended) See below for recommendations on how to install Docker Desktop and WSL 2. Visual Code is not strictly needed, but the VS Docker plugin is one of the best tools to interact with Docker. 12.3 Docker on World Bank laptops 12.3.1 Install Docker Install Docker Dekstop (v. 3.6.0 or later) Remember to check the box to enable WSL integration. Activate WSL Check if the WSL feature on Windows is activated (e.g. run wsl --help from Powershell). If it is then proceed to step 3. Note: If you get a message saying service cannot be started try restarting WSL like suggested below under Known problems and solutions. Go to Control Panel -&gt; Programs -&gt; Turn Windows features on or off and check the box Windows Subsystem for Linux, or follow these steps if you prefer to activate WSL from the command line. Reboot system. Install WSL 2 Download and install the Linux kernel update package. Set WSL 2 as your default version (run wsl --set-default-version 2 from Powershell). Configure Docker access privileges In order to run Docker wo/ admin privileges you need to add yourself to the docker-users group. Open Computer Management. Go to Local User and Groups -&gt; Groups -&gt; docker-users and add your regular account (WB/wb&lt;UPI&gt;) to the list of users. Reboot system. Start Docker Desktop and enjoy your hard work :-) 12.3.2 Known problems and solutions Docker Dekstop sometimes fails with the following error. System.InvalidOperationException: Failed to deploy distro docker-desktop to C:\\Users\\&lt;User&gt;\\AppData\\Local\\Docker\\wsl\\distro: exit code: -1 stdout: Error: 0xffffffff The root cause of this problem is likely that the World Bank VPN connection modifies the same network configuration as the wsl VM. Turning off your VPN when working with Docker locally is thus a good idea. A temporary solution to the issue is to open a CMD shell in Admin mode, and then run the following code to restart LxssManager . &gt; sc config LxssManager start=auto [SC] ChangeServiceConfig SUCCESS After this you will also need to restart Docker Desktop. 12.3.3 Tips and tricks Start your Docker working day by opening a CMD shell in admin-mode and run sc config LxssManager start=auto. Keep the shell open because you might need to re-run the command if WSL crashes. Turn off your VPN when using Docker. This avoids the issue with WSL crashing from time to time. It still might happen though if the VPN switces on automatically or if you for other reasons switch back and forth. It is possible that solutions like wsl-vpnkit and vpnkit will help with the WSL issues, but these have not been tested yet. Feel free to give it a try yourself. If WSL causes too much pain, try switching to the legacy Hyper-V backend. (Go to Docker Desktop -&gt; General Setting -&gt; Use the WSL 2 based engine). Re-build an image from a partial cache by adding a ARG or ENV variable right before the layer you want to invalidate (yes, this is hacky but it works). E.g. do something like: # Install PIP specific R packages ENV test=test06302021 RUN Rscript -e &quot;remotes::install_github(&#39;PIP-Technical-Team/wbpip@master&#39;, repos = &#39;${CRAN}&#39;)&quot; RUN Rscript -e &quot;remotes::install_github(&#39;PIP-Technical-Team/pipapi@master&#39;, repos = &#39;${CRAN}&#39;)&quot; 12.4 Create volume The data that is used in the poverty calculations is stored outside the Docker container. We therefore need to link a data source on our local machine to the container when we run it. This could be accomplished with a bind mount, but the preferred way is to use a named volume. You can create and populate a volume by following the steps below. First we create an empty volume, then we link this to a temporay container, before using docker cp to copy the files to the container and volume. After the data has been copied you can discard the temporary container. The choice of Ubuntu 20.04 for the temporary container is arbitrary. You could use another image if you want. # Create volume docker volume create pip-vol # Mount volume to tmp container docker run -d --name pip-data --mount type=volume,source=pip-vol,target=/data ubuntu:20.04 # Copy data to container (and volume) docker cp &lt;data-folder&gt;/. pip-data:data # Stop and remove tmp container docker stop pip-data docker rm pip-data For some purposes you will only need to create this volume once, but remember to update the contents of the volume in case the data structure changes or you want to make sure the container is running against the latest available data. See the Azure DevOps Data repo (DEV branch) for the latest updates. The volume can be inspected with the docker inspect and docker system commands. If you are running Docker Desktop on Windows with WSL 2.0 then then the physical location of Docker volumes is usually found under \\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes. For more information on volumes see the Use volumes section in Dockers reference manual. 12.5 Build image Docker images are built using the docker build command. Here we build the image and set the image name to pip-api. docker build -t pip-api . You can also set a specific tag for the image. For example you can specify the version number or the base image used as source. How you decide to tag your images is up to you. Often a tag of latest (the default) will suffice, but in other instances keeping track of different versions of your development image will be useful. # Set tag when building docker build -t pip-api:0.0.4 # Set tag after build (version number) docker image tag pip-api:latest pip-api:0.0.4 # Set tag after build (base image) docker image tag pip-api:latest pip-api:ubi8 Note that each layer in a Dockerfile is cached when built. These layers wont be re-run unless the code to produce that specific layer or a previous layer in the Dockerfile changes. This is handy for fast iterations, but can also cause the image to be outdated. If needed you can use the --no-cache flag to force a re-build. This is useful when you want to make sure that the latest Linux updates are installed or for updating the {wbpip} and {pipapi} R packages. See also the Tips and Tricks section on how to do partial re-builds. docker build --no-cache -t pip-api . 12.6 Run container Run the container by exposing port 80 and mounting a volume with the survey and auxiliary data. The data structure in the attached volume must correspond to the sub-folder specifications in R/main.R. docker run --rm -d -p 80:80/tcp --mount src=pip-vol,target=/ipp,type=volume,readonly --name pip-api pip-api You can also run the container with a bind mount if you prefer, but note that reading the files inside mount folder will be slower then with a volumne setup. docker run --rm -d -p 80:80/tcp --mount src=&quot;&lt;data-folder&gt;&quot;,target=/data,type=bind --name pip-api pip-api Navigate to http://localhost/__docs__/ to see the running API. For details on the docker run command and its options see the Docker Docs. 12.7 Debugging Run container interactively: Run the container in interactive mode, using the -it flag, to see output messages. $ docker run --rm -it -p 80:80/tcp --mount src=pip-vol,target=/ipp,type=volume,readonly --name pip-api pip-api Running plumber API at http://0.0.0.0:80 Running swagger Docs at http://127.0.0.1:80/__docs__/ Inspect container: For development purposes it can be useful to inspect the Docker container. Luckily it is very easy to enter a running container with the docker exec command. # Enter the container as default user $ docker exec -it pip-api /bin/bash plumber@bd8ea77299ca:/$ # Enter the container as root user $ docker exec -it -u 0 pip-api /bin/bash root@bd8ea77299ca:/$ 12.8 Security Since the PIP Techincal Team is developing both the API and the Docker container, we also have a larger responsibility for security. When deploying to Azure the image will need to go through a security scan, provided by Aquasec, that is generally quite strict. Any high level vulnerability found by the Aquasec scan will result in a failed deployment. It is not possible to leverage Aquasec on local machines, but we can get an indication of potential vulnerabilites by taking advantage of the built in security scan that comes with Docker. Additionally it is also possible to scan the contents of the R packages used in the image. Note that neither the Snyk or Oyster scans described below are requirements from OIS. They are only included here as additional and preliminary tools. Run image security scan: Before deploying to Azure you can run preliminary security scans with Snyk. See Vulnerability scanning for Docker local images and Docker Security Scanning Guide 2021 for details. docker scan --file .\\Dockerfile pip-api It is important to note that the Aquasec scan that runs on Azure is different that the Snyk scan, and can detect different vulnerabilites. Even though few or zero vulnerabilites are found by Snyk, further issues might still be detected by Aquasec. Run security scan for R packages: You can scan the R packages inside the container with the {oysteR} package, which scans R projects against insecure dependencies using the OSS Index. First log into a running container as root, and start R. $ docker exec -it -u 0 pip-api /bin/bash root@ab54fbf1eb36:/# R Then install the {oyster} package, and run an audit. install.packages(&quot;oysteR&quot;) library(&quot;oysteR&quot;) audit &lt;- audit_installed_r_pkgs() get_vulnerabilities(audit) Note that finding zero vulnerabilities is no guarantee against threats. For example is scanning of C++ code inside R packages not yet implemented. For more details and latest developments on the {oyster} package see the README on Github. 12.9 Base image options In the development process of the Dockerfile there have been used different base images. The current image in the main branch is based on ubi8, but other options (rocker, centos8) are available in seperate branches. These seperate branches will not be actively maintained, but should be kept in case there is a need to switch to another base image in the future. The main need to switch base image is likely to stem from the Azure DevOps security scan. For example do images based on rocker or centos8 work well locally, but neither will pass deployment. CentOS images are unfortunately not on the list of approved images by OIS, while Rocker images (which are based on Ubuntu 20.04 LTS) currently has a vulnerability (CVE-2019-18276) that is classified as a high level threat by Aqua Scan. In fact one of the Linux dependencies for R on UBI 8, libX11, also currently trigger a high level vulnerability (CVE-2020-14363) in Aqua Scan. Even though this issue has been solved in the offical RHEL 8 release, the issue still persist for UBI 8. It can however be solved by manually downloading a newer version of libX11 (1.6.12 or later), and removing the problematic version (1.6.8). In case there is any need to develop with other base images in the future it is strongly recommended to start with one of the Linux distributions where RStudio provides precompiled binaries for R. This avoids the need to install R from source. 12.10 Using R on Linux Docker images are usually based on Linux containers. Some familiarity with Linux is thus necessary in order to develop and work with Docker. R users in particular should familiarize themselves with the difference between installing R and R packages from source vs binary, as well as potential Linux dependencies that might be nessecary to make a specific package work. Installing R: Installing R from source is possible on most Linux distributions, but this not always an easy endavour. It will often require you to maintain a list of the nessecary Linux dependencies for R. Installing from source is also much slower than installing from a binary version. Another way to install R on Linux is to use the available pre-compiled binary in your Linux distros repository (e.g sudo apt install r-base on Ubuntu). The problem with this approach is that the latest version of R might not yet be available, and for some Linux distros there will also only be a limited number of R versions available. This impedes the flexilibility needed for a production grade project. The recommended approach is thus to rely on RStudios precompiled binaries. These binaries are available for the most common Linux distributions, including Ubuntu, Debian and CentOS / RHEL. Relying on these binaries will limited the number of base image options avaiable, e.g. a ligth-weight distro like Alpine is not supported, nor is the newest release of Ubuntu (Hirsute Hippo). But it will make much easier to maintain, upgrade and change the R version used in the project. Installing R packages: As a contrast to Windows and macOS, CRAN only supports source versions of the R packages available for Linux. This will significantly slow down the installation process of each pacakge. Luckily RStudio provides binary versions of all packages on CRAN for a series of different Linux distros through their Public Pacakge Manager. It is therefore strongly recommended that you rely on RSPM as your installation repo. An additional benefit of RSPM is that you can also select a specific date for the package versions to use. On the RSPM website go the Setup page on the main menu and select the appropiate client OS and date. You should then see a URL similar too https://packagemanager.rstudio.com/all/__linux__/centos8/4743918, where centos8 represents the distro and 4743918 the date. For a more detailed introduction to the difference between binary and source packages, see Package structure and state in the book R Packages. Installing R package dependencies: Linux distros doesnt come with the dependencies needed to work with all R packages. This includes common packages like {data.table} and {plumber}. It is important that such dependencies are installed prior to the installation of your R packages. The best way to look for dependencies is to use the RSPM website and search for the specific package in question. Remeber to also select the correct Linux distribution. 12.11 Resources For a further introduction to Docker take a look at this Data Science for Economists lecture on Docker by Grant McDermott. See Best practices for writing Dockerfiles for advice on how to build Docker images. Follow the Docker for Windows release notes for information on new releases and bug fixes. "],["azure.html", "Chapter 13 Deploying to Azure 13.1 Poverty Calculator Data 13.2 Poverty Calculator Docker image", " Chapter 13 Deploying to Azure The Azure DevOps site for PIP currently consists of two repos; ITSES-POVERTYSCORE and ITSES-POVERTYSCORE-DATA. Each repo consists of three branches; DEV, QA and PROD. The PIP Techincal Team will handle deployments to DEV and QA, while ITS need to approve deployments to Production. Note: PROD deployments are TBD 13.1 Poverty Calculator Data Before you start with the data deployment process you will need to clone the Azure DevOps Data repo (DEV branch) to your local machine. You will only need to this once. After that you can follow the step by step guide below. 13.1.1 Deploying data on DEV Step 1: Sync you local DEV branch with the remote. Commit and push any data changes to the remote. Step 2: Nagivate to the Azure DevOps Data repo. Click on Pipelines. Step 3: Select the FILE-COPY-DEV-CI pipeline. Step 4: Click on Run pipeline. Step 5: Click on Variables -&gt; FolderName, and add the name of the folder which should be copied to the Cloud Blob Store. Step 6: Click on Run. Step 7: View the new build. Step 8: Click on Releases and select the FILE-COPY-DEV-CD release. Approve the pending request. Step 9: Verify that the build completed. 13.1.2 Deploying data on QA Step 1: Nagivate to the Azure DevOps Data repo. Step 2: Click on Create pull request. Select from DEV to QA. Step 3: Go through the steps to commit and approve the pull request. Please make sure that the Delete source branch box is unchecked, ie. dont delete the DEV branch. Step 5: Go to Pipelines -&gt; Piplines and select the FILE-COPY-QA-CI pipeline. Verify that the pipeline is building. If it wasnt triggered you will need to trigger it manually. Step 6: Go to Pipelines -&gt; Releases and select the FILE-COPY-QA-CD release. Approve the request, and verify that the build completes. 13.1.3 Deploying data to Production TBD. 13.2 Poverty Calculator Docker image Before you start with the application deployment process you will need to clone the Azure DevOps Docker repo (DEV branch) to your local machine. You will only need to this once. After that you can follow the step by step guide below. Please note that it is important that data changes are pushed through and released before deploying the Docker image. This is because the Docker container will need to restart in order to pick up changes in the mounted folder or volume. The best way to do this is to deploy the data, and then re-deploy the Docker image since a re-deployment includes a container restart. 13.2.1 Deploying image on DEV Step 1: Verify that the latest code in the master branches of {wbpip} and {pipapi} works with the latest data on DEV. This can be done by running the {pipapi} pacakge in a local RStudio session. Step 2: [Optional] Verify that the most recent Dockerfile builds on your local machine. This is certainly something that should be done if the contents of the Dockerfile has changed, or before major releases. But in a continous workflow where you know that Dockerfile hasnt changed, it migth be sufficent to verify that the R packages in question are working. Step 3: Navigate to the Azure DevOps Docker repo. Go Pipelines -&gt; Pipelines. Trigger the ACR-DEV-CI pipeline, either by a) Pushing an updated Dockerfile to the remote repo or b) Running the Pipeline manually. Step 4: Go to the Pipelines -&gt; Releases and select Create release in order to run a new deployment. View the logs to see results from the image build and security scan. Step 5:. Vist the DEV API website for further testing. 13.2.2 Deploying image on QA Step 1: Check that the DEV deployment is working correctly. Step 2: Make sure the data on QA is up-to-date (in sync with DEV). If it isnt you will need to create a PR and merge the data first. Step 3: Create a pull request from DEV to QA. Go through the steps to commit and approve the pull request. Please make sure that the Delete source branch box is unchecked, ie. dont delete the DEV branch. Step 4: Go to the Release pipline to see results from the image build and security scan. Step 5: Vist the QA API website for further testing. 13.2.3 Deploying image to Production TBD. "],["gh-workflows.html", "Chapter 14 Github Workflows 14.1 Package webpage using {pkgdown} 14.2 Package build checks 14.3 Code coverage 14.4 Other workflows", " Chapter 14 Github Workflows Github Actions allows for a series of automated workflows that are very handy when it comes to publishing R packages, as well as other more general CI/CD work. This chapter explains the most common workflows used across the PIP project. Other example workflows can be found here. 14.1 Package webpage using {pkgdown} pkgdown makes it easy to build websites for R packages. With a Github Action workflow this is made even easier because the website will be built and published automatically everytime you commit to the master (main) branch. There is thus no need to run pkgdown::build_site() yourself. 14.1.1 Create gh-pages branch The most common is for a {pkgdown} site to live in a seperate branch in your repository called gh-pages. This should be an empty orphan branch, with no files or commits other then the inital root commit. Some {usethis} helpers claims to create this branch for you, but this doesnt always work. Follow the steps to below to create an empty branch manually. # Create new orphan branch git checkout --orphan gh-pages # Remove everything git rm -rf . # Create empty commit git commit --allow-empty -m &quot;root commit&quot; # Push to remote git push origin gh-pages # Switch back to master git checkout master 14.1.2 Create setup files Create a new local branch in your repository, and then take advantage of the helper functions in the {usethis} package. Run this from the working directory of your local branch: usethis::use_pkgdown_github_pages() This should create the entire setup needed, including adding the files _pkgdown.yml and .github/workflows/pkgdown.yaml to your working directory. Create a pull request and merge the changes to the master branch. 14.1.3 Activate Github Pages Go to Settings -&gt; Github Pages. Activate Github Pages and set it to build from gh-pages/root. Remember to also add your page link to the About section of your repo. 14.2 Package build checks A crucial part of any R package development process is the build check. With Github Actions you can add workflows to automatically check your package for different versions of R and on different operating systems. A good way to get started is to use the simple release workflow, but more advanced packages will benefit from a standard or custom workflow. A standard workflow checks if the package builds on the latest available R version on all three major operating systems (Windows, macOS, Ubuntu), and should be used for all packages that are planned to be published on CRAN. A custom workflow is helpful if you want to check your package against specific versions of R or other OS variants. For the current PIP R packages both {wbpip} and {pipapi} go through a standard build check, while other packages have custom workflows. For example do the build checks for {pipaux} and {pipdm} test if the package works for the latest version of R and the current R version on the PovcalNet remote server. 14.3 Code coverage Another important part of package development is to check the code coverage of your unit tests. This is typically done with the {covr} package. With Github Actions you can automatically upload coverage reports to codecov.io, to let yourself and others more easily keep track of the test coverage of your package. 14.3.1 Create setup files Create a new local branch in your repository, and then take advantage of the helper functions in the {usethis} package. Run this from the working directory of your local branch: usethis::use_github_action(&quot;test-coverage&quot;) This should create the entire setup needed, including adding the files codecov.yml and .github/workflows/test-coverage.yaml to your working directory. Create a pull request and merge the changes to the master branch. 14.3.2 Integrate with codecov.io TBD. 14.4 Other workflows The PR commands workflow enables the use of two specific commands in pull request issue comments. /document will use roxygen2 to rebuild the documentation for the package and commit the result to the pull request. /style will use styler to restyle your package. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
