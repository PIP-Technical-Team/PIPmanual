[["index.html", "PIP Manual Prerequisites", " PIP Manual DECIS Poverty GP 2021-05-06 Prerequisites You need to make sure the bookdown package is installed in your computer install.packages(&quot;bookdown&quot;) # or the development version devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction PIP workflow is  "],["load.html", "Chapter 2 Load microdata and Auxiliary data 2.1 Auxiilary data 2.2 Microdata", " Chapter 2 Load microdata and Auxiliary data Make sure you have all the packages installed and loaded into memory. Given that they are hosted in Github, the code below makes sure that any package in the PIP workflow can be installed correctly. ## First specify the packages of interest packages = c(&quot;pipaux&quot;, &quot;pipload&quot;) ## Now load or install&amp;load all package.check &lt;- lapply( packages, FUN = function(x) { if (!require(x, character.only = TRUE)) { pck_name &lt;- paste0(&quot;PIP-Technical-Team/&quot;, x) devtools::install_github(pck_name) library(x, character.only = TRUE) } } ) #&gt; Loading required package: pipaux #&gt; Loading required package: pipload 2.1 Auxiilary data Even though pipaux has more than 25 functions, most of its features can be executed by using only the pipaux::load_aux and pipaux::update_aux functions. 2.1.1 udpate data the main function of the pipaux package is udpate_aux. The first argument of this function is measure and it refers to the measure data to be loaded. The measures available are cpi, gdm, gdp, pce, pfw, pop, and ppp. pipaux::update_aux(measure = &quot;cpi&quot;) #&gt; no labels available for measure `country_list` #&gt; v CPI data is up to date 2.1.2 Load data Loading auxiliary data is the job of the package pipload through the function pipload::pip_load_aux(), though pipaux also provides pipaux::load_aux() for the same purpose. Notice that, though both function do exactly the same, the loading function from pipload has the prefix pip_ to distinguish it from the one in pipaux. However, we are going to limit the work of pipaux to update auxiliary data and the work of pipload to load data. Thus, all the examples below use pipload for loading either microdata or auxiliary data. df &lt;- pipload::pip_load_aux(measure = &quot;cpi&quot;) #&gt; v Most recent version of data loaded #&gt; //w1wbgencifs01/pip/PIP-Data_QA/_aux/cpi/cpi.fst head(df) #&gt; country_code cpi_year survey_year cpi ccf survey_acronym change_cpi2011 #&gt; 1: AGO 2000 2000.21 0.03385145 1 HBS 0 #&gt; 2: AGO 2008 2008.50 0.72328920 1 IBEP-MICS 1 #&gt; 3: AGO 2018 2018.17 2.93543023 1 IDREA 1 #&gt; 4: ALB 1996 1996.00 0.44446184 1 EWS 1 #&gt; 5: ALB 2002 2002.00 0.78033877 1 LSMS 1 #&gt; 6: ALB 2005 2005.00 0.83852839 1 LSMS 1 #&gt; cpi2011 cpi_domain cpi_domain_value cpi2011_unadj cpi2011_AM20 cpi2011_unadj_AM20 #&gt; 1: 0.03385145 1 1 0.03385145 0.033848061 0.033848061 #&gt; 2: 0.72328920 1 1 0.72328920 0.723337197 0.723337197 #&gt; 3: 2.93543023 1 1 2.93543023 3.060594983 3.060594983 #&gt; 4: 0.44446184 1 1 0.44446184 0.444432734 0.444432734 #&gt; 5: 0.78033877 1 1 0.78033877 0.780287716 0.780287716 #&gt; 6: 0.83852839 1 1 0.83852839 0.838473458 0.838473458 #&gt; cpi2005_AM20 cpi_final_2019 cpi_data_level cpi_id #&gt; 1: 0.071889997 NA national CPI_v05_M_v01_A #&gt; 2: 1.528669953 NA national CPI_v05_M_v01_A #&gt; 3: NA NA national CPI_v05_M_v01_A #&gt; 4: 0.530049980 NA national CPI_v05_M_v01_A #&gt; 5: 0.950504005 NA national CPI_v05_M_v01_A #&gt; 6: 1.000000000 NA national CPI_v05_M_v01_A 2.2 Microdata Loading PIP microdata is the most practical action in the pipload package. However, it is important to understand the logic of microdata. PIP microdata has several characteristics, There could be more than once survey for each Country/Year. This happens when there are more than one welfare variable available such as income and consumption. Some countries, like Mexico, have the two different welfare types in the same survey for the same country/year. This add a layer of complexity when the objective is to known which is default one. There are multiple version of the same harmonized survey. These version are organized in a two-type vintage control. It is possible to have a new version of the data because the Raw datathe one provided by the official NSOhas been updated, or because there has been un update in the harmonization process. Each survey could be use for more than one analytic tool in PIP (e.g., Poverty Calculator, Table Maker, or SOL). Thus, the data to be loaded depends on the tool in which it is going to be used. Thus, in order to make the process of finding and loading data efficiently, pipload is a three-step process. 2.2.1 Inventory file The inventory file resides in y:/PIP-Data/_inventory/inventory.fst. This file is a data frame with all the microdata available in the PIP structure. It has two main variables, orig and filename. The former refers to the full directory path of the database, whereas the latter is only the file name. the other variables in this data frame are derived from these two. The inventory file is used to speed up the file searching process in pipload. In previous packages, each time the user wanted to find a particular data base, it was necessary to look into the folder structure and extract the name of all the file that meet a particular criteria. This is time-consuming and inefficient. The advantage of this method though, is that, by construction, it finds all the the data available. By contrast, the inventory file method is much faster than the searching method, as it only requires to load a light file with all the data available, filter the data, and return the required information. The drawback, however, is that it needs to be kept up to date as data changes constantly. To update the inventory file, you need to use the function pip_update_inventory. If you dont provide any argument, it will update the whole inventory, which may take around 10 to 15 minthe function will warn you about it. By provide the country/ies you want to update, the process is way faster. # update one country pip_update_inventory(&quot;MEX&quot;) #&gt; i file inventory.fst is up to date. #&gt; No update performed # Load inventory file df &lt;- pip_load_inventory() head(df[, &quot;filename&quot;]) #&gt; filename #&gt; 1: AGO_2000_HBS_V01_M_V01_A_PIP_PC-GPWG.dta #&gt; 2: AGO_2008_IBEP-MICS_V02_M_V02_A_PIP_PC-GPWG.dta #&gt; 3: AGO_2008_IBEP-MICS_V02_M_V02_A_PIP_TB-ALL.dta #&gt; 4: AGO_2018_IDREA_V01_M_V01_A_PIP_PC-GPWG.dta #&gt; 5: AGO_2018_IDREA_V01_M_V01_A_PIP_TB-ALL.dta #&gt; 6: ALB_1996_EWS_V01_M_V01_A_PIP_PC-HIST.dta 2.2.2 Finding data Every dataset in the PIP microdata repository is identified by seven variables! Country code, survey year, survey acronym, master version, alternative version, tool, and source. So giving the user the responsibility to know all the different combinations of each file is a heavy burden. Thus, the data finder, pip_find_data(), will provide the names of all the files available that meet the criteria in the arguments provided by the user. For instance, if the use wants to know the all the file available for Paraguay, we could type, pip_find_data(country = &quot;PRY&quot;)[[&quot;filename&quot;]] #&gt; [1] &quot;PRY_1990_EH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; &quot;PRY_1995_EH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; #&gt; [3] &quot;PRY_1997_EIH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_1999_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; #&gt; [5] &quot;PRY_2001_EIH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2001_EIH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [7] &quot;PRY_2002_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2002_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [9] &quot;PRY_2003_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2003_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [11] &quot;PRY_2004_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2004_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [13] &quot;PRY_2005_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2005_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [15] &quot;PRY_2006_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2006_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [17] &quot;PRY_2007_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2007_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [19] &quot;PRY_2008_EPH_V01_M_V05_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2008_EPH_V01_M_V05_A_PIP_TB-ALL.dta&quot; #&gt; [21] &quot;PRY_2009_EPH_V01_M_V06_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2009_EPH_V01_M_V06_A_PIP_TB-ALL.dta&quot; #&gt; [23] &quot;PRY_2010_EPH_V01_M_V06_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2010_EPH_V01_M_V06_A_PIP_TB-ALL.dta&quot; #&gt; [25] &quot;PRY_2011_EPH_V01_M_V07_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2011_EPH_V01_M_V07_A_PIP_TB-ALL.dta&quot; #&gt; [27] &quot;PRY_2012_EPH_V01_M_V04_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2012_EPH_V01_M_V04_A_PIP_TB-ALL.dta&quot; #&gt; [29] &quot;PRY_2013_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2013_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [31] &quot;PRY_2014_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2014_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [33] &quot;PRY_2015_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2015_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [35] &quot;PRY_2016_EPH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2016_EPH_V01_M_V02_A_PIP_TB-ALL.dta&quot; #&gt; [37] &quot;PRY_2017_EPH_V01_M_V02_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2017_EPH_V01_M_V02_A_PIP_TB-ALL.dta&quot; #&gt; [39] &quot;PRY_2018_EPH_V01_M_V03_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2018_EPH_V01_M_V03_A_PIP_TB-ALL.dta&quot; #&gt; [41] &quot;PRY_2019_EPH_V01_M_V01_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2019_EPH_V01_M_V01_A_PIP_TB-ALL.dta&quot; Yet, if the user need to be more precise in its request, she can add information to the different arguments of the function. For example, this is data available in 2012, pip_find_data(country = &quot;PRY&quot;, year = 2012)[[&quot;filename&quot;]] #&gt; [1] &quot;PRY_2012_EPH_V01_M_V04_A_PIP_PC-GPWG.dta&quot; &quot;PRY_2012_EPH_V01_M_V04_A_PIP_TB-ALL.dta&quot; 2.2.3 Loading data Function pip_load_data takes care of loading the data. The very first instruction within pip_load_data is to find the data avialable in the repository by using pip_load_inventory(). The difference however is two-fold. First, pip_load_data will load the default and/or most recent version of the country/year combination available. Second, it gives the user the possibility to load different datasets in either list or dataframe form. For instance, if the user wants to load the Paraguay data in 2014 and 2015 used in the Poverty Calculator tool, she may type, df &lt;- pip_load_data(country = &quot;PRY&quot;, year = c(2014, 2015), tool = &quot;PC&quot;) #&gt; i Loading data and creating a dataframe v Loading data and creating a dataframe ... done janitor::tabyl(df, survey_id) #&gt; survey_id n percent #&gt; PRY_2014_EPH_V01_M_V03_A_PIP_PC-GPWG 20109 0.39548834 #&gt; PRY_2015_EPH_V01_M_V03_A_PIP_PC-GPWG 30737 0.60451166 "],["pcpipeline.html", "Chapter 3 Poverty Calculator Pipeline (pre-computed estimations) 3.1 Folder structure 3.2 Prerequisites 3.3 Structure of the _targets.R file 3.4 Understanding the pipeline 3.5 Understanding pipdm functions 3.6 Executing the _targets.R file 3.7 Debugging", " Chapter 3 Poverty Calculator Pipeline (pre-computed estimations) The Poverty Calculator Pipelinehereafter, and only for the rest of this chapter, pc pipelineis the technical procedure to calculate the pre-computed estimations of the PIP project. These estimations have two main purposes: Provide the user with instantaneous information about distributive measures of all the household surveys in the PIP repository that do not depend on the value of the poverty line. Avoiding thus the need for re-computation as it was the case in PovcalNet for some of these measures. Provide the necessary inputs to the PIP API. This chapter walks you through the folder structure of the folder, the main R script, _targets.R, and the complete and partial execution of the script. Also, it provides some tips for debugging. 3.1 Folder structure The pc pipeline is hosted in the Github repository PIP-Technical-Team/pip_ingestion_pipeline. At the root of the repo you will find a series of files and folders. #&gt; +-- batch #&gt; +-- pip_ingestion_pipeline.Rproj #&gt; +-- R #&gt; +-- README.md #&gt; +-- renv #&gt; +-- renv.lock #&gt; +-- run.R #&gt; +-- _packages.R #&gt; \\-- _targets #&gt; \\-- _targets.R Folders R Contains long R functions used during the pipeline batch Script for timing the execution of the pipeline. This folder should probably be removed _targets Folder for all objects created during the pipeline. You dont need to look inside as it content is managed by the targets package. renv Folder for reproducible environment. Files _packages.R Created by targets::tar_renv(). Do not modify manually. _targets.R Contains the pipeline. This is the most important file. 3.2 Prerequisites Before you start working on the pc pipeline, you need to make sure to have the following PIP packages. Note: Notice that directives below have suffixes like @development, which specify the branch of the particular package that you need to use. Ideally, the master branch of all packages should be used, but that will only happen until the end of the development process. Note2: If you update any of the packages developed by the PIP team, make sure you always increased the version of the package using the function usethis::use_version(). Even if the change in the package is small, you need on increased the version of the package. Otherwise, {targets} wont execute the sections of the pipeline that run the functions you changed. remotes::install_github(&quot;PIP-Technical-Team/pipdm@development&quot;) remotes::install_github(&quot;PIP-Technical-Team/pipload@development&quot;) remotes::install_github(&quot;PIP-Technical-Team/wbpip@halfmedian_spl&quot;) install.packages(&quot;joyn&quot;) In case renv is not working for you, you may need to install all the packages mentioned in the _packages.R script at the root of the folder. Also, make sure to install the most recent version of targets and tarchetypes packages. 3.3 Structure of the _targets.R file Even thought the pc pipeline script looks like a regular R Script, it is structured in a specific way in order to make it work with {[targets](https://docs.ropensci.org/targets/)} package, starting by the fact that it must be called _targets.R in the root of the project. It highly recommended you read the entire targets manual to fully understand how it works. Also, during this chapter, we will referring to the manual constantly to expand in any particular targets concept. Start up The first part of the pipeline sets up the environment. It, loads the {targets} and {tarchetypes} packages, creates default values like directories, time stamps, survey and reference years boundaries, compression level of .fst files, among other things. executes tar_option_set() to set up some option in {targets}. Two particular options are important, packages and imports for tracking changes in package dependencies. You can read more about it in sections Loading and configuring R packages and Packages-based invalidation of the targets manual. Attach all the packages and functions of the project by running source('_packages.R') and source('R/_common.R') Step 1: small functions According to the section Functions in pipelines of the targets manual, it is recommend to only use functions rather than expressions during the executions. Presumably, the reason for this is that targets track changes in functions but not in expressions. Thus, this section of the scripts defines small functions that are executed along the pc pipeline. In the section above, the scripts source('R/_common.R') loads longer functions. Yet, keep in mind that the 'R/_common.R' was used in a previous version of the pc pipeline before {targets} was implemented. Now, most of the function in 'R/_common.R' are included in the {pipdm} package. Step 2: prepare data This section used to longer in previous versions of the pc pipeline because identified the auxiliary data, loaded he PIP microdata inventory, and created the cache files. Now, it only identifies the auxiliary data and creates the data frame aux_tb, which is used inside the pc pipeline to create targets for each auxiliary file. Not much more to say here. 3.3.1 Step 3: The actual pipeline This part of the pc pipeline is long and it is explained in detail in the next section. Suffice is to say that the order of the pc pipeline is the following, Load all necessary data. That is, auxiliary data and inventories, and then create any cache fie that has not been created yet. Calculate means in LCU Crate deflated survey mean (DSM) table Calculate reference year table (aka., interpolated means table) Calculate distributional stats Create output tables join survey mean table with dist table join reference year table with dist table coverage table aggregate population at the regional level table Clean and save. 3.4 Understanding the pipeline One thing is to understand the how the {targets} package works and something else is to understand how the targets of the Poverty Calculator pc pipeline are created. For the former, you can read the targets manual. For the latter, we should start by making a distinction between the different types of targets. In {targets} terminology, there are two kinds of targets, stems and branches. Stems are unitary targets. That is, for each target there is only one single R object. Branches, on the other hand, are targets that contain several objects or subtargets inside (You can learn more about them in the chapter Dynamic branching of the targets manual). We will see the use of this type of targets when we talk about the use of cache files. Stem targets There are two ways to create stem targets: either using tar_target() or using tar_map() from the {tarchetypes} package. The tar_map() function allows to create stem targets iteratively. See for instance the creation of targets for each auxiliary data, tar_map( values = aux_tb, names = &quot;auxname&quot;, # create dynamic name tar_target( aux_dir, auxfiles, format = &quot;file&quot; ), tar_target( aux, pipload::pip_load_aux(file_to_load = aux_dir, apply_label = FALSE) ) ) tar_map() takes the values in the data frame aux_tb created in Step 2: prepare data and creates two type of targets. First, it create the target aux_dir, using the file paths in column auxfiles of aux_tb to let {targets} know that we will have objects that are loaded from a file and are not created inside the pc pipeline. This is why we use the option format = \"file\". Then, it uses the the column auxname of aux_tb as the names of the targets for each auxiliary data frame (prefixed by the word aux). This is why we had to add the argument file_to_load to pipload::pip_load_aux, so we can let {targets} know that the file paths defined in target aux_dir are used to create the targets prefixed with aux, which are the actual targets. For example, if I need to use the population data frame inside the pc pipeline, Id use the target aux_pop, which had a corresponding file path in aux_dir. In this way, if the original file referenced in aux_dir changes, all the targets that depend on aux_pop will be run again. Branches targets Most of the targets created in the pc pipeline are branch targets. This is so because, instead of loading all the datasets at once (drying out the computers memory), we load each data independently, perform the calculations in each data set, store all the results in a list, and then create a data frame from that list, compiling all the results in one single object. Thus, if every single data set or result were a stem target, it would be not only impossible to visualize, but also more difficult to code. The following example illustrates how it works, # step A tar_target( cache_inventory_dir, cache_inventory_path(), format = &quot;file&quot; ), # step B tar_target( cache_inventory, { x &lt;- fst::read_fst(cache_inventory_dir, as.data.table = TRUE) }, ), # step C tar_target(cache_files, get_cache_files(cache_inventory)), # step D tar_files(cache_dir, cache_files), # step E tar_target(cache, fst::read_fst(path = cache_dir, as.data.table = TRUE), pattern = map(cache_dir), iteration = &quot;list&quot;) The code above illustrates several things. It is divided in steps, being the last step step Ethe part of the code in which we create the branch target. Yet, it is important to understand the steps before. In step A we create target cache_inventory_dir, which is merely the path of the file than contains the cache inventory. Notice that it returned by function and not entered directly into the target. Since it is a file path, we need to add the argument format = \"file\" to let {targets} know that it is input data. In step B we load the cache inventory file into target cache_inventory by providing the target path that we created in step A. This file has several column. One of them contains the file path of every single cache file in the PIP network drive. That single column is extracted from the cache inventory in step C. Now, in step D, each file path is declared as input, using the convenient function tar_files(), creating thus a new target, cache_dir. FInally, we create branch target cache with all the cache files by loading each file. To do this iteratively, we parse the cache_dir target to the path argument of the function fst::read_fst() and to the pattern = map() argument of the tar_target() function. Finally, we need to specify that the output of the iteration is stored as a list, using the argument iteration = \"list\". The basic logic of branch targets is that the vector or list to iterate though should be added in the argument of the function and the pattern = map() argument of the tar_target() function. it is very similar to purrr::map() Note: If we are iterating through more than one vector or list, you need to (1) separate each of them by commas in the map() part of the argument (See example code below). (2) make sure all the vectors or lists have the same length. This is why we cannot remove NULL or NA values from any target. (3) make sure you do NOT sort any of the output targets as it will loose its correspondence with other targets. # Example of creating branch target using several lists to iterate through. tar_target( name = dl_dist_stats, command = db_compute_dist_stats(dt = cache, mean = dl_mean, pop = aux_pop, cache_id = cache_ids), pattern = map(cache, dl_mean, cache_ids), iteration = &quot;list&quot; ) Creating the cache files The creation of the cache files is done in the following code, tar_target(pipeline_inventory, { x &lt;- pipdm::db_filter_inventory( dt = pip_inventory, pfw_table = aux_pfw) # Uncomment for specific countries # x &lt;- x[country_code == &#39;IDN&#39; &amp; surveyid_year == 2015] } ), tar_target(status_cache_files_creation, pipdm::create_cache_file( pipeline_inventory = pipeline_inventory, pip_data_dir = PIP_DATA_DIR, tool = &quot;PC&quot;, cache_svy_dir = CACHE_SVY_DIR, compress = FST_COMP_LVL, force = TRUE, verbose = FALSE, cpi_dt = aux_cpi, ppp_dt = aux_ppp) ) It is important to understand this part of the pc pipeline thoroughly because the cache files used to be created in Step 2: prepare data rather than here. Now, it has not only been integrated in the pc pipeline, but it is also possible to execute the creation of cache files independently from the rest of the pipeline, by following the instructions in [Executing the _targets.R file]. 3.5 Understanding pipdm functions 3.6 Executing the _targets.R file awfawef 3.7 Debugging sdfsdf "],["references.html", "References", " References "]]
