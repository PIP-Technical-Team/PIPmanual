[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIP Manual",
    "section": "",
    "text": "Welcome!\nThese are the internal guidelines for the technical process of the PIP project. Visit the Github repository for this book."
  },
  {
    "objectID": "index.html#team",
    "href": "index.html#team",
    "title": "PIP Manual",
    "section": "Team",
    "text": "Team\nThe PIP technical team is composed of …."
  },
  {
    "objectID": "intro.html#ojectives",
    "href": "intro.html#ojectives",
    "title": "1  Introduction",
    "section": "1.1 Ojectives",
    "text": "1.1 Ojectives\nThis book explains several things,\n\nAn overview of the project from a technical perspective.\nThe interaction between R packages developed to manage the data and do the calculations.\nThe different types of data in the PIP project and the interaction between them.\nHow the poverty calculator, the table maker, and the Statistics Online (SOL) platform are updated.\nTechnical standalone procedure necessary for the execution of some parts of the project."
  },
  {
    "objectID": "intro.html#technical-requirements",
    "href": "intro.html#technical-requirements",
    "title": "1  Introduction",
    "section": "1.2 Technical requirements",
    "text": "1.2 Technical requirements\n\n\n\n\n\n\nYou need to make sure that the bookdown package is installed in your local computer\n\ninstall.packages(\"bookdown\")\n\n# or the development version\n devtools::install_github(\"rstudio/bookdown\")\n\nRemember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #.\nMake sure to install the latest version of PIP R packages by typing the following\n\nipkg &lt;- utils::installed.packages()[,1]\n\npip_install &lt;- function(pkg, ipkg) {\n  if (isFALSE(pkg %in% ipkg)) {\n    gitcall &lt;- paste0(\"PIP-Technical-Team/\", pkg)\n    remotes::install_github(gitcall, dependencies = TRUE)\n    TRUE\n  } else {\n    FALSE\n  }\n}\n\npkgs &lt;- c(\"pipload\", \"pipaux\", \"wbpip\", \"piptb\", \"pipdm\", \"pipapi\")\n\n\npurrr::walk(pkgs, pip_install, ipkg = ipkg)"
  },
  {
    "objectID": "internal_workflow.html#the-github-group",
    "href": "internal_workflow.html#the-github-group",
    "title": "2  Internal Workflow",
    "section": "2.1 The Github group",
    "text": "2.1 The Github group\nThe technical code of the PIP project is organized in Git repositories in the Github group /PIP-Technical-Team. You need to be granted collaborator status in order to contribute to any of the repositories in the group. Also, many of the repositories do not play a direct role in the PIP technical workflow. Some of them are intended for documenting parts of the workflow or for testing purposes. For example, the repository of this book–/PIPmanual–is not part of the workflow of PIP, since it is not necessary for any estimation. Yet, you need get familiar with all the repositories in case you need to make a contribution in any of them. In this chapter, however, we will focus on understanding the repositories that affect directly the PIP workflow.\nFirst, we will see the overview of the workflow. It is an overview because each bucket of its buckets is a workflow in itself. Yet, it is important to have the overview clear in order to understand how and where all the pieces fall together. Then, we will unpack each of the workflow buckets to understand them in more detail."
  },
  {
    "objectID": "internal_workflow.html#overview",
    "href": "internal_workflow.html#overview",
    "title": "2  Internal Workflow",
    "section": "2.2 Overview",
    "text": "2.2 Overview\nThe workflow overview is mainly composed of four steps.\n\nData acquisition\nData preparation\npre-computed indicators\nAPI feeding\n\n\nEach of the steps (or buckets) is prerequisite of the next one, so if something changes in one of the them, it is necessary to execute the subsequent steps."
  },
  {
    "objectID": "internal_workflow.html#data-acquisition",
    "href": "internal_workflow.html#data-acquisition",
    "title": "2  Internal Workflow",
    "section": "2.3 Data acquisition",
    "text": "2.3 Data acquisition\nBefore understanding how the input data of PIP is acquired, we need to understand PIP data itself. The PIP is fed by two kinds of data: welfare data and auxiliary data.\nWelfare data refers to the data files that contain at least one welfare vector and one population expansion factor (i.e., weights) vector. These two variables are the minimum data necessary to estimate poverty and inequality measures.1 These files come in four varieties; microdata, group data, bin data, and synthetic data. The details of welfare can be found in Section @ref(welfare-data). Regardless of the variety of the data, all welfare data in PIP are gathered from the Global Monitoring Database, GMD. For a comprehensive explanation of how the household data are selected and obtained, you may check chapter Acquiring household survey dataof the methodological PIP manual.\nMicrodata is uploaded into the PRIMUS system by the regional teams of the Poverty Global Practice. To do so, each regional team has to follow the GMD guidelines, which are verified by the Stata command {primus}. Rarely. the {primus} command does NOT capture some potential errors in the data. More details in Section @ref(primus)\nAs of now (r format(Sys.Date(), \"%B %d, %Y\")), Group data is divided two: historical group data and new group data. Historical group data is organized and provided by the PovcalNet team, who sends it to the Poverty GP to be included in the datalibweb system. New group data is collected by the poverty economist of the corresponding country, who shares it with her regional focal team, who shares it with PovcalNet team. This new group data is organized and tested by the PovcalNet team and then send back to the poverty GP to be included in the datalibweb system.\nBin data refers to welfare data from countries for which there is no poverty economist. Most of these countries are developed countries such as Canada or Japan. The main characteristic of this data is that it is only available through the LISSY system of the LIS data center, which does not allow access to the entire microdata. Thus, we need to contract the microdata into 400 bins or quantiles. The code that gathers LIS data is available in the Github repository PovcalNet-Team/LIS_data.\nFinally, synthetic data, refers to simulated microdata from an statistical procedure. As of now (r format(Sys.Date(), \"%B %d, %Y\")), the these data are estimated using multiple imputation techniques, so that any calculation must take into account the imputation-id variable. The data is calculated by the poverty economist of the country and the organizes by the global team in the Poverty GP.\nAuxiliary data refers to all data necessary to temporally deflate and line up welfare data, with the objective of getting poverty estimates comparable over time, across countries, and, more importantly, being able to estimate regional and global estimates. Some of these data are national population, GDP, consumer price index, purchasing parity power, etc. Auxiliary data also include metadata such as time comparability or type of welfare aggregate. Since each measure of auxiliary data is acquired differently, all the details are explain in Section @ref(auxiliary-data)."
  },
  {
    "objectID": "internal_workflow.html#data-preparation",
    "href": "internal_workflow.html#data-preparation",
    "title": "2  Internal Workflow",
    "section": "2.4 Data preparation",
    "text": "2.4 Data preparation\nThis step assumes that all welfare data is properly organized in the datalibweb system and vetted in PRIMUS. In contrast to the previous global-poverty calculator system, PovcalNet, the PIP system only gathers welfare from the datalibweb server.\nThe welfare data preparation is done using the repository /pipdp. As of now (r format(Sys.Date(), \"%B %d, %Y\")), this part of the process has been coded in Stata, given that the there is no an R version of {dataliweb}. As for the auxiliary data preparation, it is done with package {pipaux}, available in the repository /pipaux. Right now the automation of updating the auxiliary has not been implemented. Thus, it has to be done manually by typing pipaux::pip_update_all_aux() to update all measures, or use function pipaux::update_aux() to update a particular measure."
  },
  {
    "objectID": "internal_workflow.html#pre-computed-indicators",
    "href": "internal_workflow.html#pre-computed-indicators",
    "title": "2  Internal Workflow",
    "section": "2.5 Pre-computed indicators",
    "text": "2.5 Pre-computed indicators\nAll measures in PIP that do not depend on the value of the poverty line are pre-computed in order to make the API more efficient and responsive. Some other indicators that not depend on the poverty lines but do depend on other parameters, like the societal poverty, are not included as part of the pre-computed indicators.\nThis step is executed in the repository /pip_ingestion_pipeline, which is a pipeline powered by the {targets} package. The process to estimate the pre-computed indicators is explained in detail in Chapter @ref(pcpipeline). The pipeline makes use of two R packages, {wbpip} and {pipdm}. The former is publicly available and contains all the technical and methodological procedures to estimate poverty and inequality measures at the country, regional, and global level. The latter, makes use of {wbpip} to execute the calculations and put the resulting data in order, ready to be ingested by the PIP API."
  },
  {
    "objectID": "internal_workflow.html#api-feeding",
    "href": "internal_workflow.html#api-feeding",
    "title": "2  Internal Workflow",
    "section": "2.6 API feeding",
    "text": "2.6 API feeding\nNOTE: Tony, please finish this section."
  },
  {
    "objectID": "internal_workflow.html#packages-interaction",
    "href": "internal_workflow.html#packages-interaction",
    "title": "2  Internal Workflow",
    "section": "2.7 Packages interaction",
    "text": "2.7 Packages interaction\nNOTE: Andres, finished this section."
  },
  {
    "objectID": "internal_workflow.html#footnotes",
    "href": "internal_workflow.html#footnotes",
    "title": "2  Internal Workflow",
    "section": "",
    "text": "Population data is also necessary when working with group data.↩︎"
  },
  {
    "objectID": "Folder_structure.html#pip-data",
    "href": "Folder_structure.html#pip-data",
    "title": "3  Folder Structure",
    "section": "3.1 PIP-Data",
    "text": "3.1 PIP-Data\nThis folder is no longer in use.\nThis was the original PIP_Data folder. It was used for testing and development of {pipdp} and {pipaux}, but has since been replaced by PIP-Data_QA and PIP_Data_Testing."
  },
  {
    "objectID": "Folder_structure.html#pip-data_qa",
    "href": "Folder_structure.html#pip-data_qa",
    "title": "3  Folder Structure",
    "section": "3.2 PIP-Data_QA",
    "text": "3.2 PIP-Data_QA\nThis is the main output directory for the {pipdp} and {pipaux} packages. It contains all the survey and auxiliary data needed to run the Poverty Calculator and Table Maker pipelines.\nNote that the contents of this folder is for QA and production purposes. Please use the PIP_Data_Testing directory, or your own personal testing directory, if you intend to test code changes in {pipdp} or {pipaux}.\n\n3.2.1 _aux\nThe _aux folder contains the auxiliary data used by the Poverty Calculator Pipeline and the {pipdp} and {pipapi} packages.\nPlease note the following:\n\nThe file sna/NAS special_2021-01-14.xlsx is currently hardcoded in {pipaux}. If the contents of the file changes this package might need to be updated.\nSome other National Accounts special cases (e.g. BLZ, VEN) are manually hardcoded in pipaux. Beware of this when updating GDP/PCE.\nThe file weo/weo_&lt;YYYY-MM-DD&gt;.xls needs to be manually downloaded from IMF, opened and then re-saved as weo_&lt;YYYY-MM-DD&gt;.xls.\nThe grouped data means currently come from the PovcalNet Masterfile. This should be changed when PovcalNet goes out of production.\n\nAn explanation of each subfolder is given below.\n\n\n\n\n\n\n\n\n\nFolder\nMeasure\nUsage\nSource\n\n\n\n\ncountries\nPIP country list\npipapi\npipaux\n\n\ncountry_list\nWDI country list\npipaux, PC pipeline\npipaux\n\n\ncp\nCountry profiles\npipai\npipaux\n\n\ncpi\nCPI\nPC pipeline\npipaux\n\n\ndlw\nDLW repository\npipdp\npipdp\n\n\ngdm\nGrouped data means\nPC pipeline\npipaux\n\n\ngdp\nGross Domestic Product\nPC pipeline\npipaux\n\n\nmaddison\nMaddison Project Data\npipaux\npipaux\n\n\nindicators\nIndicators master\npipapi, PC pipeline\nManual, pipaux\n\n\npce\nPrivate consumption\nPC pipeline\npipaux\n\n\npfw\nPrice Framework\npipdp, PC pipeline\npipaux\n\n\npl\nPoverty Lines\npipapi\npipaux\n\n\npop\nPopulation\nPC pipeline\npipaux\n\n\nppp\nPurchasing Power Parity\nPC pipeline\npipaux\n\n\nregions\nRegions\npipapi\npipaux\n\n\nsna\nSpecial National Account cases\npipaux\nManual\n\n\nweo\nWorld Economic Outlook (GDP)\npipaux\nIMF, pipaux\n\n\n\nThe data in the folders countries, regions, pl, cp and indicators are loaded into the Poverty Calculator pipeline, but that they are not used for any calculations or modified in any way, when being parsed through the pipeline. They are thus only listed with {pipapi} as their usage.\nIn contrast the measures CPI, GDP, PCE, POP and PPP are both used in the pre-calculations and transformed before being saved as pipeline outputs. So even though these measures are also available in the PIP PC API, the files at this stage only have the Poverty Calculator pipeline as their use case.\n\n\n3.2.2 _inventory\nThe _inventory folder contains the PIP inventory file created by pipload::pip_update_inventory(). It is important to update this file if the survey data has been updated.\n\n\n3.2.3 Country data\nThe folders with country codes as names, e.g AGO, contain survey data for each country. This is created by {pipdp}. The folder structure within each country folder follows roughly the same convention as used by DLW.\nThere will be one data file for each survey, based on DLW’s GMD module and the grouped data in the PovcalNet-drive. These are labelled with PC in their filenames. Additionally, if there is survey data available from DLW’s ALL module there will also be a dataset for the Table Maker, labelled with TB."
  },
  {
    "objectID": "Folder_structure.html#pip-data_extsol",
    "href": "Folder_structure.html#pip-data_extsol",
    "title": "3  Folder Structure",
    "section": "3.3 PIP-Data_ExtSOL",
    "text": "3.3 PIP-Data_ExtSOL\nThis folder is for external SOL application. This is the folder where the data for the externa SOL will be synced between the network drive and the storage in Azure. The synchronization will be done daily from the ITS side. There are two subfolders GMD-DLW and HFPS-COVID19.\n\nGMD-DLW: it is the folder for GMD data for both raw and harmonized GMD data.\nHFPS-COVID19: it is the folder for High Frequent Phone Survey data for both raw and harmonized data.\n\nThe structured folders for these two catalogs are standard likes the ones used in DLW system.\nThere are dofiles on the synchronization between GMD in DLW and GMD in SOL. Those files are “Data in SOL.xlsx” and “Convert GMD to GMD SOL.do”. Only data with clear and explicit data license will be uploaded in this folder.\nFor now, only “public data” is in this application. Public means any users can download and redistribute the data, and there is no login in the country NSO website or any condition/terms when downloading the data. Otherwise, there is an explicit agreement with NSO on the usage of the data for SOL – email: Microdata for an external Statistics Online (SOL) platform.\nAt the moment, we are working with Legal on the Custom license for the data license template where SOL will be mentioned explicitly. Once the Custom license is developed (after the launch of SOL) we can reach out to all countries to ask for permission to use and “limited redistribute” in the data in our secured platform.\n\nPlease do not touch or change any content in this folder without permission"
  },
  {
    "objectID": "Folder_structure.html#pip_data_testing",
    "href": "Folder_structure.html#pip_data_testing",
    "title": "3  Folder Structure",
    "section": "3.4 PIP_Data_Testing",
    "text": "3.4 PIP_Data_Testing\nThis folder is used as a testing directory for development of the {pipdp} and {pipaux} packages."
  },
  {
    "objectID": "Folder_structure.html#pip_data_vintage",
    "href": "Folder_structure.html#pip_data_vintage",
    "title": "3  Folder Structure",
    "section": "3.5 PIP_Data_Vintage",
    "text": "3.5 PIP_Data_Vintage\nThis folder is currently not in use."
  },
  {
    "objectID": "Folder_structure.html#pip_ingestion_pipeline",
    "href": "Folder_structure.html#pip_ingestion_pipeline",
    "title": "3  Folder Structure",
    "section": "3.6 pip_ingestion_pipeline",
    "text": "3.6 pip_ingestion_pipeline\nThis is the main output directory for the both the Poverty Calculator and Table Maker pipelines.\n\n3.6.1 pc_data\n\n\n\nFolder\nExplanation\n\n\n\n\n_targets\nTargets storage\n\n\ncache\nCleaned survey data\n\n\noutput\nPC pipline output directory\n\n\nvalidation\nValidation directory\n\n\n\n\n3.6.1.1 _targets\nThis folder contains the objects that are cached by {targets} when the Poverty Calculator Pipeline is run. It is located on the shared network drive so that the latest cached objects are available to all team members. Please note however that a shared _targets folder does entail some risks. It is very important that only one person runs the pipeline at a time. Concurrent runs against the same _targets store will wreak havoc. And needless to say; if you are developing new features or customising the pipeline use a different _targets store. (We should probably have a custom DEV or testing folder for this, similar to PIP_Data_Testing.)\n\n\n3.6.1.2 cache\nThe survey data output of {pipdp} cannot be directly used as input to the Poverty Calculator Pipeline. This is because the data needs to be cleaned before running the necessary pre-calculations for the PIP database. The cleaning currently includes removal of rows with missing welfare and weight values, standardising of grouped data, and restructuring of datasets with both income and consumption based welfare values in the same survey. Please refer to {wbpip} and {pipdm} for the latest cleaning procedures.\nIn order to avoid doing this cleaning every time the pipeline runs an intermediate version of the survey data is stored in the cache/clean_survey_data/ folder. Please make sure this folder is up-to-date before running the _targets.R pipeline.\nNote: This is not exactly true. We need to solve the issue with the double caching of survey data. But the essence of what we want to accomplish is to avoid cleaning and checking the survey data in every pipeline run..\n\n\n3.6.1.3 output\nThis folder contains the outputs of the Poverty Calculator Pipeline. This is seperated into three subfolders; aux containing the auxiliary data, estimations containing the survey, interpolated and distributional pre-calculated estimates, and survey_data containing the cleaned survey data.\n\n\n3.6.1.4 validation\nTBD Andres: Could you write this section?.\n\n\n\n3.6.2 tb_data\nTBD Andres: Could you write this section?."
  },
  {
    "objectID": "joining_data.html#pfw-join",
    "href": "joining_data.html#pfw-join",
    "title": "4  Joining Data",
    "section": "4.1 The Price FrameWork (pfw) data",
    "text": "4.1 The Price FrameWork (pfw) data\nAs always, this file can be loaded by typing,\n\nlibrary(data.table)\n#pfw &lt;- pipload::pip_load_aux(\"pfw\")\n#joyn::is_id(pfw, by = c(\"country_code\", \"surveyid_year\", \"survey_acronym\"))\n\nFirst of all, notice that pfw is uniquely identified by country code, survey year, and survey acronym. The reason for this is that pfw aims at providing a link between every single household survey and all the other auxiliary data. Since welfare data is stored following the naming convention of the International Household Survey Network (IHSN), data is stored according to country, survey year, acronym of the survey, and vintage control of master and alternative versions. The vintage control of the master and alternative version of the data is not relevant for joining data because PIP uses, by default, the most recent version.\nKeep in mind that PIP estimates are reported at the country, year, domain, and welfare type level, but the last two of these are not found either in the survey ID nor as unique identifiers of the pfw. To solve this problem, the pfw data makes use of the variables welfare_type, aaa_domain, and aaa_domain_var.\nAs the name suggests, welfare_type indicates the main welfare aggregate type (i.e, income or consumption ) of the variable welfare in the GMD datasets that correspond to the survey ID formed by concatenating variables country_code, surveyid_year, and survey_acronym. For example, the welfare_type of the welfare variable in the datasets of COL_2018_GEIH_V01_M_V03_A_GMD is income.\n\n# pfw[ country_code    == \"COL\"\n#     & surveyid_year  == 2018\n#     & survey_acronym == \"GEIH\", # Not necessary since it is the only one\n#     unique(welfare_type)]\n\nThe prefix aaa in variables aaa_domain and aaa_domain_var refers to the identification code of any of the auxiliary data. Thus, you will find a gdp_domain, cpi_domain, ppp_domain and several others. All aaa_domain variables contain the lower level of geographical disaggregation of the corresponding aaa auxiliary data. There are only three possible levels of disaagregation,\nAs of now, no survey or auxiliary data is broken down at level 3 (i.e., subnational), but it is important to know that the PIP internal code takes that possibility into account for future cases.\nDepending on the country, the domain level of each auxiliary data might be different. In Indonesia, for instance, the CPI domain is national, whereas the PPP domain is “urban/rural.”\n\n# pfw[ country_code == \"IDN\" & surveyid_year == 2018, \n#     .(cpi = unique(cpi_domain), \n#       ppp = unique(ppp_domain))]\n\nFinally, and this is really important, variables aaa_domain_var contains the name of variable in the GMD dataset that uniquely identify the household survey in the corresponding aaa auxiliary data. In other words, aaa_domain_var contains the name of the variable in GMD that must be used as key to join GMD to aaa. You may ask, does the name of the variable in the aaa auxiliary data have the same variable name in the GMD data specified in aaa_domain_var? No, it does not. Since the domain level to identify observations in the aaa auxiliary data is unique, there is only one variable in auxiliary data used to merge any welfare data, aaa_data_level. Since all this process is a little cumbersome, the {pipdp} Stata package, during the process of cleaning GMD databases to PIP databases, creates as many aaa_data_level variables as needed in order to make the join of welfare data and auxiliary data simpler. You can see the lines of code that create these variables in this section of the file “pipdp_md_clean.ado.”1\n\n4.1.1 Joining data example\nLet’s see the case of Indonesia above. The pfw says that the CPI domain is “national” and the PPP domain is “urban/rural.” That means that the welfare data join to each of these auxiliary data with two different variables,\n\n# domains &lt;- \n#   pfw[ country_code == \"IDN\" & surveyid_year == 2018, \n#      .(cpi = unique(cpi_domain_var), \n#        ppp = unique(ppp_domain_var))][]\n\nThis says that the name of the variable in the welfare data to join PPP data is called uban, but there is not seem to be a variable name in GMD to join the CPI data. When the name of the variable is missing, it indicates that the welfare data is not split by any variable to merge CPI data. That is, it is at the national level.\n\n# ccode  &lt;- \"CHN\"\n# cpi &lt;- pipload::pip_load_aux(\"cpi\")\n# ppp &lt;- pipload::pip_load_aux(\"ppp\")\n# \n# CHN &lt;-  pipload::pip_load_data(country = ccode, \n#                               year    = 2015)\n# \n# dt &lt;- joyn::merge(CHN, cpi,\n#                   by = c(\"country_code\", \"survey_year\",\n#                          \"survey_acronym\", \"cpi_data_level\"),\n#                   match_type = \"m:1\", \n#                   keep = \"left\")"
  },
  {
    "objectID": "joining_data.html#special-data-cases",
    "href": "joining_data.html#special-data-cases",
    "title": "4  Joining Data",
    "section": "4.2 Special data cases",
    "text": "4.2 Special data cases\nNOTE: Andres. Add this section"
  },
  {
    "objectID": "joining_data.html#footnotes",
    "href": "joining_data.html#footnotes",
    "title": "4  Joining Data",
    "section": "",
    "text": "You can find more information about the conversion from GMD to PIP databases in Section @ref(welfare-data)↩︎"
  },
  {
    "objectID": "price_framework.html#variables",
    "href": "price_framework.html#variables",
    "title": "5  Price FrameWork (pfw) data frame",
    "section": "5.1 Variables",
    "text": "5.1 Variables\nThe table provides a short description of each variable of the Price FrameWork data frame. We provide additional information for variable."
  },
  {
    "objectID": "price_framework.html#additional-explanation",
    "href": "price_framework.html#additional-explanation",
    "title": "5  Price FrameWork (pfw) data frame",
    "section": "5.2 Additional explanation",
    "text": "5.2 Additional explanation\n\nID vars\nVariables of identification are wb_region_code, pcn_region_code, country_code, ctryname, year, surveyid_year, survey_acronym.\nThe main difference between wb_region_code and pcn_region_code is that the former only include geographical regions internally used by the world bank, whereas the latter has an additional category, “OHI,” for Other High Income countries.\n\n# janitor::tabyl(pfw, wb_region_code, pcn_region_code)\n\nFor most of the data points, there should be no difference in between year and surveyid_year. In some extreme cases (not available at the moment), year could be different due to the reporting criteria from the NSO. For our purpose, we use rep_year as the reporting year in our system for integer ones, and ref_year for decimal reporting purpose\n\n\naltname\nAlternative survey name for some surveys.\n\n# head(pfw[altname != \"\", c(\"altname\", \"survey_acronym\")])\n\n\n\nsurvey_coverage\nThis variable represent the househol survey coverage. This is different from the disaggregation data level.\n\n# janitor::tabyl(pfw, survey_coverage)\n\n\n\nwelfare_type\nThis variable contains the welfare type of the main welfare aggregate variable in a survey in case it has more than one. The welfare type of alternative welfare aggregates is found in variable oth_welfare1_type.\n\n# janitor::tabyl(pfw, welfare_type)\n\n\n\nuse_imputed\nWhether the welfare aggregate has been imputed. There are just few countries with this kind of data.\n\n\n\n\n\nuse_microdata\nWhether the welfare aggregate vector used in PIP is directly extracted from microdata without any sort of aggregation. Mos of the countries have this kind of data. Below you can see those that do not.\n\n# pfw[use_microdata != 1, unique(country_code)]\n\n\n\nuse_bin\nWhether the welfare aggregate was aggregated to 400 bins from microdata before being incorporated to the PIP repository. This is the case of houshold surveys only available in the Luxembourg Data Center (LIS). Countries with bin data is considered micro data for technical purposes.\n\n# pfw[use_bin == 1, unique(country_code)]\n\n\n\nuse_groupdata\nWhether welfare aggregate comes from grouped data. Information about this type of data is available in [@dattComputationalToolsPoverty1998; @krauseCorrigendumEllipticalLorenz2013; @villasenorEllipticalLorenzCurves1989]. The following countries have this kind of data.\n\n# pfw[use_groupdata == 1, unique(country_code)]"
  },
  {
    "objectID": "PRIMUS.html#interacting-with-primus",
    "href": "PRIMUS.html#interacting-with-primus",
    "title": "6  PRIMUS",
    "section": "6.1 Interacting with PRIMUS",
    "text": "6.1 Interacting with PRIMUS\nThe interaction with PRIMUS is done through different systems, so it is best to begin by clarifying terms.\n\nWebsite platform\nPRIMUS can be accessed by typing primus/ in your browser. As long as you’re connected to the intranet it should work fine. However, if you have any issues connecting to the platform, please send an email to Minh Cong Nguyen, requesting access.\nEach database uploaded into PRIMUS gets a unique transaction ID. This ID is important because it is not unique to a dataset but unique to the transaction (or vintage of the data). That is, if one particular dataset is uploaded more than once, it will get two different transaction IDs. When talking to the Poverty GP, you better refer to the transaction ID rather than the survey (or at least both) because, though you may be talking about the same country/year, you are actually talking about two different transactions. See for instance Brazil 2013.\n\n\nStata command\nThe Poverty GP maintains the Stata repository worldbank/primus from which you can download the command primus. Right now, this is the official place from which you can access this command. From now on, each time we refer to the command, we use primus, whereas when we refer to the website, we use PRIMUS.\nPlease, make sure you have it properly installed in your computer, by following the instruction section @ref(stata-github). Basically, you need to install first the github Stata command by E. F. Haghish\n\n\n\nNow, you can install primus by just typing the following in Stata\n\n\n\nIn case this does not work, follow instructions in section @ref(stata-github) for alternative methods.\n\n\nCorrections to primus Stata command\nThe primus command is maintained by the Poverty GP, so we have no control over modifications or improvements. The best you can do in case you need to fix or modify something in this command is to fork the repository, clone the forked repo into your computer, check out a new branch, make any modification, and generate a pull request to the master branch of the original repository. Once you have done that, make sure to send an email with your suggestions for improvement to Ani Rudra Silwal, copying to the D4G Central Team (Nobuo Yoshida and Minh Cong Nguyen)."
  },
  {
    "objectID": "PRIMUS.html#understand-primus",
    "href": "PRIMUS.html#understand-primus",
    "title": "6  PRIMUS",
    "section": "6.2 Understanding PRIMUS",
    "text": "6.2 Understanding PRIMUS\nEach time a database is uploaded into PRIMUS, it is assigned a transaction ID. During the uploading process (or right after it has finished), the three parties–DECDG, DECRG, or the Poverty GP–evaluate the quality of the new or corrected data and approve them or reject them in the system. Depending on the decision of all the parties, each transaction will take one of three possible status, pending, approved, or rejected.\n\nAs of today (2020-11-20), there is no one who represents DECRG. So, the approving process might be different and it will need to be changed in the PRIMUS system. Please check.\n\nThe transaction ID is pending when at least one of the three parties (DECDG, DECRG, or the Poverty GP) has not approved it in the system. You can click on the check box PENDING in the PRIMUS website to see which surveys have such a status, or you can use the primus command list this,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the overall status of a transaction is independent from survey ID. Thus, it is possible to find several transactions for the same country and year. Indonesia 2017, for instance, has three transactions, two of them rejected and \n\n\n\n\n\n\n\n\n\n\n\nA transaction is rejected when at least one of the three parties rejected the database. Finally, a transaction is approved only when all three parties have approved it into the system.\n\nWe recommend you understand the basic functionality of the primus command by reading the help file (type help primus in Stata)."
  },
  {
    "objectID": "PRIMUS.html#checking-primus-estimates",
    "href": "PRIMUS.html#checking-primus-estimates",
    "title": "6  PRIMUS",
    "section": "6.3 Checking PRIMUS estimates",
    "text": "6.3 Checking PRIMUS estimates\nThe real first step to check the quality of the recently uploaded data into PRIMUS is to download the basic estimates of each data and compare them with our own. There is no need to calculate and compare all the estimates available in PRIMUS but the mean in PPP, the poverty headcount, and the Gini index.\nThe primus command allows us to download the estimates of each transaction, but it has to be done one by one. Fortunately, the pcn command downloads all the estimates of pending transactions for us and properly stores them in the folder p:\\01.PovcalNet\\03.QA\\02.PRIMUS\\pending\\ r emo::ji(\"tada\") r emo::ji(\"tada\") . You only need to type,\n\n\n\nIn addition, pcn checks the date for which you’re downloading the estimates and keeps only those transactions that have been uploaded for the next spring or annual-meetings release. For instance, assume that today, 2020-11-20, you want to see the estimates of pending transactions in PRIMUS. Since annual meetings take place around September, pcn assumes you are interested in the estimates for the Spring-meetings release, around March next year. Thus, it will filter the results from primus, keeping only those transactions that were uploaded from November 2020. Now it is likely that the PRIMUS system has not been opened for uploading new data in November, as it usually opens around December and July. Thus, it is likely that you will find and error saying There is no pending data in PRIMUS for the combination of country/years selected.\nYou can load the recently-downloaded estimates by typing,\n\n\n\nNow, you have to check whether the new estimates make sense. Once way to that is to follow this do-file, p:\\01.PovcalNet\\03.QA\\02.PRIMUS\\pending\\2020_SM\\estimates\\checks\\comparisons_wrk_data.do.\n\nYou do NOT need to check the estimates with the working data (wrk) as it is suggested in the do-file above. The PovcalNet System is now fully integrated with the datalibweb system, so the CPI, PPP, and microdata will be always the same. The best you can do at this stage is to make sure the estimates in PRIMUS make sense at the country level."
  },
  {
    "objectID": "PRIMUS.html#approve-primus",
    "href": "PRIMUS.html#approve-primus",
    "title": "6  PRIMUS",
    "section": "6.4 Confirming and approving data in PRIMUS",
    "text": "6.4 Confirming and approving data in PRIMUS\nOnce you have checked that the estimates of pending transactions make sense, you need to approve them. As explained in section @ref(understand-primus), the approval on PRIMUS requires the consent of three parties. The PovcalNet team had the responsibility to approve on behalf or two of them, DECDG and DECRG. This process can easily done with the code below, which can be found in this file, p:\\01.PovcalNet\\03.QA\\02.PRIMUS\\pending\\2020_SM\\approve\\primus_approve.do.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasically, this is what you need to do with this file.\n\nModify local excl in case you do not want to approve one or several countries.\nModify local filtdate in which you select the date from which you want to approve transactions.\nMake sure at least two people approve. One on behalf of “povcalnet” (which is the alias used for DECRG) and another on behalf of “decdg.”\nPRIMUS has a double-confirmation process, so you need to “confirm” and then “approve” the transaction. For that, you only need to change the option decision() from approved to confirmed.\n\nFor some unknown reason, the PRIMUS system did not accept the approval of some transactions. If this happens again, you need to talk to Minh Cong Nguyen, so he can do the approval manually."
  },
  {
    "objectID": "welfare_data.html#welfare-origin-of-data",
    "href": "welfare_data.html#welfare-origin-of-data",
    "title": "7  Welfare data",
    "section": "7.1 From Raw to GMD",
    "text": "7.1 From Raw to GMD\n\n7.1.1 Microdata\nThe Global Monitoring Database (GMD) is the World Bank’s repository of multitopic income and expenditure household surveys used to monitor global poverty and shared prosperity. The household survey data are typically collected by national statistical offices in each country, and then compiled, processed, and harmonized. The process of harmonization is done at the regional level by the regional statistical development teams. This is to ensure that the harmonization is done with best country knowledge from the country TTLs as well as to ensure consistency of country data overtime. Selected variables have been harmonized to the extent possible such that levels and trends in poverty and other key sociodemographic attributes can be reasonably compared across and within countries over time.\nThe regional harmonization process also follows the global harmonization guidelines for most of the variables in GMD. PovcalNet also contributed historical data from before 1990, and recent survey data from Luxemburg Income Studies (LIS).\nThe global update process is coordinated by the Data for Goals (D4G) team and supported by the six regional statistics teams in the Poverty and Equity Global Practice.\nThe original data files used for the regional harmonization and GMD harmonization should be cataloged and stored in the regional shared drive and the central Microdata Library catalog (http://microdatalib/). We assume that the depositing of the original microdata in the Microdata Library is a collective responsibility shared by all World Bank staff working in all the World Bank’s regions, irrespective of global practice, since the dialogue and data acquisition through the NSO and line ministries often take place in a decentralized manner.\n\n\n7.1.2 Group Data\nSome countries have very restrictive and confidential data sharing conditions. In such cases, country economist, in collaboration with the regional team and PIP team, would send a request to the country NSO for the group or “collapsed” data for the purpose of monitoring international poverty (SDG 1.1.1). Historically, the group data can be exported from national statistics reports with either data format of type 2 or type 5. In recent years, country economists would request the group data from NSO directly. For example, we request the group data - ventile (20) for China by urban and rural, type 2. For other countries, like high income countries, such as UEA (Country code ARE), NSO directly reach out to us in order to update SDG 1.1.1, in this case the group data is shared to us and we validate the data.\n\n\n7.1.3 Bin Data\nSome of the countries that are not available in DataLibWeb can be found in the repository of the LIS Cross-National Data Center (hereafter, LIS). Currently, PovcalNet uses LIS data for 8 high-income economies: AUS, CAN, DEU, ISR, JPN, KOR, TWN & USA, plus the Pre-EUSILC years (generally before 2002) of European Economies.\nLIS datasets cannot be downloaded in full; however, they provide a remote-execution system, LISSY, that allows us interact with their microdata without having access to the individual records. We have developed a set of Stata do-files to interact with LISSY and aggregate the welfare distribution of our countries of interest to 400 bins. Then, these data is organized locally and shared with the Poverty GP to be included in DataLibWeb as a collection independent from GPWG.\n\nThe LIS_data repository\nIn order to work with the LIS data you need to clone the repository PovcalNet-Team/LIS_data. You will find in there three folders, 00.LIS_output, 01.programs, and 02.data.\n\n\nInteracting with LISSY\nOpening an account in LISSY:\nTo interact with the LIS data you need to first register here, by first completing the LIS microdata User Registration Form, and then submitting it through the same website using your institutional e-mail account. Within a couple of days, you will receive an e-mail from LIS containing your username and password.\n\nYou do not get to choose your own username or password. LISdatacenter creates both for you and those won’t change in time. Make sure to save that e-mail and record that information for your future log-ins. Also, know that LISSY passwords expire each year on December 31st. While your password won’t change, it must be renewedhere after January 1st.\n\nInteracting with LISSY:\nTo get acquainted with LISSY’s interface, coding structure, database naming and variables available, and learn how to compute estimates within LISSY we highly recommend taking some time to review the tutorials and self-teaching materials. However, in order to update the 400 bins twice a year, Stata codes have been previously written, so you simply need to follow the 5 steps in the next section.\n\n\nGetting the 400 bins from LISSY\n1. log in Go to LIS main page, scroll down and click on the lock icon \n2. Provide info Feed the three drop-down menus on top of the platform with the following information:\n\nProject: LIS\nPackage: Stata\nSubject: (Choose a name Ex: “Bins #1 - Dec 2020”)\n\n\nThe LISSY platform cannot run the code for ALL surveys available at once. If you attempt to do so, your project will stop and you will receive an e-mail containing the text:\n#####################################################\n  Your job has been killed and will be not executed  \n#####################################################\nTo avoid this, we need to run the code in groups of 5 to 6 countries, depending on the amount of years in each of them. Currently, LIS has data for 52 countries (26 of them are in the EUSILC project, and 26 do not) which usually take approximately 10 rounds of this process.\n\n3. Add do-file Copy and paste the entire content of 01.LIS_400bins.do file into the the main large command window and update the locals in lines 23-24 with the LIS 2-letter acronyms of the group of 5-6 countries in each round.\n\n\n\n\n\nRemember to update the subject with each round (Ex: “Bins #2 - Dec 2020”) so you keep track of the number of output files. Be careful not to leave out [or repeat] any country in the process.\n\n4 Submit. Click on the green arrow icon to submit your project. You will get an e-mail within some minutes with your output. If the system kills your project, your group of countries was probably too large. Remove one country and try again.\n5. Retrieve results Copy the entire text in the output e-mail you receive for each round, open your notepad and paste. Save each round in the \\00.LIS_output folder. Save each text file with the name LISSY_Dec202@_#.txt, [where @ is the year and # the round]. Consistency with this naming format is important for the next step (02.LIS_organize_output.do file)\n\n\nFrom Text file to datalibweb structure\nWe now need to convert the the text files generated by the LISSY system to actual data suitable for datalibweb. This structure is suggested by the International Household Survey Network (IHSN). Once the data is saved in folder 00.LIS_output you need to execute the file 02.LIS_organize_output.do. This file created to be executed in just one go. However, it could be ran in sections taking advantage of the different frames along the code.\nBefore you execute this code, you need to ensure a few things,\n1. Get rcall working in your computer\nThe processing of the text files is not done anymore on Stata but in R. To avoid changing systems, we need to execute R code directly from Stata. In order to do this, you need to make sure to have install R in your computer and also the Stata command rcall. The do-file 02.LIS_organize_output.do will check if you have it installed and will install it for you in case it is not. However, you can run the lines below to make sure everything is working fine. Also, you can take a look at the help file of rcall to get familiar with it.\n\n\n\n\n\n\n\n\n\n\n\n2. Personal Drive\nMake sure to add your UPI to the appropriate sections it appears by typing disp lower(\"`c(username)'\") , following the example below,\n\n\n\n\n\n3. Directives of the code\nThis do-file works like an ado-file in the sense that the output depends on the value of some local macros,\n\n\n\n\n\nIf local update_surveynames is set to 1, the code will load the sheet LIS_survname from the the file 02.data/_aux/LIS datasets.xlsx and updated the file 02.data/_aux/LIS_survname.dta. If replace is set to 1, the code will replace any output with the same name. Otherwise, it will create a new vintage version if the two files are different. If they are not different, the code will do nothing. local p_drive_output_dir is deprecated, so you must leave it as 0.\n4. Pattern of the text files\nWhen the text files with the information from LIS are stored in 00.LIS_output, they should be stored in a systematic way so that they could be loaded and processed at the same time. This can be done by specifying in a matching regular expression in local pattern. For instance, all the files downloaded in December, 2020 could by loaded and processed using the directive, local pattern = \"LISSY_Dec2020.*txt\".\n5. Output\nWhen the do-file is concluded, it saves the file 02.data/create_dta_status.dta with the status of all the surveys processed.\n\n\nCompare new LIS data to Datalibweb inventory\nTo identify what data is new and what data has changed with respect to the one available in datalibweb, you need to execute do-file 03.LIS_compare_dlw.do. Again, this do-file is intended to be executed in one run, but you can do it in parts taking advantage of the different frames. At the end of the execution the file 02.data/comparison_results.dta is created. This file contains three important variables wf, wt, and gn, which correspond to the ration of welfare means, weight means, and Gini coefficient between the data in datalibweb and the data in the folder, p:/01.PovcalNet/03.QA/06.LIS/03.Vintage_control.\nYou should only send to the Poverty GP those surveys for which at least one of these three variables is different to 1.\n\n\nThe Excel file LIS datasets.xlsx\nWith each LIS data update performed, we must first identify from LIS the new surveys (countries and/or years) they had recently added. LIS send users e-mails informing about new datasets added, and also releases newsletters with this information.\nInside the 02.data folder of your LIS_data GIT repository you will find a _aux sub folder, and the LIS datasets.xlsx file placed in there. We must manually update the tab LIS_survname tab adding new rows to the sheet. All necessary information to fill up this metadata (household size, currency, etc.) can be found in METIS.\n\nACRONYMS: The column survey_acronym is created by us. If you come across a new survey for which an acronym has not been previously established, the rules applied in the past by the team were the following:\n\nAcronyms are created based on the ENGLISH name of the survey. (Ex: German Transfer Survey (Germany) is “GTS”, followed by the suffix -LIS; thus GTS-LIS.\nFor the surveys that were Microcensus, we created the acronym “MC”, and for Denmark’s Law Model, “LM”.\nAll acronyms are created in capital letters.\n\n\nFinally, while the survey names in METIS are in English, some of the acronyms in parenthesis are still in the original language. In those cases we translated them to English. For instance, the survey name “Household Budget Survey (BdF) (France)” from METIS was changed to “Household Budget Survey (HBS) (France)” in the column surveyname of the excel.\n\n\nPrepare data for the Poverty GP\nFinally, the do-file 04.Append_new_LIS_bases.do prepares the data to be shared with the Poverty GP. Note that this do-file ONLY appends the 400 bins data of surveys that are new and those where welfare changed, which are identified in the previous step /comparison_results.dta as those gn != 1.\nBefore running the code, make sure to change the output file name to the date of your update (last one saved was “LIS_bins_Dec_21_2020.dta”). The output is saved in P:\\01.PovcalNet\\03.QA\\06.LIS\\04.Share_with_GP.\nFinally, quickly prepare a short .dta file importing the metadata already created in the LIS_survname tab from the Excel, keeping ONLY the surveys of the append output you just run and send both files to Minh Cong Nguyen &lt;mnguyen3@worldbank.org&gt; from the Poverty GP.\n\n\n\n7.1.4 Synthetic Data\nSynthetic (or imputed) data is used in particular instances where household surveys do not collect income or expenditure information; or collect partial expenditures (i.e. for a small list of items and rotate across different households). In cases of that nature, country teams incur in efforts to impute the income or consumption welfare vectors. This exercise is based on the imputation of welfare from one survey to another. The imputation modeling is between the welfare, and household & individual characteristics from a previous household survey, where data on both is available. This exercise consists of simulating welfare information for each household in the survey several times based on different simulated coefficients from the imputation models. The resulting data is stacked together from each simulated set. Final data should follow the same format as the regular GPWG module with the exception of the variable “sim” (which indicates the number of simulations in the stacked database).\nFGT indicators are estimated in the same way as any other data; however, for sensitive distributional indicators, calculations are run separately for each simulation, then averaged across results to get the final indicator. This data will get a value of 1 in the “imputed” variable of the price framework database."
  },
  {
    "objectID": "welfare_data.html#from-gmd-to-pip",
    "href": "welfare_data.html#from-gmd-to-pip",
    "title": "7  Welfare data",
    "section": "7.2 From GMD to PIP ({pipdp} package)",
    "text": "7.2 From GMD to PIP ({pipdp} package)\nThe next step in the preperation of survey data for PIP is to leverage the {pipdp} package. This package retrives survey data from DLW and standardizes it into the format used by the PIP ingestion pipelines (Poverty Calculator Pipeline and Table Maker Pipeline).\n\n7.2.1 Requirements\n{pipdp} depends on the following Stata packages.\n\n\n\n\nYou will also need to have the World Bank {datalibweb} module and the PovcalNet internal {pcn} command installed.\n\n\n7.2.2 Installation\n\n\n\n\n\n\n\n\n\n7.2.3 Remote server\nIt is recommended to both use {pipdp} on the PovcalNet remote server (WBGMSDDG001). The phyiscal location of this server is much closer to the data storage so queries to Datalibweb, as well as read/write operations to the PIP shared network drive, will be much faster.\nThe only issue with using the server is that Datalibweb will require login credentials for each new Stata session. To avoid this it is highly recommended that users apply for direct access to Datalibweb. This will grant access to the Datalibweb file storage and enable the possiblity of using the files option in all datalibweb queries.\nIn order to use the files option you will also need to make some minor adjustments to your Datalibweb setup files. In particular the global root variable in C:/ado/personal/datalibweb/GMD.do needs to be specified correctly. This can be achived by copying datalibweb.ado and GMD.do in the _aux directory to their respective locations.\nSince WBGMSDDG001 is a shared server and the DLW settings thus can be reset, you will need to make sure that these settings are correct every time you use pipdp. The modified setup files will only change the behavior of Datalibweb for users that have direct access to the file storage. It will not affect other users. You could either check and copy the files manually or run the following from the command line.\n\n\n\n\nFor details on how to connect to WBGMSDDG001 see the Remote server connection section in the PovcalNet Internal Guidelines and Protocols.\n\n\n7.2.4 Usage\npipdp has two main commands, pipdp group to copy and prepare grouped data files from the PocvalNet shared network drive and pipdp micro to download and prepare micro datasets from Datalibweb. For examples on how to use these commands see the Usage section in the {pipdp}README.\n\n\n7.2.5 Preparing survey data for Poverty Calculator Pipeline\nFollow these steps when preparing survey data for the Poverty Calculator Pipeline\n\nMake sure the Price Framework file is up to date in the directory you are using.\n\n\n\n\n\n\n\n\nMake sure the Datalibweb repository file is up to date in directory you are using. Note that if you are running this code on the PovcalNet remote server you will be required to log in to DLW, regardless if you have direct access or not. This is because the underlying datalibweb, type(GMD) repo(create dlwrepo) command always sends queries over web. If you want you to avoid the login requirement you can conduct this specific step from your local machine.\n\n\n\n\n\n\nMake sure DLW setup files are up-to-date (needed for files option).\n\n\n\n\n\n\n\nUpdate surveys\n\n\n\n\n\n\n\n\nUpdate the PIP inventory"
  },
  {
    "objectID": "welfare_data.html#survey-id",
    "href": "welfare_data.html#survey-id",
    "title": "7  Welfare data",
    "section": "7.3 Survey ID nomenclature",
    "text": "7.3 Survey ID nomenclature\nAll household surveys in the PIP repository are stored following the naming convention of the International Household Survey Network (IHSN) for archiving and managing data. This structure can be generalized as follows:\nCCC_YYYY_SSSS_ vNN_M_vNN_A_TYPE_MODULE\nwhere,\n\nCCC refers to 3 letter ISO country code\nYYYY refers to survey year when data collection started\nSSSS refers to survey acronym (e.g., LSMS, CWIQ, HBS, etc.)\nvNN is the version vintage of the raw/master data if followed by an M or of the alternative/adapted data if followed by an A. The first version will always be v01, and when a newer version is available it should be named sequentially (e.g. v02, v03, etc.). Note that when a new version is available, the previous ones must still be kept.\nTYPE refers to the collection name. In the case of PIP data, the type is precisely, PIP, but be aware that there are several other types in the datalibweb collection. For instance, the Global Monitoring Database uses GMD; the South Asia region uses SARMD; or the LAC region uses SEDLAC.\nMODULE refers to the module of the collection. This part of the survey ID is only available at the file level, not at the folder (i.e, survey) level. Since the folder structure is created at the survey level, there is no place in the survey ID to include the module of the survey. However, within a single survey, you may find different modules, which are specified in the name of the file. In the case of PIP, the module of the survey is divided in two: the PIP tool and the GMD module. The PIP tool could be PC, TB, or SOL (forthcoming), which stand for one of the PIP systems, Poverty Calculator, Table Baker (or Maker), and Statistics OnLine. the GMD module, refers to the original module in the GMD collection, such as module ALL, or GPWG, HIST, or BIN.\n\nFor example, the most recent version of the harmonized Pakistani Household Survey of 2015, she would refer to the survey ID PAK_2015_PSLM_v01_M_v02_A_PIP. In this case, PSLM refers to the acronym of the Pakistan Social and Living Standards Measurement Survey. v01_M means that the version of the raw data has not changed since it was released and v02_A means that the most recent version of the alternative version is 02.2"
  },
  {
    "objectID": "welfare_data.html#survey-id-cache",
    "href": "welfare_data.html#survey-id-cache",
    "title": "7  Welfare data",
    "section": "7.4 Survey CACHE ID nomenclature",
    "text": "7.4 Survey CACHE ID nomenclature\nNOTE:Andres, add explanation here\n\nTesting tachyons colors\n\nThis is a test Royal blue color in line"
  },
  {
    "objectID": "auxiliary_data.html#population",
    "href": "auxiliary_data.html#population",
    "title": "8  Auxiliary data",
    "section": "8.1 Population",
    "text": "8.1 Population\n\n8.1.1 Original data\nEverything related to population data should be placed in the folder y:\\PIP-Data\\_aux\\pop\\. hereafter (./).\nThe population data come from one of two different sources. WDI or an internal file provided by a member of the DECDG team. Ideally, population data should be downloaded from WDI, but sometimes the most recent data available has not been uploaded yet, so it needs to be collected internally in DECDG. As of now (August 24, 2023), the DECDG focal point to provide the population data is Emi Suzuki. You just need to send her an email, and she will provide the data to you.\nIf the data is provided by DECDG, it should be stored in the folder ./raw_data. The original excel file must be placed without modification in the folder ./raw_data/original. Then, the file is copied again one level up into the folder ./raw_data with the name population_country_yyyy-mm-dd.xlsx where yyyy-mm-dd refers to the official release date of the population data. Notice that for countries PSE, KWT and SXM, some years of population data are missing in the DECDG main file and hence in WDI. Here we complement the main file with an additional file shared by Emi to assure complete coverage. This file contains historical data and will not need to be updated every year. This additional file has the name convention population_missing_yyyy-mm-dd.xlsx and should follow the same process as the population_country file. Once all the files and their corresponding place, you can update the ./pop.fst file by typing pipaux::pip_pop_update(src = \"decdg\").\nIf the data comes directly from WDI, you just need to update the file ./pop.fst by typing pipaux::pip_pop_update(src = \"wdi\"). It is worth mentioning that the population codes used in WDI are “SP.POP.TOTL”, “SP.RUR.TOTL”, and “SP.URB.TOTL”, which are total population, rural population, and urban population, respectively. If it is the case that PIP begins using subnational population, a new set of WDI codes should be added to the R script in pipaux::pip_pop_update().\n\n\n8.1.2 Data structure\nPopulation data is loaded by typing either pipload::pip_load_aux(\"pop\") or pipaux::pip_pop(\"load\"). We highly recommend the former, as {pipload} is the intended R package for loading any PIP data.\n\n# pop &lt;- pipload::pip_load_aux(\"pop\")\n# head(pop)"
  },
  {
    "objectID": "auxiliary_data.html#national-accounts",
    "href": "auxiliary_data.html#national-accounts",
    "title": "8  Auxiliary data",
    "section": "8.2 National Accounts",
    "text": "8.2 National Accounts\nNational accounts account for the economic development of a country at an aggregate or macroeconomic level. These measure are thus useful to interpolate or extrapolate microeconomic measures mean welfare aggregate or poverty headcount when household surveys are not available. National accounts work as a proxy of the economic development that would have been present if household surveys were available.\nThere are two main types of national accounts, Household Final Consumption Expenditure (HFCE) and Gross Domestic Product (GDP)—both in real per capita terms. Please refer to Section 5.3 of [@worldbankPovertyInequalityPlatform2021] to understand the usage of national accounts data.\n\n8.2.1 GDP\nAs explained in Section 5.3 of [@worldbankPovertyInequalityPlatform2021], there are three sources of GDP data, and one more for a few particular cases. The integration of all the sources of GDP data is performed by pipaux::pip_gdp_update(), you’ll need to manually download and store the data from WEO and the data for the special cases. The national accounts series from WDI are GDP per capita  [series code: NY.GDP.PCAP.KD]. These series are in constant 2010 US$.\nThe most recent version of the WEO data most be downloaded from the World Economic Outlook Databases of the IMF.org website and saved as an .xls file in &lt;maindir&gt;/_aux/weo/. The filename should be in the following structure WEO_&lt;YYYY-DD-MM&gt;.xls. Due to potential file corruption the file must be opened and re-saved before it can be updated with pip_gdp_weo(), which is an internal function fo pipaux::pip_gdp_update(). Hopefully in the future IMF will stop using an `.xls` file that’s not really xls.\n\n\n8.2.2 Consumption (PCE)\nPrivate Consumption Expenditure (pce) is gathered from WDI, with the exception of a few special cases. As in the case of GDP, the special cases are treated in the same way with PCE. You only need to execute the function pipaux::pip_pce_update() to update the PCE data. HFCE per capita [series code: NE.CON.PRVT.PC.KD] [@prydzNationalAccountsData2019]. These series are in constant 2010 US$.\n\n\n8.2.3 National Accounts, Special Cases\nSpecial national accounts are used for lining up poverty estimates in the following cases1:\n\nNational accounts data are unavailable in the latest version of WDI.\nIn such cases, national accounts data are obtained, in order of preference, from the latest version of WEO, or the latest version of MPD. For example, the entire series of GDP per capita for Taiwan, China and Somalia are missing in WDI, so WEO series are used instead.\nNational accounts data are incomplete in the latest version of WDI.\nThese are the cases where national accounts data are not available in WDI for some historical or recent years. In such cases, national accounts data in WDI are chained on backward or forward using growth rates from WEO or MPD, in that order. For example, GDP per capita for South Sudan (2016-2019) are based on the growth rate of GDP per capita from WEO. GDP per capita data for Liberia up to 1999 are based on the growth rate in GDP per capita from MPD.\nThe available national accounts data from official sources (e.g. WDI, WEO, MPD) are considered to have quality issues.\nThis is the case for Syria. Supplementary national accounts data are obtained from other sources, including research papers or national statistical offices. GDP per capita series for Syria (for 2010 through 2019) are from research papers—@kostialSyriaConflictEconomy2016Gobat (for 2011-2015) and @devadasGrowthWarSyria2019 (for 2016-2019)—and are chained on backward with growth rates in GDP per capita from WEO. See *y:/PIP-Data/_aux/sna/* for more details on how this is implemented.\nNational accounts data need to be adjusted for the purposes of global poverty monitoring.\nThis is the case for India. Growth rates in national accounts data for rural and urban India after 2014, precisely HFCE (or formerly PCE) per capita from WDI, are adjusted with a pass-through rate of 67%, as described in Section 5 of @castanedaaguilarSeptember2020PovcalNet2020. See *y:/PIP-Data/_aux/sna/NAS special_2021-01-14.csv* for more details on how this is implemented."
  },
  {
    "objectID": "auxiliary_data.html#cpi",
    "href": "auxiliary_data.html#cpi",
    "title": "8  Auxiliary data",
    "section": "8.3 CPI",
    "text": "8.3 CPI\n\n8.3.1 Raw data\nGeneral documents on the CPI source and the CPI frameworks are posted here. Yet, for more details, please refer to [@laknerConsumerPriceIndices2018b; @azevedoPricesUsedGlobal2018a].\nThere are three sources of CPI: IFS, WEO, and country team.\nIn general, the CPI data will be taken from the IMF International Financial Statistics (IFS). For the incoming update, about 2-3 months to the upload we initial the request to DECDG CPI team for the three series: annually, quarterly, monthly from IFS CPI database. The purpose of requesting DECDG CPI team is to ensure the same vintage will be updated in WDI later in the next update cycle. We would need the three series as for some countries we only have annually, and for other countries we could have up to monthly. This is also a check for us in checking the consistency of annual and monthly series. The monthly series will be used to construct annual and quarterly series, as that there are some inconsistences between the annual and monthly series in the IFS. For the exceptional countries, Poverty GP replaces data series based on previous consultations when there is no update or better information.\nSome countries also use National series which are not available from IFS – in this case we check with country poverty TTLs to provide the updated information, especially when we have a new survey for that country.\nIn some cases where the CPI value are missing for some sources but not other sources. This is especially true for the very old year where the data is available for one source, or very recent year where WEO has a projection on the CPI while it is not available in IFS. In those cases, we will follow the logic and method described in the “CPI source document” to ensure we have all CPI values for all data points.\nGlobal D4G team will prepare and send the CPI raw series as well as the weighted numbers for the current data points in the system. The data will be query by datalibweb. The following files will be added to the system each round:\n\n\n\n\n\n\n\nFile ane\nDescription\n\n\n\n\nFinal_CPI_PPP_to_be_used.dta\nfinal weighted CPI for poverty calculation\n\n\nYearly_CPI_Final.dta\nannual CPI – combined from different sources using chained\n\n\nmethods Yearly_CPI.dta\nannual CPI constructed from the monthly CPI\n\n\nYearly_CPI_Annual.dta\nannual CPI from the annual series\n\n\nQuarterly_CPI.dta\nquarterly CPI\n\n\nMonthly_CPI.dta\nmonthly CPI series\n\n\nWEO_Yearly_CPI.dta\nannual CPI from WEO\n\n\nSpecial_CPI_series.dta\nSpecial case of CPI (national source, imputation)\n\n\n\n\n\n8.3.2 Vintage control\nVintage control of the CPI data comes in a similar fashion as welfare data, CPI_vXX_M_vXX_A, where vXX_M refers to the version of the master or raw data, and vXX_A refers to the alternative version.\nEvery year, around November-December, PIP CPI data is updated with the most recent version of the IMF CPI data, which comes with information for the most recent year available and with changes/fixes/additions of previous years for each country. When this happens, the master version of the CPI ID is increased in one unit before the data is saved. As of today, the current ID is . If data is modified during the rolling of the year, then the alternative version of the CPI ID is increased in one unit.\n\n\n8.3.3 Data structure\nWhen you load CPI data using #pipload::pip_load_aux(\"cpi\"), the data you get has already been cleaned for being use in the PIP workflow, and it is slightly different from the original CPI data stored in datalibweb servers. That is, the way CPI data is used and referred to datalibweb is different from the way it is used in PIP even though they both achive the same purpose.\nThe most important variable in CPI data is, no surprisingly, cpi. This variable however, is not available in the original CPI data from dlw. The original name of this variable comes in the form cpiYYYY, where YYYY refers to the base year of the CPI, which in turn depends on the collection year of the PPP. Today, this variable is thus “.” The name of this variable is stored in the pipaux.cpivar object in the zzz.R file of the {pipaux} package. This will supdate the option getOption(\"pipaux.cpivar\"), guaranteing that pipaux::pip_cpi_update() uses the right variable when updating the CPI data.\nAnother important variable in CPI dataframe is change_cpiYYYY, where YYYY stands for the base year of the CPI. Since it version control of the CPI data does not depend on the individual changes in the CPI series of each country but on the release of new data by the IMF or by additional modifications by the Poverty GP, variable change_cpiYYYY tracks changes in the CPI at the country/year/survey with respect to the previous version. This is very useful when you need to identify changes in output measures like poverty rates that depend on deflation. One possible source of difference is the CPI and this variable will help you identify whether the number of interest has change because the CPI has changed."
  },
  {
    "objectID": "auxiliary_data.html#ppp",
    "href": "auxiliary_data.html#ppp",
    "title": "8  Auxiliary data",
    "section": "8.4 PPP",
    "text": "8.4 PPP\n\n8.4.1 Raw data\nThe PPP data is downloaded from ICP website for most of the countries (Ask ICP team for the link, outlier, countries with changes in currency). Often there is a GPWG working group to assess the PPP and its impacts on poverty. In this case, the team would determine the countries for which there is a need to impute the PPP value, either from the ICP model or the team model.\nAfter the validation and adjustment process, the PPP values are stored in the data file for all PPP rounds with vintage controls for each round.\nThe name of the variables in the wide-format file will follow the structure ppp_YYYY_vX_vY. Where, YYYY refers to the ICP round. vX refers to the version of the release, and vY refers to the adaptation of the release. So, v1 will be the original data, whereas v2 would be the first adaptation or estimates of the release.\n\nYYYY: refers to the ICP round.\nvX: refers to the version of the release.\nvY: refers to the adaptation of the release. So, v1 will be the original data, whereas v2 would be the first adaptation or estimates of the release.\n\n\n\n8.4.2 Data structure\nPPP data is available by typing, pipload::pip_load_aux(\"ppp\"). As expected, the data you get has already been cleaned for being use in the PIP workflow, and it is slightly different from the original PPP data stored in datalibweb servers. The most important difference between the PIP data frame and the datalibweb data frame is its rectangular structure. PIP data is in long format, whereas datalibweb data in wide format.\nThe reason for having PPP data in long format in PIP is that some countries, very few, use a different PPP year than the rest of the countries. Instead of using a different variable for the calculations of those specific countries, we use the same variable for all the countries but filter the corresponding observations for each country using metadata from the Price Framework database.\nThe PPP data is at the country/ppp year/data_level/release version/adapation version level. Yet, several filters most always be applied before this data can be used. Ultimately, the data frame should be at the country/data_level level to used properly. As a general rule, the filter must be done by selecting the most recent release_version and the most recent adaptation_version in each year. Then you can just filter by the PPP year you want to work with. In order to make this process even easier we have created variables ppp_default and ppp_default_by_year, which dummy variables to filter data. If you keep all observations that ppp_default == 1 you will get the current PPP used for all PIP calculations. If you use ppp_default_by_year == 1, you get the default version used in each PPP year. This is useful in case you want to make comparisons between PPP releases. This two variables are created in function pipaux::pip_ppp_clean() , in particular in these lines."
  },
  {
    "objectID": "auxiliary_data.html#price-framework-pfw",
    "href": "auxiliary_data.html#price-framework-pfw",
    "title": "8  Auxiliary data",
    "section": "8.5 Price FrameWork (PFW)",
    "text": "8.5 Price FrameWork (PFW)\nblah\n\n8.5.1 Original data\nasds"
  },
  {
    "objectID": "auxiliary_data.html#abbreviations",
    "href": "auxiliary_data.html#abbreviations",
    "title": "8  Auxiliary data",
    "section": "8.6 Abbreviations",
    "text": "8.6 Abbreviations\nHFCE – final consumption expenditure\nMDP – Maddison Project Database\nPCE – private consumption expenditure\nWDI – World Development Indicators\nWEO – World Economic Outlook"
  },
  {
    "objectID": "auxiliary_data.html#footnotes",
    "href": "auxiliary_data.html#footnotes",
    "title": "8  Auxiliary data",
    "section": "",
    "text": "The examples of special cases mentioned in this document are based on the March 2021 PovcalNet update.↩︎"
  },
  {
    "objectID": "pc_pipeline.html#folder-structure",
    "href": "pc_pipeline.html#folder-structure",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.1 Folder structure",
    "text": "9.1 Folder structure\nThe pipeline is hosted in the Github repository PIP-Technical-Team/pip_ingestion_pipeline. At the root of the repo you will find a series of files and folders.\n\n#&gt; +-- batch\n#&gt; +-- pip_ingestion_pipeline.Rproj\n#&gt; +-- R\n#&gt; +-- README.md\n#&gt; +-- renv\n#&gt; +-- renv.lock\n#&gt; +-- run.R\n#&gt; +-- _packages.R\n#&gt; \\-- _targets\n#&gt; \\-- _targets.R\n\n\nFolders\n\nR contains long R functions used during the pipeline\nbatch is a script for timing the execution of the pipeline. This folder should probably be removed\n_targets is a folder for all objects created during the pipeline. You don’t need to look inside as its content is managed by the targets package.\nrenv is a folder for reproducible environment.\n\n\n\nFiles\n\n_packages.R is created by targets::tar_renv(). Do not modify manually.\n_targets.R contains the pipeline. This is the most important file."
  },
  {
    "objectID": "pc_pipeline.html#prerequisites",
    "href": "pc_pipeline.html#prerequisites",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.2 Prerequisites",
    "text": "9.2 Prerequisites\nBefore you start working on the pipeline, you need to make sure to have the following PIP packages.\n\nNote 1: notice that instructions below contain suffixes like @development. These specify the branch of the particular package that you need to use. Ideally, all packages should use the master branch; however, that will only be possible until the end of the development process.\n\n\nNote 2: if you update any of the packages developed by the PIP team, make sure you always increased the version of the package using the function usethis::use_version(). Even if the change in the package is small, you need on increased the version of the package. Otherwise, {targets} won’t execute the sections of the pipeline that run the functions you changed.\n\n\nremotes::install_github(\"PIP-Technical-Team/pipdm@development\")\nremotes::install_github(\"PIP-Technical-Team/pipload@development\")\nremotes::install_github(\"PIP-Technical-Team/wbpip@halfmedian_spl\")\ninstall.packages(\"joyn\")\n\nIn case renv is not working for you, you may need to install all the packages mentioned in the _packages.R script at the root of the folder. Also, make sure to install the most recent version of targets and tarchetypes packages."
  },
  {
    "objectID": "pc_pipeline.html#structure-of-the-_targets.r-file",
    "href": "pc_pipeline.html#structure-of-the-_targets.r-file",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.3 Structure of the _targets.R file",
    "text": "9.3 Structure of the _targets.R file\nEven thought the pipeline script looks like a regular R script, it is structured in a specific way in order to make it work with the {targets} package. In fact, notice that it must be called _targets.R at the root of the project. It is highly recommended that you read the entire targets manual to fully understand how it works. We will often be referring to such manual in order to expand on any particular targets’ concept.\n\nStart up\nThe first part of the pipeline sets up the environment. The process involve,\n\nloading the {targets} and {tarchetypes} packages;\n\n\n\ncreating default values like directories, time stamps, survey and reference years boundaries, compression level of .fst files, etc.;\nexecuting tar_option_set() to set up an option in {targets}. packages and imports are two particularly important options to track changes in package dependencies. You can read more about it in the sections Loading and configuring R packages and Packages-based invalidation of the targets manual;\nattaching all the packages and functions of the project by running source('_packages.R') and source('R/_common.R').\n\n\n\nStep 1: small functions\nAccording to the section Functions in pipelines of the targets manual, it is recommend to only use functions rather than expressions during the executions. Presumably, the reason for this is that targets track changes in functions but not in expressions. Thus, this scripts section defines small functions that are executed along the pipeline. In the section above, the scripts source('R/_common.R') loads longer functions. Yet, keep in mind that the 'R/_common.R' was used in a previous version of the pipeline before {targets} was implemented. Now, most of the function in 'R/_common.R' are included in the {pipdm} package.\n\n\nStep 2: preparing the data\nThis section used to be longer in previous versions of the pipeline because it used to identify the auxiliary data, load the PIP microdata inventory, and create the cache files. It now only identifies the auxiliary data.\n\n\nStep 3: The actual pipeline\nAlthough better explained in the next section, the overall order of the pipeline is as follows:\n\nLoad all necessary data (that is, auxiliary data and inventories), and then create any cache fie that has not been created yet.\nCalculate means in LCU\nCrate deflated survey mean (DSM) table\nCalculate reference year table (aka., interpolated means table)\nCalculate distributional stats\nCreate output tables\n\njoin survey mean table with dist table\njoin reference year table with dist table\ncoverage table aggregate population at the regional level table\n\nClean and save."
  },
  {
    "objectID": "pc_pipeline.html#understanding-the-pipeline",
    "href": "pc_pipeline.html#understanding-the-pipeline",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.4 Understanding the pipeline",
    "text": "9.4 Understanding the pipeline\nWe must understand not only how the {targets} package works, but also how the targets of the Poverty Calculator Pipeline are created. For the former, you can read the targets manual. For the latter, we should start by making a distinction between the different types of targets.\nIn {targets} terminology, there are two kinds of targets, stems and branches. Stems are unitary targets. That is, for each target there is only one single R object. Branches, on the other hand, are targets that contain several objects or subtargets inside (you can learn more about them in the chapter Dynamic branching of the targets manual). We will see the use of this type of targets when we talk about the use of cache files.\n\nStem targets\nThere are two ways to create stem targets: either using tar_target() or using tar_map() from the {tarchetypes} package. The tar_map() function allows to create stem targets iteratively. See for instance the creation of targets for each auxiliary data:\n\ntar_map(\n  values = aux_tb, \n  names  = \"auxname\", \n  \n  # create dynamic name\n  tar_target(\n    aux_dir,\n    auxfiles, \n    format = \"file\"\n  ), \n  tar_target(\n    aux,\n    pipload::pip_load_aux(file_to_load = aux_dir,\n                          apply_label = FALSE)\n  )\n  \n)\n\ntar_map() takes the values in the data frame aux_tb created in Step 2: preparing the data and creates two type of targets. First, it creates the target aux_dir that contains the paths of the auxiliary files, which are available in the column auxfiles of aux_tb. This is done by creating an internal target within tar_map() and using the argument format = \"file\". This process lets {targets} know that we will have objects that are loaded from a file and are not created inside the pc pipeline.\nThen, tar_map() uses the the column auxname of aux_tb to name the targets that contain the auxiliary files. Each target will be prefixed by the word “aux”. This is why we had to add the argument file_to_load to pipload::pip_load_aux, so we can let {targets} know that the file paths defined in target aux_dir are used to create the targets prefixed with “aux”, which are the actual targets. For example, if I need to use the population data frame inside the pc pipeline, I’d use the target aux_pop, which had a corresponding file path in aux_dir. This way, if the original file referenced in aux_dir changes, all the targets that depend on aux_pop will be run again.\n\n\nBranches targets\nLet’s think of a branch target like…\nAs explained above, branch targets are targets made of many “subtargets” that follow a particular pattern. Most of the targets created in the pc pipeline are branch targets because we need to execute the same procedure in every cache file. This could have been done internally in a single one, but then we would lose the tracking features of {targets}. Additionally, we could have created a stem target for every cache file, result, and output file, but that would have been both impossible to visualize and more difficult to code. Hence, branch targets is the best option.\nThe following example illustrates how it works,\n\n# step A\ntar_target(\n  cache_inventory_dir, \n  cache_inventory_path(),\n  format = \"file\"\n),\n\n# step B\ntar_target(\n  cache_inventory, \n  {\n    x &lt;- fst::read_fst(cache_inventory_dir, \n                       as.data.table = TRUE)\n  },\n),\n\n# step C\ntar_target(cache_files,\n           get_cache_files(cache_inventory)),\n\n# step D\ntar_files(cache_dir, cache_files),\n\n# step E\ntar_target(cache, \n           fst::read_fst(path = cache_dir, \n                         as.data.table = TRUE), \n           pattern = map(cache_dir), \n           iteration = \"list\")\n\nThe code above illustrates several things. It is divided in steps, with the last step (step E) being the part of the code where the branch target is created. Yet, it is important to understand all the previous steps.\nIn step A we create target cache_inventory_dir, which is merely the file path that contains the cache inventory. Notice that it is returned by a function and not entered directly into the target. Since it is a file path, we need to add the argument format = \"file\" to let {targets} know that it is input data. In step B we load the cache inventory file into target cache_inventory by providing the target “path” that we created in step A. This file has several columns. One of them contains the file path of every single cache file in the PIP network drive. That single column is extracted from the cache inventory in step C. Then, in step D, each file path is declared as input, using the convenient function tar_files(), creating thus a new target, cache_dir. Finally, we create branch target cache with all the cache files by loading each file. To do this iteratively, we parse the cache_dir target to the path argument of the function fst::read_fst() and to the pattern = map() argument of the tar_target() function. At the very end, we need to specify that the output of the iteration is stored as a list, using the argument iteration = \"list\".\nThe basic logic of branch targets is that the vector or list to iterate through should be parsed to the function’s argument and to the pattern = map() argument of the tar_target() function. It is very similar to purrr::map()\n\nNote: if we are iterating through more than one vector or list, you need to (1) separate each of them by commas in the map() part of the argument (See example code below). (2) make sure all the vectors or lists have the same length. This is why we cannot remove NULL or NA values from any target. (3) make sure you do NOT sort any of the output targets as it will loose its correspondence with other targets.\n\n\n# Example of creating branch target using several lists to iterate through.\ntar_target(\n  name      = dl_dist_stats,\n  command   = db_compute_dist_stats(dt       = cache, \n                                    mean     = dl_mean, \n                                    pop      = aux_pop, \n                                    cache_id = cache_ids), \n  pattern   =  map(cache, dl_mean, cache_ids), \n  iteration = \"list\"\n)\n\n\n\nCreating the cache files\nThe following code illustrates the creation of cache files:\n\ntar_target(pipeline_inventory, {\n  x &lt;- pipdm::db_filter_inventory(\n    dt = pip_inventory,\n    pfw_table = aux_pfw)\n  \n  # Uncomment for specific countries\n  # x &lt;- x[country_code == 'IDN' & surveyid_year == 2015]\n}\n),\ntar_target(status_cache_files_creation, \n           pipdm::create_cache_file(\n             pipeline_inventory = pipeline_inventory,\n             pip_data_dir       = PIP_DATA_DIR,\n             tool               = \"PC\",\n             cache_svy_dir      = CACHE_SVY_DIR,\n             compress           = FST_COMP_LVL,\n             force              = TRUE,\n             verbose            = FALSE,\n             cpi_dt             = aux_cpi,\n             ppp_dt             = aux_ppp)\n)\n\nIt is important to understand this part of the pc pipeline thoroughly because the cache files used to be created in Step 2: preparing the data rather than here. Now, it has not only been integrated in the pc pipeline, but it is also possible to execute the creation of cache files independently from the rest of the pipeline, by following the instructions in [Executing the _targets.R file].\nThe first target, pipeline_inventory is just the inner join of the pip inventory dataset and the price framework (pfw) file, to make sure we only include what the pfw says. This data set also contains a lot of useful information to create the cache files. Note that the commented line in this target would filter the pipeline inventory to have only the information for IDN, 2015. If you need to update specific cache files, you must add the proper filtering condition there.\nIn the second target, status_cache_files_creation, you will create the cache files but notice that the returning value of the function pipdm::create_cache_file() is not the cache file per-se, but a list with the status of the creation process. If the creation of a particular file fails, it does not stop the iteration that creates all the cache files. At the end of the process, it returns a list with the creation status of each cache file. Notice that the function pipdm::create_cache_file() requires the CPI and the PPP auxiliary data. That is because the variable welfare_ppp, which is the welfare aggregate in 2011 PPP values, is added to the cache files. FInally, and more importantly, the argument force = TRUE ensures that even if the cache file already exists, it should be modified. This is important when you require additional features in the cache file from the then ones it currently has. If set to TRUE, it will replace any file in the network drive that is listed in pipeline_inventory. If set to FALSE, only the files that are in pipeline_inventory -but not in the cache folder- will be created. Use this option only when you need to add new features to all cache data, or when you are testing and only need a few surveys with the new features."
  },
  {
    "objectID": "pc_pipeline.html#understanding-pipdm-functions",
    "href": "pc_pipeline.html#understanding-pipdm-functions",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.5 Understanding {pipdm} functions",
    "text": "9.5 Understanding {pipdm} functions\nThe {pipdm} package is the backbone of the pc pipeline. It is in charge of executing the functions in {wbpip} and consolidate the new DataBases. This is why many of the functions in {pipdm} are prefixed with “db_”.\n\n9.5.1 Internal structure of {pipdm} functions\nThe main objective of {pipdm} is to execute the functions in {wbpip} to do the calculations and then build the data frames. As of today (2023-08-24), the process is a little intricate.\nLet’s take the example of estimating distributive measures in the pipeline. The image below shows that there are at least three intermediate function levels between the db_compute_dist_stats() function, which is directly executed in the pc pipeline, and the wbpip::md_compute_dist_stats(), which makes the calculations. Also, notice that the functions are very general in regards to the output. No higher level function is specific enough to retrieve only one measure, such as the Gini coefficient, or the median, or the quantiles of the distribution. If you need to add or modify one particular distributive measure, you must do it in functions inside wbpip::md_compute_dist_stats(), making sure the new output does not mess up the execution of any of the intermediate functions before the results get to db_compute_dist_stats().\n\nThis long chain of functions is inflexible and makes debugging very difficult. So, if you need to make any modification, first identify the chain of execution in each pipdm function you modify, and then make sure your changes do not affect the output format as it may break the execution chain. This is also a good example of why this structure needs to be improved.\n\n\n9.5.2 Updating {pipdm} (or any other PIP package)\nAs explained above, if you need to modify any function in pipdm or in wbpip, you need to make sure that the output does not conflict with the execution chain. Additionally, If you update any of the packages developed by the PIP team, make sure you always increased the version of the package using the function usethis::use_version(). Even if the change in the package is small, you need to increase the version of the package. Otherwise, {targets} won’t execute the sections of the pipeline that run the functions you changed. Finally as explained in the Prerequisites, if you are working on a branch different than master, make sure you install that version of the package before running the pipeline."
  },
  {
    "objectID": "pc_pipeline.html#executing-the-_targets.r-file",
    "href": "pc_pipeline.html#executing-the-_targets.r-file",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.6 Executing the _targets.R file",
    "text": "9.6 Executing the _targets.R file\nThe .Rprofile at the root of the directory makes sure that both {targets} and {tarchetypes} are loaded when the project is started. The whole pipeline execution might be very time consuming because it still needs to load all the data in the network drive. If you use a desktop remote connection the execution might be faster than running it locally, but it is still very time consuming. So, it is advisable to only execute the targets that are directly affected by your changes and manually check that everything looks ok. After that, you can execute the entire code confidently and leave it running overnight.\nIn order to execute the whole pipeline, you only need to type the directive tar_make() in the console. If you want to execute only one target, then type the name of the target in the same directive, e.g., tar_make(dl_dist_stats). Keep in mind that if the inputs of prior targets to the objective target have changed, those targets will be executed first."
  },
  {
    "objectID": "pc_pipeline.html#debugging",
    "href": "pc_pipeline.html#debugging",
    "title": "9  Poverty Calculator Pipeline (pre-computed estimations)",
    "section": "9.7 Debugging",
    "text": "9.7 Debugging\nDebugging in targets is not easy. Yet, there are two ways to do it. The first way is provided in the chapter Debugging of the Targets Manual. It provides clear instruction on how to debug while still being in the pipeline, but it could be the case that you don’t find this method flexible to dig deep enough into the problem. Alternatively, you could debug by stepping out of the pipeline a little bit and gain more flexibility, as described below.\nDebugging is needed in one of two cases: one, because you got an error when running the pipeline with tar_make() or, two, because your results are odd. In either case, you should probably have an idea–though not always–of where the problem is. If the problem is an error in the execution of the pipeline, {targets} printed messages are usually informative.\n\nDebugging stem targets\nLet’s see a simple example. Assume the problem is in the target dt_dist_stats, which is created by executing the function db_create_dist_table of the {pipdm} package. Since the problem is in there, all the targets and inputs necessary to create dt_dist_stats should be available in the _targets/ data store. So, you can load them using tar_load() and execute the function in debugging mode. Like this,\n\ntar_load(dl_dist_stats)\ntar_load(svy_mean_ppp_table)\ntar_load(cache_inventory)\n\ndebugonce(pipdm::db_create_dist_table)\npipdm::db_create_dist_table(\n  dl        = dl_dist_stats,\n  dsm_table = svy_mean_ppp_table, \n  crr_inv   = cache_inventory\n  )\n\nNote that you must use the :: because the environment in which {targets} runs is different from your global environment, in which you might not have attached all the libraries.\n\n\nDebugging branch targets\nThe challenge debugging branch targets is that if the problem is in a specific survey, you can’t access the “subtarget” using the survey ID, or something of that nature, because the name of the subtarget is created by {targets} using a random number. This requires a little more of work.\nImagine now that the distributive measures of IDN 2015 are wrong. You see the pipeline and notice that these calculations are executed in target dl_dist_stats, which is the branch target created over all the cache files! It would look like something like this:\n\ntar_target(\n  name      = dl_dist_stats,\n  command   = db_compute_dist_stats(dt       = cache, \n                                    mean     = dl_mean, \n                                    pop      = aux_pop, \n                                    cache_id = cache_ids), \n  pattern   =  map(cache, dl_mean, cache_ids), \n  iteration = \"list\"\n)\n\nIn order to find the problem in IDN 2015, this what you could do:\n\n# Load data\ndt &lt;- pipload::pip_load_cache(\"IDN\", 2015, \"PC\")\ntar_load(dl_mean)\ntar_load(cache_ids)\ntar_load(aux_pop)\n\n# Extract corresponding mean and cache ID\nidt      &lt;- which(cache_ids == unique(dt$cache_id))\ncache_id &lt;- cache_ids[idt]\nmean_i   &lt;- dl_mean[[idt]]\n\n# Esecute the function of interest\ndebugonce(pipdm:::compute_dist_stats)\nds &lt;- pipdm::db_compute_dist_stats(dt       = dt, \n                                   mean     = mean_i, \n                                   pop      = aux_pop, \n                                   cache_id = cache_id)\n\nFirst, you load all the inputs. Since target dl_mean is a relatively light object, we load it directly from the _targets/ data store. Targets cache_ids and aux_pop are data frames, not lists, so we also load them from memory. The microdata, however, is problematic because target cache, which is the one that is parsed to create the actual dl_dist_stata target, is a huge list with all the micro, grouped, and imputed data. The solution is then to load the data frame of interest, using either pipload or fst.\nSecondly, we need to filter the list dl_mean and the data frame cache_ids to parse only the information accepted by the pipdm::db_compute_dist_stats() function. This has to be done when debugging because in the actual target this is done iteratively in pattern   =  map(cache, dl_mean, cache_ids).\nFinally, you execute the function of interest. Notice something else. The target aux_pop is parsed as a single data frame because pipdm::db_compute_dist_stats() requires it that way. This is also one of the reasons why these functions in {pipdm} need some fixing and consistency in the format of the their inputs."
  },
  {
    "objectID": "tm_pipeline.html",
    "href": "tm_pipeline.html",
    "title": "10  Table Maker Pipeline",
    "section": "",
    "text": "NOTE:Andres, write this chapter when the pipeline is done"
  },
  {
    "objectID": "load_md_aux.html#auxiilary-data",
    "href": "load_md_aux.html#auxiilary-data",
    "title": "11  Load microdata and Auxiliary data",
    "section": "11.1 Auxiilary data",
    "text": "11.1 Auxiilary data\nEven though pipaux has more than functions, most of its features can be executed by using only the pipaux::load_aux and pipaux::update_aux functions.\n\n11.1.1 udpate data\nthe main function of the pipaux package is udpate_aux. The first argument of this function is measure and it refers to the measure data to be loaded. The measures available are ****.\n\n# pipaux::update_aux(measure = \"cpi\")\n\n\n\n11.1.2 Load data\nLoading auxiliary data is the job of the package pipload through the function pipload::pip_load_aux(), though pipaux also provides pipaux::load_aux() for the same purpose. Notice that, though both function do exactly the same, the loading function from pipload has the prefix pip_ to distinguish it from the one in pipaux. However, we are going to limit the work of pipaux to update auxiliary data and the work of pipload to load data. Thus, all the examples below use pipload for loading either microdata or auxiliary data.\n\n# df &lt;- pipload::pip_load_aux(measure = \"cpi\")\n# head(df)"
  },
  {
    "objectID": "load_md_aux.html#microdata",
    "href": "load_md_aux.html#microdata",
    "title": "11  Load microdata and Auxiliary data",
    "section": "11.2 Microdata",
    "text": "11.2 Microdata\nLoading PIP microdata is the most practical action in the pipload package. However, it is important to understand the logic of microdata.\nPIP microdata has several characteristics,\n\nThere could be more than once survey for each Country/Year. This happens when there are more than one welfare variable available such as income and consumption.\nSome countries, like Mexico, have the two different welfare types in the same survey for the same country/year. This add a layer of complexity when the objective is to known which is default one.\nThere are multiple version of the same harmonized survey. These version are organized in a two-type vintage control. It is possible to have a new version of the data because the Raw data–the one provided by the official NSO–has been updated, or because there has been un update in the harmonization process.\nEach survey could be use for more than one analytic tool in PIP (e.g., Poverty Calculator, Table Maker, or SOL). Thus, the data to be loaded depends on the tool in which it is going to be used.\n\nThus, in order to make the process of finding and loading data efficiently, pipload is a three-step process.\n\n11.2.1 Inventory file\nThe inventory file resides in y:/PIP-Data/_inventory/inventory.fst. This file is a data frame with all the microdata available in the PIP structure. It has two main variables, orig and filename. The former refers to the full directory path of the database, whereas the latter is only the file name. the other variables in this data frame are derived from these two.\nThe inventory file is used to speed up the file searching process in pipload. In previous packages, each time the user wanted to find a particular data base, it was necessary to look into the folder structure and extract the name of all the file that meet a particular criteria. This is time-consuming and inefficient. The advantage of this method though, is that, by construction, it finds all the the data available. By contrast, the inventory file method is much faster than the “searching” method, as it only requires to load a light file with all the data available, filter the data, and return the required information. The drawback, however, is that it needs to be kept up to date as data changes constantly.\nTo update the inventory file, you need to use the function pip_update_inventory. If you don’t provide any argument, it will update the whole inventory, which may take around 10 to 15 min–the function will warn you about it. By provide the country/ies you want to update, the process is way faster.\n\n# # update one country\n# pip_update_inventory(\"MEX\")\n# \n# # Load inventory file\n# df &lt;- pip_load_inventory()\n# head(df[, \"filename\"])\n\n\n\n11.2.2 Finding data\nEvery dataset in the PIP microdata repository is identified by seven variables! Country code, survey year, survey acronym, master version, alternative version, tool, and source. So giving the user the responsibility to know all the different combinations of each file is a heavy burden. Thus, the data finder, pip_find_data(), will provide the names of all the files available that meet the criteria in the arguments provided by the user. For instance, if the use wants to know the all the file available for Paraguay, we could type,\n\n# pip_find_data(country = \"PRY\")[[\"filename\"]]\n\nYet, if the user need to be more precise in its request, she can add information to the different arguments of the function. For example, this is data available in 2012,\n\n# pip_find_data(country = \"PRY\", \n#               year = 2012)[[\"filename\"]]\n\n\n\n11.2.3 Loading data\nFunction pip_load_data takes care of loading the data. The very first instruction within pip_load_data is to find the data avialable in the repository by using pip_load_inventory(). The difference however is two-fold. First, pip_load_data will load the default and/or most recent version of the country/year combination available. Second, it gives the user the possibility to load different datasets in either list or dataframe form. For instance, if the user wants to load the Paraguay data in 2014 and 2015 used in the Poverty Calculator tool, she may type,\n\n\n# df &lt;- pip_load_data(country = \"PRY\",\n#                     year    = c(2014, 2015), \n#                     tool    = \"PC\")\n# \n# janitor::tabyl(df, survey_id)"
  },
  {
    "objectID": "docker.html#introduction",
    "href": "docker.html#introduction",
    "title": "12  Docker Container",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nThis chapter explains how to develop and run the Docker container for the Poverty Calculator API on local World Bank laptops. For DEV, QA and Production deployments please see the chapter on Deploying to Azure. For the Table Maker API please refer to TBD."
  },
  {
    "objectID": "docker.html#prerequisites",
    "href": "docker.html#prerequisites",
    "title": "12  Docker Container",
    "section": "12.2 Prerequisites",
    "text": "12.2 Prerequisites\n\nLocal admin account\nDocker Desktop for Windows\nWSL 2 (recommended)\nVisual Studio Code (recommended)\n\nSee below for recommendations on how to install Docker Desktop and WSL 2.\nVisual Code is not strictly needed, but the VS Docker plugin is one of the best tools to interact with Docker."
  },
  {
    "objectID": "docker.html#docker-on-world-bank-laptops",
    "href": "docker.html#docker-on-world-bank-laptops",
    "title": "12  Docker Container",
    "section": "12.3 Docker on World Bank laptops",
    "text": "12.3 Docker on World Bank laptops\n\n12.3.1 Install Docker\n\nInstall Docker Dekstop (v. 3.6.0 or later)\n\nRemember to check the box to enable WSL integration.\n\nActivate WSL\n\nCheck if the WSL feature on Windows is activated (e.g. run wsl --help from Powershell). If it is then proceed to step 3. Note: If you get a message saying “service cannot be started” try restarting WSL like suggested below under Known problems and solutions.\nGo to Control Panel -&gt; Programs -&gt; Turn Windows features on or off and check the box Windows Subsystem for Linux, or follow these steps if you prefer to activate WSL from the command line.\nReboot system.\n\nInstall WSL 2\n\nDownload and install the Linux kernel update package.\nSet WSL 2 as your default version (run wsl --set-default-version 2 from Powershell).\n\nConfigure Docker access privileges\n\nIn order to run Docker wo/ admin privileges you need to add yourself to the “docker-users” group.\nOpen Computer Management. Go to Local User and Groups -&gt; Groups -&gt; docker-users and add your regular account (WB/wb&lt;UPI&gt;) to the list of users.\nReboot system.\n\nStart Docker Desktop and enjoy your hard work :-)\n\n\n\n12.3.2 Known problems and solutions\nDocker Dekstop sometimes fails with the following error.\nSystem.InvalidOperationException:\nFailed to deploy distro docker-desktop to C:\\Users\\&lt;User&gt;\\AppData\\Local\\Docker\\wsl\\distro: exit code: -1\nstdout: Error: 0xffffffff\nThe root cause of this problem is likely that the World Bank VPN connection modifies the same network configuration as the wsl VM. Turning off your VPN when working with Docker locally is thus a good idea.\nA temporary solution to the issue is to open a CMD shell in Admin mode, and then run the following code to restart LxssManager .\n&gt; sc config LxssManager start=auto\n[SC] ChangeServiceConfig SUCCESS\nAfter this you will also need to restart Docker Desktop.\n\n\n12.3.3 Tips and tricks\n\nStart your Docker working day by opening a CMD shell in admin-mode and run sc config LxssManager start=auto. Keep the shell open because you might need to re-run the command if WSL crashes.\nTurn off your VPN when using Docker. This avoids the issue with WSL crashing from time to time. It still might happen though if the VPN switces on automatically or if you for other reasons switch back and forth.\nIt is possible that solutions like wsl-vpnkit and vpnkit will help with the WSL issues, but these have not been tested yet. Feel free to give it a try yourself.\nIf WSL causes too much pain, try switching to the legacy Hyper-V backend. (Go to Docker Desktop -&gt; General Setting -&gt; Use the WSL 2 based engine).\nRe-build an image from a partial cache by adding a ARG or ENV variable right before the layer you want to invalidate (yes, this is hacky but it works). E.g. do something like:\n\n# Install PIP specific R packages \nENV test=test06302021\nRUN Rscript -e \"remotes::install_github('PIP-Technical-Team/wbpip@master', repos = '${CRAN}')\"\nRUN Rscript -e \"remotes::install_github('PIP-Technical-Team/pipapi@master', repos = '${CRAN}')\""
  },
  {
    "objectID": "docker.html#create-volume",
    "href": "docker.html#create-volume",
    "title": "12  Docker Container",
    "section": "12.4 Create volume",
    "text": "12.4 Create volume\nThe data that is used in the poverty calculations is stored outside the Docker container. We therefore need to link a data source on our local machine to the container when we run it. This could be accomplished with a bind mount, but the preferred way is to use a named volume.\nYou can create and populate a volume by following the steps below. First we create an empty volume, then we link this to a temporay container, before using docker cp to copy the files to the container and volume. After the data has been copied you can discard the temporary container. The choice of Ubuntu 20.04 for the temporary container is arbitrary. You could use another image if you want.\n# Create volume \ndocker volume create pip-vol\n# Mount volume to tmp container  \ndocker run -d --name pip-data --mount type=volume,source=pip-vol,target=/data ubuntu:20.04\n# Copy data to container (and volume)\ndocker cp &lt;data-folder&gt;/. pip-data:data\n# Stop and remove tmp container \ndocker stop pip-data \ndocker rm pip-data\nFor some purposes you will only need to create this volume once, but remember to update the contents of the volume in case the data structure changes or you want to make sure the container is running against the latest available data. See the Azure DevOps Data repo (DEV branch) for the latest updates.\nThe volume can be inspected with the docker inspect and docker system commands. If you are running Docker Desktop on Windows with WSL 2.0 then then the physical location of Docker volumes is usually found under \\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes. For more information on volumes see the Use volumes section in Docker’s reference manual."
  },
  {
    "objectID": "docker.html#build-image",
    "href": "docker.html#build-image",
    "title": "12  Docker Container",
    "section": "12.5 Build image",
    "text": "12.5 Build image\nDocker images are built using the docker build command. Here we build the image and set the image name to pip-api.\ndocker build -t pip-api .\nYou can also set a specific tag for the image. For example you can specify the version number or the base image used as source. How you decide to tag your images is up to you. Often a tag of latest (the default) will suffice, but in other instances keeping track of different versions of your development image will be useful.\n# Set tag when building\ndocker build -t pip-api:0.0.4\n# Set tag after build (version number) \ndocker image tag pip-api:latest pip-api:0.0.4\n# Set tag after build (base image)\ndocker image tag pip-api:latest pip-api:ubi8 \nNote that each layer in a Dockerfile is cached when built. These layers won’t be re-run unless the code to produce that specific layer or a previous layer in the Dockerfile changes. This is handy for fast iterations, but can also cause the image to be outdated.\nIf needed you can use the --no-cache flag to force a re-build. This is useful when you want to make sure that the latest Linux updates are installed or for updating the {wbpip} and {pipapi} R packages. See also the Tips and Tricks section on how to do partial re-builds.\ndocker build --no-cache -t pip-api ."
  },
  {
    "objectID": "docker.html#run-container",
    "href": "docker.html#run-container",
    "title": "12  Docker Container",
    "section": "12.6 Run container",
    "text": "12.6 Run container\nRun the container by exposing port 80 and mounting a volume with the survey and auxiliary data. The data structure in the attached volume must correspond to the sub-folder specifications in R/main.R.\ndocker run --rm -d -p 80:80/tcp --mount src=pip-vol,target=/ipp,type=volume,readonly --name pip-api pip-api\nYou can also run the container with a bind mount if you prefer, but note that reading the files inside mount folder will be slower then with a volumne setup.\ndocker run --rm -d -p 80:80/tcp --mount src=\"&lt;data-folder&gt;\",target=/data,type=bind --name pip-api pip-api\nNavigate to http://localhost/__docs__/ to see the running API.\nFor details on the docker run command and its options see the Docker Docs."
  },
  {
    "objectID": "docker.html#debugging",
    "href": "docker.html#debugging",
    "title": "12  Docker Container",
    "section": "12.7 Debugging",
    "text": "12.7 Debugging\nRun container interactively:\nRun the container in interactive mode, using the -it flag, to see output messages.\n$ docker run --rm -it -p 80:80/tcp --mount src=pip-vol,target=/ipp,type=volume,readonly --name pip-api pip-api\nRunning plumber API at http://0.0.0.0:80\nRunning swagger Docs at http://127.0.0.1:80/__docs__/\nInspect container:\nFor development purposes it can be useful to inspect the Docker container. Luckily it is very easy to enter a running container with the docker exec command.\n# Enter the container as default user \n$ docker exec -it pip-api /bin/bash\nplumber@bd8ea77299ca:/$ \n# Enter the container as root user\n$ docker exec -it -u 0 pip-api /bin/bash\nroot@bd8ea77299ca:/$"
  },
  {
    "objectID": "docker.html#security",
    "href": "docker.html#security",
    "title": "12  Docker Container",
    "section": "12.8 Security",
    "text": "12.8 Security\nSince the PIP Techincal Team is developing both the API and the Docker container, we also have a larger responsibility for security. When deploying to Azure the image will need to go through a security scan, provided by Aquasec, that is generally quite strict. Any high level vulnerability found by the Aquasec scan will result in a failed deployment. It is not possible to leverage Aquasec on local machines, but we can get an indication of potential vulnerabilites by taking advantage of the built in security scan that comes with Docker. Additionally it is also possible to scan the contents of the R packages used in the image.\nNote that neither the Snyk or Oyster scans described below are requirements from OIS. They are only included here as additional and preliminary tools.\nRun image security scan:\nBefore deploying to Azure you can run preliminary security scans with Snyk. See Vulnerability scanning for Docker local images and Docker Security Scanning Guide 2021 for details.\ndocker scan --file .\\Dockerfile pip-api\nIt is important to note that the Aquasec scan that runs on Azure is different that the Snyk scan, and can detect different vulnerabilites. Even though few or zero vulnerabilites are found by Snyk, further issues might still be detected by Aquasec.\nRun security scan for R packages:\nYou can scan the R packages inside the container with the {oysteR} package, which scans R projects against insecure dependencies using the OSS Index.\nFirst log into a running container as root, and start R.\n$ docker exec -it -u 0 pip-api /bin/bash\nroot@ab54fbf1eb36:/# R \nThen install the {oyster} package, and run an audit.\ninstall.packages(\"oysteR\")\nlibrary(\"oysteR\")\naudit &lt;- audit_installed_r_pkgs()\nget_vulnerabilities(audit)\nNote that finding zero vulnerabilities is no guarantee against threats. For example is scanning of C++ code inside R packages not yet implemented. For more details and latest developments on the {oyster} package see the README on Github."
  },
  {
    "objectID": "docker.html#base-image-options",
    "href": "docker.html#base-image-options",
    "title": "12  Docker Container",
    "section": "12.9 Base image options",
    "text": "12.9 Base image options\nIn the development process of the Dockerfile there have been used different base images.\nThe current image in the main branch is based on ubi8, but other options (rocker, centos8) are available in seperate branches. These seperate branches will not be actively maintained, but should be kept in case there is a need to switch to another base image in the future.\nThe main need to switch base image is likely to stem from the Azure DevOps security scan. For example do images based on rocker or centos8 work well locally, but neither will pass deployment. CentOS images are unfortunately not on the list of approved images by OIS, while Rocker images (which are based on Ubuntu 20.04 LTS) currently has a vulnerability (CVE-2019-18276) that is classified as a high level threat by Aqua Scan.\nIn fact one of the Linux dependencies for R on UBI 8, libX11, also currently trigger a high level vulnerability (CVE-2020-14363) in Aqua Scan. Even though this issue has been solved in the offical RHEL 8 release, the issue still persist for UBI 8. It can however be solved by manually downloading a newer version of libX11 (1.6.12 or later), and removing the problematic version (1.6.8).\nIn case there is any need to develop with other base images in the future it is strongly recommended to start with one of the Linux distributions where RStudio provides precompiled binaries for R. This avoids the need to install R from source."
  },
  {
    "objectID": "docker.html#using-r-on-linux",
    "href": "docker.html#using-r-on-linux",
    "title": "12  Docker Container",
    "section": "12.10 Using R on Linux",
    "text": "12.10 Using R on Linux\nDocker images are usually based on Linux containers. Some familiarity with Linux is thus necessary in order to develop and work with Docker.\nR users in particular should familiarize themselves with the difference between installing R and R packages from source vs binary, as well as potential Linux dependencies that might be nessecary to make a specific package work.\nInstalling R:\nInstalling R from source is possible on most Linux distributions, but this not always an easy endavour. It will often require you to maintain a list of the nessecary Linux dependencies for R. Installing from source is also much slower than installing from a binary version.\nAnother way to install R on Linux is to use the available pre-compiled binary in your Linux distro’s repository (e.g sudo apt install r-base on Ubuntu). The problem with this approach is that the latest version of R might not yet be available, and for some Linux distro’s there will also only be a limited number of R versions available. This impedes the flexilibility needed for a production grade project.\nThe recommended approach is thus to rely on RStudio’s precompiled binaries. These binaries are available for the most common Linux distributions, including Ubuntu, Debian and CentOS / RHEL. Relying on these binaries will limited the number of base image options avaiable, e.g. a ligth-weight distro like Alpine is not supported, nor is the newest release of Ubuntu (Hirsute Hippo). But it will make much easier to maintain, upgrade and change the R version used in the project.\nInstalling R packages:\nAs a contrast to Windows and macOS, CRAN only supports source versions of the R packages available for Linux. This will significantly slow down the installation process of each pacakge.\nLuckily RStudio provides binary versions of all packages on CRAN for a series of different Linux distro’s through their Public Pacakge Manager. It is therefore strongly recommended that you rely on RSPM as your installation repo.\nAn additional benefit of RSPM is that you can also select a specific date for the package versions to use.\nOn the RSPM website go the Setup page on the main menu and select the appropiate client OS and date. You should then see a URL similar too https://packagemanager.rstudio.com/all/__linux__/centos8/4743918, where centos8 represents the distro and 4743918 the date.\nFor a more detailed introduction to the difference between binary and source packages, see Package structure and state in the book R Packages.\nInstalling R package dependencies:\nLinux distro’s doesn’t come with the dependencies needed to work with all R packages. This includes common packages like {data.table} and {plumber}. It is important that such dependencies are installed prior to the installation of your R packages.\nThe best way to look for dependencies is to use the RSPM website and search for the specific package in question. Remeber to also select the correct Linux distribution."
  },
  {
    "objectID": "docker.html#resources",
    "href": "docker.html#resources",
    "title": "12  Docker Container",
    "section": "12.11 Resources",
    "text": "12.11 Resources\nFor a further introduction to Docker take a look at this Data Science for Economists lecture on Docker by Grant McDermott.\nSee Best practices for writing Dockerfiles for advice on how to build Docker images.\nFollow the Docker for Windows release notes for information on new releases and bug fixes."
  },
  {
    "objectID": "azure.html#azure-pc-data",
    "href": "azure.html#azure-pc-data",
    "title": "13  Deploying to Azure",
    "section": "13.1 Poverty Calculator Data",
    "text": "13.1 Poverty Calculator Data\nNote: The Azure Data repo has now been directly synced with pipeline outputs folder on the PIP network drive. There is thus no longer any need to push changes from your local computer.\n\n13.1.1 Deploying data on DEV\nDeploying the data to DEV consists of four main stages:\n\nSync local data to remote Git repository on TFS\nRun the Continuous Integration pipeline (CI)\nRun the Continuous Deployment pipeline (CD) to move the data to Azure blob storage\nData is moved to the Virtual Machine (VM) running the API This stage is completed automatically every 30 minutes (as of 11/02/2021)\n\nYou can deploy data to DEV by following the steps below:\n\n13.1.1.1 Sync data to remote repo on TFS\nStep 1: Sync you local DEV branch with the remote.\nStep 2: Commit and push any data changes to the remote.\n\n\n13.1.1.2 Run CI pipeline\nStep 3: Nagivate to the Azure DevOps Data repo. Click on Pipelines.\n\nStep 4: Select the FILE-COPY-DEV-CI pipeline.\n\nStep 5: Click on Run pipeline.\n\nStep 6: Click on Variables -&gt; FolderName, and add the name of the folder which should be copied to the Cloud Blob Store.\nNote: This step is likely to change when the Data pipeline is re-written to handle mulitiple data version folders.\n\n\n\nStep 7: Click on Run.\n\nStep 8: View the new build.\n\n\n\n13.1.1.3 Run CD pipeline\nStep 9: Click on Releases and select the FILE-COPY-DEV-CD release. Approve the pending request.\n\n\n\nStep 10: Verify that the build completed.\n\n\n\n13.1.1.4 Move data to VM\nStep 11: Wait for the data on the VM to be updated. There is an automatic cron job that runs behind the scenes to copy the data from the blob storage to the VM. This runs at regular intervals, every 30 (?) minutes. You can use the /data-timestamp endpoint to verify that the transfer has completed. For additional information you can also use the /dir-info endpoint.\n\n\n13.1.1.5 Restart the Docker container\nStep 12: Restart the Docker container by running the RESTARTCONTAINER-DEV-CD release pipeline.\n\n\n13.1.1.6 Clear the VM cache\nStep 13: If you conducted a data update that includes changes to survey data or the estimation tables for an already existing version folder you will need to clear the cache on the VM. Use the specific API endpoints for cache handling to do this. Note: If in doubt always clear the cache.\n\n\n\n13.1.2 Deploying data on QA\nStep 1: Nagivate to the Azure DevOps Data repo.\nStep 2: Click on Create pull request. Select from DEV to QA.\nStep 3: Go through the steps to commit and approve the pull request. Please make sure that the “Delete source branch” box is NOT checked, ie. don’t delete the DEV branch. (This default should be modified going forward)\nStep 5: Go to Pipelines -&gt; Pipelines and select the FILE-COPY-QA-CI pipeline. Verify that the pipeline is building. If it wasn’t triggered you will need to trigger it manually.\nStep 6: Go to Pipelines -&gt; Releases and select the FILE-COPY-QA-CD release. Approve the request, and verify that the build completes.\nStep 7: Run the RESTARTCONTAINER-QA-CD release pipeline after the data transfer to the VMs has completed.\nStep 8: Clear the cache on the VMs if needed. Note: If in doubt always clear the cache.\n\n\n13.1.3 Deploying data to Production\nStep 1: Nagivate to the Azure DevOps Data repo.\nStep 2: Click on Create pull request. Select from QA to PROD.\nStep 3: Go through the steps to commit and approve the pull request. Please make sure that the “Delete source branch” box is unchecked, ie. don’t delete the QA branch.\nStep 5: Go to Pipelines -&gt; Pipelines and select the FILE-COPY-PROD-CI pipeline. Verify that the pipeline is building. If it wasn’t triggered you will need to trigger it manually.\nStep 6: Go to Pipelines -&gt; Releases and select the FILE-COPY-QA-CD release. Wait for ITS to approve the request, and then verify that the build completes.\nStep 7: Run the RESTARTCONTAINER-PROD-CD release pipeline after the data transfer to the VMs has completed.\nStep 8: Clear the cache on the VMs."
  },
  {
    "objectID": "azure.html#azure-pc-docker",
    "href": "azure.html#azure-pc-docker",
    "title": "13  Deploying to Azure",
    "section": "13.2 Poverty Calculator Docker image",
    "text": "13.2 Poverty Calculator Docker image\nBefore you start with the application deployment process you will need to clone the Azure DevOps Docker repo (DEV branch) to your local machine. You will only need to do this once. After that you can follow the step by step guide below.\nPlease note that it is important that data changes are pushed through and released before deploying the Docker image. This is because the Docker container will need to restart in order to pick up changes in the mounted folder or volume. The best way to do this is to deploy the data, and then use the API restart pipeline to ensure a restart of the Docker container.\n\n13.2.1 Deploying image on DEV\nStep 1: Verify that the latest code in the master branches of {wbpip} and {pipapi} works with the latest data on DEV. This can be done by running the {pipapi} package in a local RStudio session.\nStep 2: \\[Optional\\] Verify that the most recent Dockerfile builds on your local machine. This is certainly something that should be done if the contents of the Dockerfile has changed, or before major releases. But in a continuous workflow where you know that Dockerfile hasn’t changed, it might be sufficient to verify that the R packages in question are working.\nStep 3: Navigate to the Azure DevOps Docker repo. Go Pipelines -&gt; Pipelines. Trigger the CONTAINER-DEV-CI pipeline, either by\na) Pushing an updated Dockerfile to the remote repo or\nb) Running the Pipeline manually.\nStep 4: Go to the Pipelines -&gt; Releases, and select “Create release” in order to run a new deployment. View the logs to see results from the image build and security scan.\nStep 5:. Visit the DEV API website for further testing.\n\n\n13.2.2 Deploying image on QA\nStep 1: Check that the DEV deployment is working correctly.\nStep 2: Make sure the data on QA is up-to-date (in sync with DEV). If it isn’t you will need to create a PR and merge the data first.\nStep 3: Create a pull request from DEV to QA. Go through the steps to commit and approve the pull request. Please make sure that the “Delete source branch” box is NOT checked, ie. don’t delete the DEV branch.\nStep 4: Go to the Release pipeline to see results from the image build and security scan.\nStep 5: Visit the QA API website for further testing.\n\n\n13.2.3 Deploying image to Production\nStep 1: Check that the QA deployment is working correctly.\nStep 2: Make sure the data on PROD is up-to-date (in sync with QA). If it isn’t you will need to create a PR and merge the data first.\nStep 3: Create a pull request from QA to PROD. Go through the steps to commit and approve the pull request. Please make sure that the “Delete source branch” box is NOT checked, ie. don’t delete the QA branch.\nStep 4: Go to the Release pipeline to see results from the image build and security scan.\nStep 5: Visit the PROD API website for further testing."
  },
  {
    "objectID": "workflows.html#package-webpage-using-pkgdown",
    "href": "workflows.html#package-webpage-using-pkgdown",
    "title": "14  Github Workflows",
    "section": "14.1 Package webpage using {pkgdown}",
    "text": "14.1 Package webpage using {pkgdown}\npkgdown makes it easy to build websites for R packages. With a Github Action workflow this is made even easier because the website will be built and published automatically everytime you commit to the master (main) branch. There is thus no need to run pkgdown::build_site() yourself.\n\n14.1.1 Create gh-pages branch\nThe most common is for a {pkgdown} site to live in a seperate branch in your repository called gh-pages. This should be an empty orphan branch, with no files or commits other then the inital root commit.\nSome {usethis} helpers claims to create this branch for you, but this doesn’t always work. Follow the steps to below to create an empty branch manually.\n# Create new orphan branch \ngit checkout --orphan gh-pages\n# Remove everything\ngit rm -rf .\n# Create empty commit\ngit commit --allow-empty -m \"root commit\"\n# Push to remote\ngit push origin gh-pages\n# Switch back to master\ngit checkout master \n\n\n14.1.2 Create “setup” files\nCreate a new local branch in your repository, and then take advantage of the helper functions in the {usethis} package. Run this from the working directory of your local branch:\nusethis::use_pkgdown_github_pages()\nThis should create the entire setup needed, including adding the files _pkgdown.yml and .github/workflows/pkgdown.yaml to your working directory.\nCreate a pull request and merge the changes to the master branch.\n\n\n14.1.3 Activate Github Pages\nGo to Settings -&gt; Github Pages. Activate Github Pages and set it to build from gh-pages/root.\nRemember to also add your page link to the About section of your repo."
  },
  {
    "objectID": "workflows.html#package-build-checks",
    "href": "workflows.html#package-build-checks",
    "title": "14  Github Workflows",
    "section": "14.2 Package build checks",
    "text": "14.2 Package build checks\nA crucial part of any R package development process is the build check.\nWith Github Actions you can add workflows to automatically check your package for different versions of R and on different operating systems. A good way to get started is to use the simple release workflow, but more advanced packages will benefit from a standard or custom workflow.\nA standard workflow checks if the package builds on the latest available R version on all three major operating systems (Windows, macOS, Ubuntu), and should be used for all packages that are planned to be published on CRAN. A custom workflow is helpful if you want to check your package against specific versions of R or other OS variants.\nFor the current PIP R packages both {wbpip} and {pipapi} go through a standard build check, while other packages have custom workflows. For example do the build checks for {pipaux} and {pipdm} test if the package works for the latest version of R and the current R version on the PovcalNet remote server."
  },
  {
    "objectID": "workflows.html#code-coverage",
    "href": "workflows.html#code-coverage",
    "title": "14  Github Workflows",
    "section": "14.3 Code coverage",
    "text": "14.3 Code coverage\nAnother important part of package development is to check the code coverage of your unit tests. This is typically done with the {covr} package.\nWith Github Actions you can automatically upload coverage reports to codecov.io, to let yourself and others more easily keep track of the test coverage of your package.\n\n14.3.1 Create “setup” files\nCreate a new local branch in your repository, and then take advantage of the helper functions in the {usethis} package. Run this from the working directory of your local branch:\nusethis::use_coverage(\"codecov\")\nusethis::use_github_action(\"test-coverage\")\nThis should create the entire setup needed, including adding the files codecov.yml and .github/workflows/test-coverage.yaml to your working directory.\nCreate a pull request and merge the changes to the master branch.\n\n\n14.3.2 Integrate with codecov.io\nAll the repositories are already synced with codecov.io, so you only need to make sure all your test are passing and push to GH to activate your test coverage."
  },
  {
    "objectID": "workflows.html#other-workflows",
    "href": "workflows.html#other-workflows",
    "title": "14  Github Workflows",
    "section": "14.4 Other workflows",
    "text": "14.4 Other workflows\nThe PR commands workflow enables the use of two specific commands in pull request issue comments. /document will use roxygen2 to rebuild the documentation for the package and commit the result to the pull request. /style will use styler to restyle your package."
  },
  {
    "objectID": "subtrees.html#note",
    "href": "subtrees.html#note",
    "title": "15  Subtrees using Smartgit",
    "section": "15.1 NOTE:",
    "text": "15.1 NOTE:\nThis chapter is basically a “copy and paste” document from the original Smartgit page. The value added is the images that illustrate what is written.\nSubtrees is a way to integrate other repositories into a, more general repository. Each repository that is added to the main repository is a “subtree” and will be stored as a sub-folder into the root directory of the main repository."
  },
  {
    "objectID": "subtrees.html#subtrees-in-the-ui",
    "href": "subtrees.html#subtrees-in-the-ui",
    "title": "15  Subtrees using Smartgit",
    "section": "15.2 Subtrees in the UI",
    "text": "15.2 Subtrees in the UI\nRefs belonging to subtrees will be denoted in the Branches view by a folder-symbol.\n\n\n\nsubtree_brach\n\n\nLocal subtrees will show up in the Subtrees category. The root commits of a subtree will be denoted by a folder-symbol in the Log Graph.\n\n\n\nsubtree_graph"
  },
  {
    "objectID": "subtrees.html#basic-subtree-operations",
    "href": "subtrees.html#basic-subtree-operations",
    "title": "15  Subtrees using Smartgit",
    "section": "15.3 Basic Subtree operations",
    "text": "15.3 Basic Subtree operations\nTo add a new subtree, select the root directory of the repository in the Repositories view and invoke Remote -&gt; Subtree -&gt;Add.\n\n\n\nadd_subtree\n\n\nIf you’re adding several subtrees to the main repo, rename the remote subtree when you’re adding a new subtree\n\n\n\nchange_name\n\n\nTo fetch new (remote) changes from the subtree repository, select the subtree remote in the Branches view and invoke Pull from the context menu.\n\n\n\nsubtree_pull1\n\n\nTo merge (or cherry-pick) a subtree use the Merge (or Cherry-Pick) command. SmartGit will understand whether the source commit is a subtree and in this case perform a subtree merge (or cherry-pick).\n\n\n\nsubtree_merge\n\n\nTo push commits back to a subtree, select the (remote) subtree branch in the Branches view and invoke Remote|Subtree|Push. Pushing a subtree involves splitting changes back from main repository to subtree repository (more details can be found below).\nTo reset your main repository to a certain subtree, first Check Out the branch which should be reset. Then, in the Branches view, select the subtree branch to which your main repository should be reset to and invoke Subtree|Reset from the context menu.\n\n15.3.0.1 Note\n\nIf there are already existing subtrees in your main repository you will have to add corresponding remotes for these subtrees using Remote|Add."
  },
  {
    "objectID": "subtrees.html#synchronizing-changes-back-to-the-subtree-repository",
    "href": "subtrees.html#synchronizing-changes-back-to-the-subtree-repository",
    "title": "15  Subtrees using Smartgit",
    "section": "15.4 Synchronizing changes back to the subtree repository",
    "text": "15.4 Synchronizing changes back to the subtree repository\nThere are two ways of pushing changes to the remote.\n\n15.4.1 Using Push\n\nCheck Out the the repository’s branch which will be pushed to a specific subtree branch.\nMake sure your changes are staged and commited in the branch of your choice in the main repository\nIn the Branches view, select the subtree branch to which the changes should be pushed and invoke Subtree-&gt;Pushfrom the context menu.\nUse the prefix to specify that this change is coming from a repo that is using the modifying repo as a subtree\n\nThe commits created by the Split-command will be written to the Subtree-Branch to update. If you have selected a remote subtree branch, you will have to select Subtree-Branch to update or enter the name of a new branch there; in either case the selected branch will then be updated/created. In this case I decided to implement the changes in the dev branch of the subtree. Since that branch does not exist it won’t be created\n\n\n\n15.4.2 Using Split\nAfter having applied changes to files in your main repository which actually belong to a subtree repository, you can “synchronize” these changes back to the subtree repository using the Split command:\n\nCheck Out the main repository’s branch which should be split back.\nMake sure your changes are staged and commited in the branch of your choice in the main repository\nIn the Branches view, select the subtree branch to which the changes should be split back and invoke Subtree|Split from the context menu. The commits created by the Split-command will be written to the Subtree-Branch to update. If you have selected a remote subtree branch, you will have to select Subtree-Branch to update or enter the name of a new branch there; in either case the selected branch will then be updated/created. In this case I decided to implement the changes in the dev branch of the subtree. Since that branch does not exist it won’t be created\n\n15.4.2.1 Note\n\nSmartGit will not actually split changes back to this particular branch but the branch selection serves primarily to identify to which subtree the changes should be split back. The underlying Git command will then detect the appropriate subtree commits onto which the new commits will be split back.\n\nTo push changes back to your remote subtree repository, tracking between the local subtree branch which has just been updated/created in step 2 and the remote subtree branch has to be set up. This can optionally be done as part of the Add wizard or you can do it manually:\n\nIn the Branches view, select both the local and remote subtree branch and invoke Set Tracked Branch from the context menu.\nThe Branches will now denote ahead/behind commits for the local subtree branch.\nVerify these ahead/behind commits also in the Graph.\nPush back the subtree commits by selecting the local subtree branch in the Branches view and invoking Push from the context menu."
  },
  {
    "objectID": "subtrees.html#information",
    "href": "subtrees.html#information",
    "title": "15  Subtrees using Smartgit",
    "section": "15.5 Information",
    "text": "15.5 Information\n\nSmartgit\nviz"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "r if (knitr::is_html_output()) ' '"
  }
]