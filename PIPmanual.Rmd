--- 
title: "PIP Manual"
author: ["DECIS", "Poverty GP"]
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is are the guidelines to all the technical and methodological decisions taken during the creation of packges for the PIP workflow."
---

# Prerequisites {-}

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')

set.seed(1234)
options(digits = 3)

knitr::opts_chunk$set(
  comment = "#>",
  warning = FALSE
  #collapse = TRUE,
  #cache = TRUE,
  #out.width = "70%",
  #fig.align = 'center',
  #fig.width = 6,
  #fig.asp = 0.618,  # 1 / phi
  #fig.show = "hold"
)

options(dplyr.print_min = 6, 
        dplyr.print_max = 6)

```


You need to make sure the `bookdown` package is installed in your computer
```{r eval=FALSE}
install.packages("bookdown")

# or the development version
 devtools::install_github("rstudio/bookdown")
```

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.


Make sure to install the latest version of PIP R packages by typing the following

```{r, eval=FALSE}
ipkg <- utils::installed.packages()[,1]

pip_install <- function(pkg, ipkg) {
  if (isFALSE(pkg %in% ipkg)) {
    gitcall <- paste0("PIP-Technical-Team/", pkg)
    remotes::install_github(gitcall, dependencies = TRUE)
    TRUE
  } else {
    FALSE
  }
}

pkgs <- c("pipload", "pipaux", "wbpip", "piptb", "pipdm", "pipapi")


purrr::walk(pkgs, pip_install, ipkg = ipkg)

```

<!--chapter:end:index.Rmd-->

# (PART) Introduction {.unnumbered}

# Introduction {#intro}

PIP workflow is ...

<!--chapter:end:intro.Rmd-->

# Internal Workflow

This chapter explains the internal technical workflow of the PIP project. Many
of the different steps of the workflow are explained in more detail in other
chapters of this book. This chapter, however, presents an overview of the
workflow and its components.

## The Github group

The technical code of the PIP project is organized in Git repositories in the
Github group [/PIP-Technical-Team](https://github.com/PIP-Technical-Team). You
need to be granted collaborator status in order to contribute to any of the
repositories in the group. Also, many of the repositories do not play a direct
role in the PIP technical workflow. Some of them are intended for documenting
parts of the workflow or for testing purposes. For example, the repository of
this book--[/PIPmanual](https://github.com/PIP-Technical-Team/PIPmanual)--is not
part of the workflow of PIP, since it is not necessary for any estimation. Yet,
you need get familiar with all the repositories in case you need to make a
contribution in any of them. In this chapter, however, we will focus on
understanding the repositories that affect directly the PIP workflow.

First, we will see the overview of the workflow. It is an overview because each
bucket of its buckets is a workflow in itself. Yet, it is important to have the
overview clear in order to understand how and where all the pieces fall
together. Then, we will unpack each of the workflow buckets to understand them
in more detail.

## Overview

The workflow overview is mainly composed of four steps.

1.  Data acquisition
2.  Data preparation
3.  pre-computed indicators
4.  API feeding

![](img/PIP_workflow_overview.png "PIP workflow overview")

Each of the steps (or buckets) is prerequisite of the next one, so if something
changes in one of the them, it is necessary to execute the subsequent steps.

## Data acquisition

Before understanding how the input data of PIP is acquired, we need to
understand PIP data itself. The PIP is fed by two kinds of data: welfare data
and auxiliary data.

**Welfare data** refers to the data files that contain at least one welfare
vector and one population expansion factor (i.e., weights) vector. These two
variables are the minimum data necessary to estimate poverty and inequality
measures.[^internal_workflow-1] These files come in four varieties; microdata,
group data, bin data, and synthetic data. The details of welfare can be found in
Section \@ref(welfare-data). Regardless of the variety of the data, all welfare
data in PIP are gathered from the Global Monitoring Database, GMD. For a
comprehensive explanation of how the household data are selected and obtained,
you may check chapter [*Acquiring household survey
data*](https://povcalnet-team.github.io/Methodology/acquiring.html)of the
[methodological PIP
manual](https://povcalnet-team.github.io/Methodology/index.html).

[^internal_workflow-1]: Population data is also necessary when working with
    group data.

*Microdata* is uploaded into the PRIMUS system by the regional teams of the
Poverty Global Practice. To do so, each regional team has to follow the GMD
guidelines, which are verified by the Stata command `{primus}`. Rarely. the
`{primus}` command does NOT capture some potential errors in the data. More
details in Section \@ref(primus)

As of now (`r format(Sys.Date(), "%B %d, %Y")`), *Group data* is divided two:
historical group data and new group data. Historical group data is organized and
provided by the PovcalNet team, who sends it to the Poverty GP to be included in
the datalibweb system. New group data is collected by the poverty economist of
the corresponding country, who shares it with her regional focal team, who
shares it with PovcalNet team. This new group data is organized and tested by
the PovcalNet team and then send back to the poverty GP to be included in the
datalibweb system.

*Bin data* refers to welfare data from countries for which there is no poverty
economist. Most of these countries are developed countries such as Canada or
Japan. The main characteristic of this data is that it is only available through
the [LISSY system](https://www.lisdatacenter.org/data-access/lissy/) of the [LIS
data center](https://www.lisdatacenter.org/), which does not allow access to the
entire microdata. Thus, we need to contract the microdata into 400 bins or
quantiles. The code that gathers LIS data is available in the Github repository
[PovcalNet-Team/LIS_data](https://github.com/PovcalNet-Team/LIS_data).

Finally, *synthetic data*, refers to simulated microdata from an statistical
procedure. As of now (`r format(Sys.Date(), "%B %d, %Y")`), the these data are
estimated using multiple imputation techniques, so that any calculation must
take into account the imputation-id variable. The data is calculated by the
poverty economist of the country and the organizes by the global team in the
Poverty GP.

**Auxiliary data** refers to all data necessary to temporally deflate and line
up welfare data, with the objective of getting poverty estimates comparable over
time, across countries, and, more importantly, being able to estimate regional
and global estimates. Some of these data are national population, GDP, consumer
price index, purchasing parity power, etc. Auxiliary data also include metadata
such as time comparability or type of welfare aggregate. Since each measure of
auxiliary data is acquired differently, all the details are explain in Section
\@ref(auxiliary-data).

## Data preparation

This step assumes that all welfare data is properly organized in the datalibweb
system and vetted in PRIMUS. In contrast to the previous global-poverty
calculator system, PovcalNet, the PIP system only gathers welfare from the
datalibweb server.

The welfare data preparation is done using the repository
[/pipdp](https://github.com/PIP-Technical-Team/pipdp). As of now
(`r format(Sys.Date(), "%B %d, %Y")`), this part of the process has been coded
in Stata, given that the there is no an R version of `{dataliweb}`. As for the
auxiliary data preparation, it is done with package `{pipaux}`, available in the
repository [/pipaux](https://github.com/PIP-Technical-Team/pipaux). Right now
the automation of updating the auxiliary has not been implemented. Thus, it has
to be done manually by typing `pipaux::pip_update_all_aux()` to update all
measures, or use function `pipaux::update_aux()` to update a particular measure.

## Pre-computed indicators

All measures in PIP that do not depend on the value of the poverty line are
pre-computed in order to make the API more efficient and responsive. Some other
indicators that not depend on the poverty lines but do depend on other
parameters, like the societal poverty, are not included as part of the
pre-computed indicators.

This step is executed in the repository
[/pip_ingestion_pipeline](https://github.com/PIP-Technical-Team/pip_ingestion_pipeline),
which is a pipeline powered by the `{targets}` package. The process to estimate
the pre-computed indicators is explained in detail in Chapter \@ref(pcpipeline).
The pipeline makes use of two R packages, `{wbpip}` and `{pipdm}`. The former is
publicly available and contains all the technical and methodological procedures
to estimate poverty and inequality measures at the country, regional, and global
level. The latter, makes use of `{wbpip}` to execute the calculations and put
the resulting data in order, ready to be ingested by the PIP API.

## API feeding

[We need to finish this part (Tony?)]{style="color:red"} .

## Packages interaction

[We need to finish this part]{style="color:red"}.

<!--chapter:end:internal_workflow.Rmd-->

# Folder Structure

<!--chapter:end:Folder_structure.Rmd-->

# Joining Data

Since the PIP project is comprised of several databases at very different domain
levels, this chapter provides all the guidelines to join data frames correctly.
The first to understand is that the reporting measures in PIP (e.g., poverty and
inequality) are uniquely identified by four variables: *Country*, *year*,
*domain*, and *welfare type*. *Country* refers to independent economies that
conduct independent household surveys. For instance, China and Taiwan are
treated as **two different economies** by the World Bank, and hence by PIP, even
though under some criteria some people think that Taiwan is part of China.
*Year* refers to reporting year rather than the actual calendar years over which
the survey was conducted. Some household surveys like India 2011/2012 are
conducted over the of two calendar year, but the welfare aggregate is deflated
to the reporting year, 2011. *Domain* refers to the smallest geographical
disaggregation for which it is possible to deflate and line up to PPP values the
welfare aggregate of a household survey. The criteria to determine the reporting
domain of a household survey is still under consideration, but ideally it such
for which there is CPI, PPP, and population auxiliary data, as well as a
household survey representative at that level. There are some exceptions to this
criterion like China or the Philippines, but this cases explained in detailed in
Section \@ref(special-data-cases). As of today
(`r format(Sys.Date(), "%B %d, %Y")`), most country/years are reported at the
national domain and few are reported at the urban/rural domain. However, the PIP
technical infrastructure has been designed to incorporate other domain levels
if, at some point in time, it is the case. Finally, the *welfare type* specifies
whether the welfare aggregate is based on income or in consumption. For the
latter case, though some households surveys capture expenditure instead, they
are still considered consumption-based surveys.

The challenge to join different data frames in PIP is that these four variables
that uniquely identify the reporting measures **are not** available to uniquely
identify any of the PIP data files---with the exception of the cache files that
we discuss below. This challenge is easily addressed by having a clear
understanding of the Price FrameWork (pfw) data frame. This file does not only
contain valuable metadata, but it could also be considered as the anchor among
all PIP data.

## The Price FrameWork (pfw) data

As always, this file can be loaded by typing,

```{r}
library(data.table)
pfw <- pipload::pip_load_aux("pfw")
joyn::is_id(pfw, by = c("country_code", "surveyid_year", "survey_acronym"))
```

First of all, notice that `pfw` is uniquely identified by country code, survey
year, and survey acronym. The reason for this is that pfw aims at providing a
link between every single household survey and all the other auxiliary data.
Since welfare data is stored following the naming convention of the
[International Household Survey Network (IHSN)](http://ihsn.org/), data is
stored according to country, survey year, acronym of the survey, and vintage
control of master and alternative versions. The vintage control of the master
and alternative version of the data is not relevant for joining data because PIP
uses, by default, the most recent version.

Keep in mind that PIP estimates are reported at the country, year, domain, and
welfare type level, but the last two of these are not found either in the survey
ID nor as unique identifiers of the pfw. To solve this problem, the pfw data
makes use of the variables `welfare_type,` `aaa_domain`, and `aaa_domain_var`.

As the name suggests, `welfare_type` indicates the **main** welfare aggregate
type (i.e, income or consumption ) of the variable `welfare` in the GMD datasets
that correspond to the survey ID formed by concatenating variables
`country_code`, `surveyid_year`, and `survey_acronym`. For example, the
`welfare_type` of the `welfare` variable in the datasets of
***COL_2018_GEIH**\_V01_M\_V03_A\_GMD* is income.

```{r}
pfw[ country_code    == "COL"
    & surveyid_year  == 2018
    & survey_acronym == "GEIH", # Not necessary since it is the only one
    unique(welfare_type)]
```

The prefix `aaa` in variables `aaa_domain` and `aaa_domain_var` refers to the
identification code of any of the auxiliary data. Thus, you will find a
`gdp_domain`, `cpi_domain`, `ppp_domain` and several others. All `aaa_domain`
variables contain the *lower* level of geographical disaggregation of the
corresponding `aaa` auxiliary data. There are only three possible levels of
disaagregation,

```{r, echo=FALSE}
dd <- tibble::tribble(
        ~domain.value,      ~meaning,
                   1L,    "national",
                   2L, "urban/rural",
                   3L, "subnational"
        )
knitr::kable(dd, 
             format =  "html", 
             col.names = gsub("[.]", " ", names(dd)))
```

As of now, no survey or auxiliary data is broken down at level 3 (i.e.,
subnational), but it is important to know that the PIP internal code takes that
possibility into account for future cases.

Depending on the country, the domain level of each auxiliary data might be
different. In Indonesia, for instance, the CPI domain is national, whereas the
PPP domain is "urban/rural."

```{r}
pfw[ country_code == "IDN" & surveyid_year == 2018, 
    .(cpi = unique(cpi_domain), 
      ppp = unique(ppp_domain))]
```

Finally, [and this is really important]{style="color:red"}, variables
`aaa_domain_var` contains the name of variable in the GMD dataset that uniquely
identify the household survey *in the corresponding `aaa`* *auxiliary data*. In
other words, `aaa_domain_var` contains *the name* of the variable in GMD that
must be used as *key* *to join* GMD to `aaa`. You may ask, does the name of the
variable in the `aaa` auxiliary data have the same variable name in the GMD data
specified in `aaa_domain_var`? No, it does not. Since the domain level to
identify observations in the `aaa` auxiliary data is unique, there is only one
variable in auxiliary data used to merge any welfare data, `aaa_data_level`.
Since all this process is a little cumbersome, the
{[pipdp](https://github.com/PIP-Technical-Team/pipdp)} Stata package, during the
process of cleaning GMD databases to PIP databases, creates as many
`aaa_data_level` variables as needed in order to make the join of welfare data
and auxiliary data simpler. You can see the lines of code that create these
variables in [this
section](https://github.com/PIP-Technical-Team/pipdp/blob/9c4e32636dd1c71954816f8a7c8ad743349959a6/pipdp_md_clean.ado#L225-L294)
of the file "pipdp_md_clean.ado."[^joining_data-1]

[^joining_data-1]: You can find more information about the conversion from GMD
    to PIP databases in Section \@ref(welfare-data)

### Joining data example

Let's see the case of Indonesia above. The pfw says that the CPI domain is
"national" and the PPP domain is "urban/rural." That means that the welfare data
join to each of these auxiliary data with two different variables,

```{r}
domains <- 
  pfw[ country_code == "IDN" & surveyid_year == 2018, 
     .(cpi = unique(cpi_domain_var), 
       ppp = unique(ppp_domain_var))][]
```

This says that the name of the variable in the welfare data to join PPP data is
called `uban`, but there is not seem to be a variable name in GMD to join the
CPI data. When the name of the variable is missing, it indicates that the
welfare data is not split by any variable to merge CPI data. That is, it is at
the national level.

```{r}
ccode  <- "CHN"
cpi <- pipload::pip_load_aux("cpi")
ppp <- pipload::pip_load_aux("ppp")

CHN <-  pipload::pip_load_data(country = ccode, 
                              year    = 2015)

dt <- joyn::merge(CHN, cpi,
                  by = c("country_code", "survey_year",
                         "survey_acronym", "cpi_data_level"),
                  match_type = "m:1", 
                  keep = "left")
```

## Special data cases

<!--chapter:end:joining_data.Rmd-->

# (PART) PIP data {.unnumbered}

# Welfare data {#welfare-data}

blah blah blah

## Origin of data {#welfare-origin-of-data}

blah

## From GMD to PIP ({pipdp} package) {#from-gmd-to-pip}

blah

## Survey ID nomenclature

All household surveys in the PIP repository are stored following the naming
convention of the [International Household Survey Network
(IHSN)](http://ihsn.org/) for [archiving and managing
data](http://www.ihsn.org/archiving). This structure can be generalized as
follows:

    CCC_YYYY_SSSS_ vNN_M_vNN_A_TYPE_MODULE

where,

-   `CCC` refers to 3 letter ISO country code
-   `YYYY` refers to survey year when data collection started
-   `SSSS` refers to survey acronym (e.g., LSMS, CWIQ, HBS, etc.)
-   `vNN` is the version vintage of the raw/master data if followed by an `M` or
    of the alternative/adapted data if followed by an `A`. The first version
    will always be v01, and when a newer version is available it should be named
    sequentially (e.g. v02, v03, etc.). Note that when a new version is
    available, the previous ones must still be kept.
-   `TYPE` refers to the collection name. In the case of PIP data, the type is
    precisely, **PIP**, but be aware that there are several other types in the
    datalibweb collection. For instance, the Global Monitoring Database uses
    **GMD**; the South Asia region uses **SARMD**; or the LAC region uses
    **SEDLAC**.
-   `MODULE` refers to the module of the collection. This part of the survey ID
    is only available at the *file level*, not at the folder (i.e, survey)
    level. Since the folder structure is created at the survey level, there is
    no place in the survey ID to include the module of the survey. However,
    within a single survey, you may find different modules, which are specified
    in the name of the file. In the case of PIP, the module of the survey is
    divided in two: the PIP tool and the GMD module. The *PIP tool* could be
    *PC*, *TB,* or *SOL* (forthcoming), which stand for one of the PIP systems,
    Poverty Calculator, Table Baker (or Maker), and Statistics OnLine. the *GMD
    module*, refers to the original module in the GMD collection, such as module
    *ALL*, or *GPWG, HIST,* or *BIN.*

For example, the most recent version of the harmonized Pakistani Household
Survey of 2015, she would refer to the survey ID
`PAK_2015_PSLM_v01_M_v02_A_PIP`. In this case, PSLM refers to the acronym of the
Pakistan Social and Living Standards Measurement Survey. v01_M means that the
version of the raw data has not changed since it was released and v02_A means
that the most recent version of the alternative version is 02.2

<!--chapter:end:welfare_data.Rmd-->

# Auxiliary data

```{r, echo=FALSE, include=FALSE}
library(data.table)
library(pipaux)
```

As it was said in Section \@ref(data-acquisition), auxiliary data is the data
used to temporally deflate and line up welfare data, with the objective of
getting poverty estimates comparable over time, across countries, and, more
importantly, being able to estimate regional and global estimates. Yet,
auxiliary data also refers to metadata with functional and qualitative
information. Functional information is such that is used in internal
calculations such as time comparability or surveys availability. Qualitative
information is just useful information that does not affect, neither depend on,
quantitative data. It is primary collected and made available for the end user.

As explain in Chapter \@ref(folder-structure), all auxiliary data is stored in
`"y:/PIP-Data/_aux/"`.

```{r aux-dir, echo=FALSE}
fs::dir_tree("y:/PIP-Data/_aux/", 
             recurse = FALSE, 
             regex = ".*(?<!\\.txt)$", 
             perl = TRUE)
```

The naming convention of subfolders inside the \_aux directory is useful because
auxiliary data is commonly referred to in all technical processes by its
convention rather than by it actual name. For instance Gross Domestic Product or
Purchasing Power Parity are better known as gdp and ppp, respectively. Yet,
other measures such as national population or consumption also make use of
conventions.

In this chapter you will learn everything related to each of the files that
store auxiliary data. Notice that the chapter is structured by files rather than
by measures or types of auxiliary data because you may find more than one
measure in one file.

The R package that manages auxiliary data is `{pipaux}`.

As explained in Chapter \@ref(folder-structure), within the folder of each
auxiliary file, you will find, at a minimum, a `_vintage` folder, one `xxx.fst`,
one `xxx.dta` file, and one `xxx_datasignature.txt` , where `xxx` stands for the
name of the file.

## Population

### Original data

Everything related to population data should be placed in the folder
`y:\PIP-Data\_aux\pop\`. hereafter (`./`).

The population data come from one of two different sources. WDI or an internal
file provided by a member of the DECDG team. Ideally, population data should be
downloaded from WDI, but sometimes the most recent data available has not been
uploaded yet, so it needs to be collected internally in DECDG. As of now
(`r format(Sys.time(), "%B %d, %Y")`), the DECDG focal point to provide the
population data is [Emi Suzuki](mailto:esuzuki1@worldbank.org). You just need to
send her an email, and she will provide the data to you.

If the data is provided by DECDG, it should be stored in the folder
`./raw_data`. The original excel file must be placed without modification in the
folder `./raw_data/original`. Then, the file is copied again one level up into
the folder `./raw_data` with the name `population_country_yyyy-mm-dd.xlsx` where
`yyyy-mm-dd` refers to the official release date of the population data. Notice
that for countries PSE, KWT and SXM, some years of population data are missing
in the DECDG main file and hence in WDI. Here we complement the main file with
an additional file shared by Emi to assure complete coverage. This file contains
historical data and will not need to be updated every year. This additional file
has the name convention `population_missing_yyyy-mm-dd.xlsx` and should follow
the same process as the `population_country` file. Once all the files and their
corresponding place, you can update the `./pop.fst` file by typing
`pipaux::pip_pop_update(src = "decdg")`.

If the data comes directly from WDI, you just need to update the file
`./pop.fst` by typing `pipaux::pip_pop_update(src = "wdi")`. It is worth
mentioning that the population codes used in WDI are "SP.POP.TOTL",
"SP.RUR.TOTL", and "SP.URB.TOTL", which are total population, rural population,
and urban population, respectively. If it is the case that PIP begins using
subnational population, a new set of WDI codes should be added to the R script
in
[pipaux::pip_pop_update()](https://github.com/PIP-Technical-Team/pipaux/blob/cd7738f98ec5373db8333c3573700b4991776c8d/R/pip_pop_update.R#L17).

### Data structure

Population data is loaded by typing either `pipload::pip_load_aux("pop")` or
`pipaux::pip_pop("load")`. We highly recommend the former, as `{pipload}` is the
intended R package for loading any PIP data.

```{r}
pop <- pipload::pip_load_aux("pop")
head(pop)
```

## National Accounts

National accounts account for the economic development of a country at an
aggregate or macroeconomic level. These measure are thus useful to interpolate
or extrapolate microeconomic measures mean welfare aggregate or poverty
headcount when household surveys are not available. National accounts work as a
proxy of the economic development that would have been present if household
surveys were available.

There are two main types of national accounts, Household Final Consumption
Expenditure (HFCE) and Gross Domestic Product (GDP)---both in real per capita
terms. Please refer to [Section
5.3](https://povcalnet-team.github.io/Methodology/lineupestimates.html#nationalaccounts)
of [@worldbankPovertyInequalityPlatform2021] to understand the usage of national
accounts data.

### GDP

As explained in [Section
5.3](https://povcalnet-team.github.io/Methodology/lineupestimates.html#nationalaccounts)
of [@worldbankPovertyInequalityPlatform2021], there are three sources of GDP
data, and one more for a few particular cases. The integration of all the
sources of GDP data is performed by `pipaux::pip_gdp_update()`, you'll need to
manually download and store the data from WEO and the data for the special
cases.

The most recent version of the WEO data most be downloaded from the [World
Economic Outlook
Databases](https://www.imf.org/en/Publications/SPROLLS/world-economic-outlook-databases)
of the IMF.org website and saved as an .xls file in `<maindir>/_aux/weo/`. The
filename should be in the following structure `WEO_<YYYY-DD-MM>.xls`. Due to
potential file corruption the file must be opened and re-saved before it can be
updated with `pip_gdp_weo()`, which is an internal function fo
`pipaux::pip_gdp_update()`. Hopefully in the future IMF will stop using an
\`.xls\` file that's not really xls.

[Note: explanation of special cases provided by Samuel.]{style="color:red"}

### Consumption (PCE)

Private Consumption Expenditure (pce) is gathered from WDI, with the exception
of a few special cases. As in the case of GDP, the special cases are treated in
the same way with PCE. You only need to execute the function
`pipaux::pip_pce_update()` to update the PCE data.

## CPI

### Raw data

[Note: Minh to explain how the CPI data is collected and organized in the dlw
folders.]{style="color:red"}

### Vintage control

```{r, echo=FALSE, include=FALSE}
cpi <- pipload::pip_load_aux("cpi")
cpi_id <- cpi[, 
              unique(cpi_id)]
```

Vintage control of the CPI data comes in a similar fashion as welfare data,
`CPI_vXX_M_vXX_A`, where `vXX_M` refers to the version of the master or raw
data, and `vXX_A` refers to the alternative version.

Every year, around November-December, PIP CPI data is updated with the most
recent version of the IMF CPI data, which comes with information for the most
recent year available and with changes/fixes/additions of previous years for
each country. When this happens, the master version of the CPI ID is increased
in one unit before the data is saved. As of today, the current ID is `r cpi_id`.
If data is modified during the rolling of the year, then the alternative version
of the CPI ID is increased in one unit.

### Data structure 

When you load CPI data using `pipload::pip_load_aux("cpi")`, the data you get
has already been cleaned for being use in the PIP workflow, and it is slightly
different from the original CPI data stored in datalibweb servers. That is, the
way CPI data is used and referred to datalibweb is different from the way it is
used in PIP even though they both achive the same purpose.

The most important variable in CPI data is, no surprisingly, `cpi`. This
variable however, is not available in the original CPI data from dlw. The
original name of this variable comes in the form `cpiYYYY`, where `YYYY` refers
to the base year of the CPI, which in turn depends on the collection year of the
PPP. Today, this variable is thus "`r getOption("pipaux.cpivar")`." The name of
this variable is stored in the `pipaux.cpivar` object in the `zzz.R` file of the
`{pipaux}` package. This will supdate the option `getOption("pipaux.cpivar")`,
guaranteing that `pipaux::pip_cpi_update()` uses the right variable when
updating the CPI data.

## PPP

[Note: Minh to explain how the PPP data is collected and organized in the dlw
folders.]{style="color:red"}

PPP data is organized... s

### Original data

blah

## Price FrameWork (PFW)

blah

### Original data

<!--chapter:end:auxiliary_data.Rmd-->

# PRIMUS

According to the description in the Stata repository
[worldbank/primus](https://github.com/worldbank/primus),

> The **PRIMUS** system is designed to facilitate this process of generating
> internal estimates of the World Bank's poverty indicators and reduce the time
> needed for resolving discrepancies. It is a workflow management platform for
> the submission, review and approval of poverty estimates and a tracking
> facility to capture the inputs and results of the estimation process for
> future reference and audits.

As such, **PRIMUS** is the platform used by the PovcalNet team to approve the
adoption of new survey data into the PovcalNet system.

## Interacting with PRIMUS

The interaction with **PRIMUS** is done through different systems, so it is best
to begin by clarifying terms.

### Website platform {.unnumbered}

PRIMUS can be accessed by typing
[primus/](http://spqsapps.worldbank.org/qs/primus/Pages/PRIMUShome_new.aspx) in
your browser. As long as you're connected to the intranet it should work fine.
However, if you have any issues connecting to the platform, please send an email
to [Minh Cong Nguyen](mailto:mnguyen3@worldbank.org), requesting access.

Each database uploaded into PRIMUS gets a unique transaction ID. This ID is
important because it is not unique to a dataset but unique to the transaction
(or vintage of the data). That is, if one particular dataset is uploaded more
than once, it will get two different transaction IDs. When talking to the
Poverty GP, you better refer to the transaction ID rather than the survey (or at
least both) because, though you may be talking about the same country/year, you
are actually talking about two different transactions. See for instance [Brazil
2013](http://spqsapps.worldbank.org/qs/primus/Pages/PRIMUShome_new.aspx#tab=tab3&country=BRA&year=2013).

### Stata command {.unnumbered}

The Poverty GP maintains the Stata repository
[worldbank/primus](https://github.com/worldbank/primus) from which you can
download the command `primus`. Right now, this is the official place from which
you can access this command. From now on, each time we refer to the command, we
use `primus`, whereas when we refer to the website, we use PRIMUS.

Please, make sure you have it properly installed in your computer, by following
the instruction section \@ref(stata-github). Basically, you need to install
first the [github](https://github.com/haghish/github) Stata command by [E. F.
Haghish](https://github.com/haghish)

```{stata, eval = FALSE}
net install github, from("https://haghish.github.io/github/")
```

Now, you can install `primus` by just typing the following in Stata

```{stata, eval = FALSE}
github install worldbank/primus
```

In case this does not work, follow instructions in section \@ref(stata-github)
for alternative methods.

### Corrections to `primus` Stata command {.unnumbered}

The `primus` command is maintained by the Poverty GP, so we have no control over
modifications or improvements. The best you can do in case you need to fix or
modify something in this command is to fork the repository, clone the forked
repo into your computer, check out a new branch, make any modification, and
generate a pull request to the master branch of the original repository. Once
you have done that, make sure to send an email with your suggestions for
improvement to [Ani Rudra Silwal](mailto:asilwal@worldbank.org), copying to the
D4G Central Team (Nobuo Yoshida and Minh Cong Nguyen).

## Understanding PRIMUS {#understand-primus}

Each time a database is uploaded into PRIMUS, it is assigned a transaction ID.
During the uploading process (or right after it has finished), the three
parties--DECDG, DECRG, or the Poverty GP--evaluate the quality of the new or
corrected data and approve them or reject them in the system. Depending on the
decision of all the parties, each transaction will take one of three possible
status, *pending*, *approved,* or *rejected*.

::: {.rmdbox .rmdwarning}
As of today (2020-11-20), there is no one who represents DECRG. So, the
approving process might be different and it will need to be changed in the
PRIMUS system. Please check.
:::

The transaction ID is *pending* when at least one of the three parties (DECDG,
DECRG, or the Poverty GP) has not approved it in the system. You can click on
the check box *PENDING* in the PRIMUS website to see which surveys have such a
status, or you can use the `primus` command list this,

```{stata, eval= FALSE}
qui primus query, overallstatus(PENDING)
list transaction_id country year date_modified in 1/`=min(10, _N)'
     +----------------------------------------------+
     |              transaction_id   country   year |
     |----------------------------------------------|
  1. | TRN-000327173-EAP-IDN-QR48Q       IDN   2017 |
  2. | TRN-000327173-ECA-DEU-YJYVZ       DEU   1995 |
  3. | TRN-000327173-ECA-DEU-2P4DR       DEU   2002 |
  4. | TRN-000327173-ECA-DEU-LJN8R       DEU   2003 |
  5. | TRN-000327173-ECA-DEU-ZSN9J       DEU   2005 |
     |----------------------------------------------|
  6. | TRN-000327173-ECA-DEU-UBS7M       DEU   2008 |
  7. | TRN-000327173-ECA-DEU-41TOU       DEU   2009 |
  8. | TRN-000327173-EAP-AUS-KKZ2E       AUS   2004 |
     +----------------------------------------------+
```

Notice that the overall status of a transaction is independent from survey ID.
Thus, it is possible to find several transactions for the same country and year.
Indonesia 2017, for instance, has three transactions, two of them rejected and
one of them pending.

```{stata, eval = FALSE}
qui primus query, country(IDN) year(2017)
list transaction_id overall_status date_modified in 1/`=min(10, _N)'
     +--------------------------------------------------+
     |              transaction_id        date_modified |
     |--------------------------------------------------|
  1. | TRN-000104674-EAP-IDN-8R9IF   23may2018 15:28:47 |
  2. | TRN-000327173-EAP-IDN-TYA1A   23may2018 23:57:27 |
  3. | TRN-000327173-EAP-IDN-QR48Q   24may2018 00:27:33 |
     +--------------------------------------------------+
```

A transaction is *rejected* when at least one of the three parties rejected the
database. Finally, a transaction is *approved* only when all three parties have
approved it into the system.

::: {.rmdbox .rmdtip}
We recommend you understand the basic functionality of the `primus` command by
reading the help file (type `help primus` in Stata).
:::

## Checking PRIMUS estimates

The real first step to check the quality of the recently uploaded data into
PRIMUS is to download the basic estimates of each data and compare them with our
own. There is no need to calculate and compare all the estimates available in
PRIMUS but the mean in PPP, the poverty headcount, and the Gini index.

The `primus` command allows us to download the estimates of each transaction,
but it has to be done one by one. Fortunately, the `pcn` command downloads all
the estimates of pending transactions for us and properly stores them in the
folder `p:\01.PovcalNet\03.QA\02.PRIMUS\pending\` `r emo::ji("tada")`
`r emo::ji("tada")` . You only need to type,

```{stata, eval = FALSE}
pcn primus pending, down(estimates)
```

In addition, `pcn` checks the date for which you're downloading the estimates
and keeps only those transactions that have been uploaded for the next spring or
annual-meetings release. For instance, assume that today, 2020-11-20, you want
to see the estimates of pending transactions in PRIMUS. Since annual meetings
take place around September, `pcn` assumes you are interested in the estimates
for the Spring-meetings release, around March next year. Thus, it will filter
the results from `primus`, keeping only those transactions that were uploaded
from November 2020. Now it is likely that the PRIMUS system has not been opened
for uploading new data in November, as it usually opens around December and
July. Thus, it is likely that you will find and error saying
`There is no pending data in PRIMUS for the combination of country/years selected`.

You can load the recently-downloaded estimates by typing,

```{stata, eval = FALSE}
pcn primus pending, load(estimates)
```

Now, you have to check whether the new estimates make sense. Once way to that is
to follow this do-file,
`p:\01.PovcalNet\03.QA\02.PRIMUS\pending\2020_SM\estimates\checks\comparisons_wrk_data.do`.

::: {.rmdbox .rmdimportant}
You do NOT need to check the estimates with the working data (wrk) as it is
suggested in the do-file above. The PovcalNet System is now fully integrated
with the `datalibweb` system, so the CPI, PPP, and microdata will be always the
same. The best you can do at this stage is to make sure the estimates in PRIMUS
make sense at the country level.
:::

## Confirming and approving data in PRIMUS {#approve-primus}

Once you have checked that the estimates of pending transactions make sense, you
need to approve them. As explained in section \@ref(understand-primus), the
approval on PRIMUS requires the consent of three parties. The PovcalNet team had
the responsibility to approve on behalf or two of them, DECDG and DECRG. This
process can easily done with the code below, which can be found in this file,
`p:\01.PovcalNet\03.QA\02.PRIMUS\pending\2020_SM\approve\primus_approve.do`.

```{stata, eval = FALSE}
/*==================================================
0: Program set up
==================================================*/
version 14
drop _all

*---------- Modify this
local excl = "BRA SOM SSD" // countries to exclude 
local excl = "" // countries to exclude 

/*==================================================
Load data
==================================================*/

primus query, overalls(pending)
//------------Cut off date
local filtdate = "2019-12-01" // filter date (december last year)
local filtdate = "2020-02-18" // filter date (surveys uploaded by Minh)
keep if  date_modified >= clock("`filtdate'", "YMD")

//------------Select username
if (lower("`c(username)'") == "wb424681") {
  local dep = "povcalnet"
}
else if (lower("`c(username)'") == "wb384996") {
  local dep = "decdg"
}
else {
  noi disp in red "you don't have rights to run this code"
  break
}

tab `dep'
keep if `dep' == "PENDING"

if ("`excl'" != "") {
  local excl: subinstr local excl " " "|", all
  drop if regexm("`country'", "`excl'") 
}

/*=================================================
Approve (Do NOT modify)
==================================================*/

local n = _N
preserve 
qui foreach i of numlist 1/`n' {
  restore, preserve
  local country = country[`i']
  local year    = year[`i']
  local id      = transaction_id[`i']
  
  noi disp in y "primus action, tranxid(`id') decision(approved)"
  cap noi primus action, tranxid(`id') decision(approved)
  if (_rc) noi disp "problem with `id'"
}

```

Basically, this is what you need to do with this file.

1.  Modify `local excl` in case you do **not** want to approve one or several
    countries.
2.  Modify `local filtdate` in which you select the date from which you want to
    approve transactions.
3.  Make sure at least two people approve. One on behalf of "povcalnet" (which
    is the alias used for DECRG) and another on behalf of "decdg."
4.  PRIMUS has a double-confirmation process, so you need to "confirm" and then
    "approve" the transaction. For that, you only need to change the option
    `decision()` from `approved` to `confirmed`.

For some unknown reason, the PRIMUS system did not accept the approval of some
transactions. If this happens again, you need to talk to [Minh Cong
Nguyen](mailto:mnguyen3@worldbank.org), so he can do the approval manually.

<!--chapter:end:PRIMUS.Rmd-->

# (PART) Technical Considerations {.unnumbered}

# Load microdata and Auxiliary data {#load}

Make sure you have all the packages installed and loaded into memory. Given that they are hosted in Github, the code below makes sure that any package in the PIP workflow can be installed correctly. 

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

last_item <- function(x, word = "and") {
  if (!(is.character(x))) {
    warning("`x` must be character. coercing to character")
    x <- as.character(x)
  }

  lx <- length(x)
  if (lx == 1) {
    y <- x
  }
  else if (lx == 2) {
    y <- paste(x[1], word, x[2])
  }
  else {
    y <- c(x[1:lx-1], paste(word, x[lx]))
    y <- paste(y, collapse = ", ")
  }
  return(y)
}


```

```{r load}
## First specify the packages of interest
packages = c("pipaux", "pipload")

## Now load or install&load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      pck_name <- paste0("PIP-Technical-Team/", x)
      devtools::install_github(pck_name)
      library(x, character.only = TRUE)
    }
  }
)

```

## Auxiilary data

```{r numb-func, include = FALSE}
lf <- lsf.str("package:pipaux", pattern = "^pip_")
  lf <- as.character(lf)
  num_functions <- length(lf)
```
Even though `pipaux` has more than `r num_functions` functions, most of its features 
can be executed by using only the `pipaux::load_aux` and `pipaux::update_aux` functions. 


### udpate data
```{r functions-av, include = FALSE}
  lf <- lsf.str("package:pipaux", pattern = "^pip_[a-z]{3}$")
  lf <- as.character(lf)
  lf <- gsub("pip_", "", lf)
```

the main function of the `pipaux` package is `udpate_aux`. The first argument of this function is `measure` and it refers to the measure data to be loaded. The measures available are **`r last_item(lf)`**.
```{r update-aux}
pipaux::update_aux(measure = "cpi")
```

### Load data
Loading auxiliary data is the job of the package `pipload` through the function `pipload::pip_load_aux()`, though `pipaux` also provides `pipaux::load_aux()` for the same purpose. Notice that, though both function do exactly the same, the loading function from `pipload` has the prefix `pip_` to distinguish it from the one in `pipaux`. However, we are going to limit the work of `pipaux` to update auxiliary data and the work of `pipload` to load data. Thus, all the examples below use `pipload` for loading either microdata or auxiliary data. 

```{r load-aux}
df <- pipload::pip_load_aux(measure = "cpi")
head(df)
```


## Microdata 

Loading PIP microdata is the most practical action in the `pipload` package. However, it is important to understand the logic of microdata. 

PIP microdata has several characteristics, 

* There could be more than once survey for each Country/Year. This happens when there are more than one welfare variable available such as income and consumption. 
* Some countries, like Mexico, have the two different welfare types in the same survey for the same country/year. This add a layer of complexity when the objective is to known which is default one. 
* There are multiple version of the same harmonized survey. These version are organized in a two-type vintage control. It is possible to have a new version of the data because the Raw data--the one provided by the official NSO--has been updated, or because there has been un update in the harmonization process.
* Each survey could be use for more than one analytic tool in PIP (e.g., Poverty Calculator, Table Maker, or SOL). Thus, the data to be loaded depends on the tool in which it is going to be used. 

Thus, in order to make the process of finding and loading data efficiently, `pipload` is a three-step process.

### Inventory file
The inventory file resides in `y:/PIP-Data/_inventory/inventory.fst`. This file is a data frame with all the microdata available in the PIP structure. It has two main variables, `orig` and `filename`. The former refers to the full directory path of the database, whereas the latter is only the file name. the other variables in this data frame are derived from these two. 

The inventory file is used to speed up the file searching process in `pipload`. In previous packages, each time the user wanted to find a particular data base, it was necessary to look into the folder structure and extract the name of all the file that meet a particular criteria. This is time-consuming and inefficient. The advantage of this method though, is that, by construction, it finds all the the data available. By contrast, the inventory file method is much faster than the "searching" method, as it only requires to load a light file with all the data available, filter the data, and return the required information. The drawback, however, is that it needs to be kept up to date as data changes constantly. 

To update the inventory file, you need to use the function `pip_update_inventory`. If you don't provide any argument, it will update the whole inventory, which may take around 10 to 15 min--the function will warn you about it. By provide the country/ies you want to update, the process is way faster. 

```{r update-inventory}
# update one country
pip_update_inventory("MEX")

# Load inventory file
df <- pip_load_inventory()
head(df[, "filename"])

```


### Finding data 

Every dataset in the PIP microdata repository is identified by seven variables! Country code, survey year, survey acronym, master version, alternative version, tool, and source. So giving the user the responsibility to know all the different combinations of each file is a heavy burden. Thus, the data finder, `pip_find_data()`, will provide the names of all the files available that meet the criteria in the arguments provided by the user. For instance, if the use wants to know the all the file available for Paraguay, we could type, 

```{r inventory-pry}
pip_find_data(country = "PRY")[["filename"]]
```

Yet, if the user need to be more precise in its request, she can add information to the different arguments of the function. For example, this is data available in 2012, 

```{r inventory-pry2012}
pip_find_data(country = "PRY", 
              year = 2012)[["filename"]]
```

### Loading data

Function `pip_load_data` takes care of loading the data. The very first instruction within `pip_load_data` is to find the data avialable in the repository by using `pip_load_inventory()`. The difference however is two-fold. First, `pip_load_data` will load the default and/or most recent version of the country/year combination available. Second, it gives the user the possibility to load different datasets in either list or dataframe form. For instance, if the user wants to load the Paraguay data in 2014 and 2015 used in the Poverty Calculator tool, she may type, 

```{r load-data}

df <- pip_load_data(country = "PRY",
                    year    = c(2014, 2015), 
                    tool    = "PC")

janitor::tabyl(df, survey_id)
```




<!--chapter:end:load_md_aux.Rmd-->

# Poverty Calculator Pipeline (pre-computed estimations) {#pcpipeline}

The Poverty Calculator Pipeline--hereafter, and only for the rest of this
chapter, pipeline--is the technical procedure to calculate the pre-computed
estimations of the PIP project. These estimations have two main purposes:

1.  Provide the user with instantaneous information about distributive measures
    of all the household surveys in the PIP repository that do not depend on the
    value of the poverty line. Avoiding thus the need for re-computation as it
    was the case in PovcalNet for some of these measures.
2.  Provide the necessary inputs to the PIP API.

This chapter walks you through the folder structure of the folder, the main R
script, `_targets.R`, and the complete and partial execution of the script.
Also, it provides some tips for debugging.

## Folder structure

The pipeline is hosted in the Github repository
[PIP-Technical-Team/pip_ingestion_pipeline](https://github.com/PIP-Technical-Team/pip_ingestion_pipeline).
At the root of the repo you will find a series of files and folders.

```{r, eval=FALSE}
#> +-- batch
#> +-- pip_ingestion_pipeline.Rproj
#> +-- R
#> +-- README.md
#> +-- renv
#> +-- renv.lock
#> +-- run.R
#> +-- _packages.R
#> \-- _targets
#> \-- _targets.R
```

### Folders {.unnumbered}

-   `R` Contains long R functions used during the pipeline

-   `batch` Script for timing the execution of the pipeline. This folder should
    probably be removed

-   `_targets` Folder for all objects created during the pipeline. You don't
    need to look inside as it content is managed by the `targets` package.

-   `renv` Folder for reproducible environment.

### Files {.unnumbered}

-   `_packages.R` Created by `targets::tar_renv()`. Do not modify manually.

-   `_targets.R` Contains the pipeline. This is the most important file.

## Prerequisites

Before you start working on the pipeline, you need to make sure to have the
following PIP packages.

[Note:]{style="color:red"} Notice that directives below have suffixes like
`@development`, which specify the branch of the particular package that you need
to use. Ideally, the master branch of all packages should be used, but that will
only happen until the end of the development process.

[Note2:]{style="color:red"} If you update any of the packages developed by the
PIP team, make sure you always increased the version of the package using the
function `usethis::use_version()`. Even if the change in the package is small,
you need on increased the version of the package. Otherwise, `{targets}` won't
execute the sections of the pipeline that run the functions you changed.

```{r, eval=FALSE}
remotes::install_github("PIP-Technical-Team/pipdm@development")
remotes::install_github("PIP-Technical-Team/pipload@development")
remotes::install_github("PIP-Technical-Team/wbpip@halfmedian_spl")
install.packages("joyn")
```

In case `renv` is not working for you, you may need to install all the packages
mentioned in the `_packages.R` script at the root of the folder. Also, make sure
to install the most recent version of `targets` and `tarchetypes` packages.

## Structure of the `_targets.R` file

Even thought the pipeline script looks like a regular R Script, it is structured
in a specific way in order to make it work with
`{[targets](https://docs.ropensci.org/targets/)}` package, starting by the fact
that it must be called `_targets.R` in the root of the project. It highly
recommended you read the entire [targets
manual](https://books.ropensci.org/targets/) to fully understand how it works.
Also, during this chapter, we will referring to the manual constantly to expand
in any particular targets' concept.

### Start up {.unnumbered}

The first part of the pipeline sets up the environment. It,

1.  loads the `{targets}` and `{tarchetypes}` packages,

<!-- -->

2.  creates default values like directories, time stamps, survey and reference
    years boundaries, compression level of .fst files, among other things.
3.  executes `tar_option_set()` to set up some option in `{targets}`. Two
    particular options are important, `packages` and `imports` for tracking
    changes in package dependencies. You can read more about it in sections
    [Loading and configuring R
    packages](https://books.ropensci.org/targets/practices.html#loading-and-configuring-r-packages)
    and [Packages-based
    invalidation](https://books.ropensci.org/targets/practices.html#packages-based-invalidation)
    of the targets manual.
4.  Attach all the packages and functions of the project by running
    `source('_packages.R')` and `source('R/_common.R')`

### Step 1: small functions {.unnumbered}

According to the section [Functions in
pipelines](https://books.ropensci.org/targets/functions.html#functions-in-pipelines)
of the targets manual, it is recommend to only use functions rather than
expressions during the executions. Presumably, the reason for this is that
targets track changes in functions but not in expressions. Thus, this section of
the scripts defines small functions that are executed along the pipeline. In the
section above, the scripts `source('R/_common.R')` loads longer functions. Yet,
keep in mind that the `'R/_common.R'` was used in a previous version of the
pipeline before `{targets}` was implemented. Now, most of the function in
`'R/_common.R'` are included in the `{pipdm}` package.

### Step 2: prepare data {.unnumbered}

This section used to longer in previous versions of the pipeline because
identified the auxiliary data, loaded he PIP microdata inventory, and created
the cache files. Now, it only identifies the auxiliary data. Not much to be said
here.

### Step 3: The actual pipeline

This part of the pipeline is long and it is explained in detail in the next
section. Suffice is to say that the order of the pipeline is the following,

1.  Load all necessary data. That is, auxiliary data and inventories, and then
    create any cache fie that has not been created yet.

2.  Calculate means in LCU

3.  Crate deflated survey mean (DSM) table

4.  Calculate reference year table (aka., interpolated means table)

5.  Calculate distributional stats

6.  Create output tables

    1.  join survey mean table with dist table

    2.  join reference year table with dist table

    3.  coverage table aggregate population at the regional level table

7.  Clean and save.

## Understanding the pipeline

One thing is to understand the how the `{targets}` package works and something
else is to understand how the targets of the Poverty Calculator Pipeline are
created. For the former, you can read the targets manual. For the latter, we
should start by making a distinction between the different types of targets.

In `{targets}` terminology, there are two kinds of targets, **stems** and
**branches**. **Stems** are unitary targets. That is, for each target there is
only one single R object. **Branches**, on the other hand, are targets that
contain several objects or *subtargets* inside (You can learn more about them in
the chapter [Dynamic
branching](https://books.ropensci.org/targets/dynamic.html#dynamic) of the
targets manual). We will see the use of this type of targets when we talk about
the use of cache files.


### Stem targets {.unnumbered}

There are two ways to create **stem** targets: either using `tar_target()` or
using `tar_map()` from the `{tarchetypes}` package. The `tar_map()` function
allows to create **stem** targets iteratively. See for instance the creation of
targets for each auxiliary data,

```{r, eval=FALSE}
tar_map(
  values = aux_tb, 
  names  = "auxname", 
  
  # create dynamic name
  tar_target(
    aux_dir,
    auxfiles, 
    format = "file"
  ), 
  tar_target(
    aux,
    pipload::pip_load_aux(file_to_load = aux_dir,
                          apply_label = FALSE)
  )
  
)
```

`tar_map()` takes the values in the data frame `aux_tb` created in [Step 2:
prepare data](#pipe-prepare-data) and creates two type of targets. First, it
creates the target `aux_dir` that contains the paths of the auxiliary files,
which are available in the column `auxfiles` of `aux_tb`. This is done by
creating an internal target within `tar_map()` and using the argument
`format = "file"`. This process lets `{targets}` know that we will have objects
that are loaded from a file and are not created inside the pc pipeline.

Then, `tar_map()` uses the the column `auxname` of `aux_tb` to name the targets
that contain the auxiliary files. Each target will be prefixed by the word
"aux". This is why we had to add the argument `file_to_load` to
`pipload::pip_load_aux`, so we can let `{targets}` know that the file paths
defined in target `aux_dir` are used to create the targets prefixed with "aux",
which are the actual targets. For example, if I need to use the population data
frame inside the pc pipeline, I'd use the target `aux_pop`, which had a
corresponding file path in `aux_dir`. In this way, if the original file
referenced in `aux_dir` changes, all the targets that depend on `aux_pop` will
be run again.

### Branches targets {.unnumbered}

Let's think of a branch target like

As explained above, branch targets are targets made of many "subtargets" that
follow a particular pattern. the Most of the targets created in the pc pipeline
are **branch** targets because we need to execute the same procedure in every
cache file. This could have been done internally in one single, but then we
would lose the tracking features of `{targets}`. Also, we could have created a
**stem** target for every cache file, result, and output file, but that would
have been not only impossible to visualize, but also more difficult to code.
Thus branch targets is the best option.

The following example illustrates how it works,

```{r, eval=FALSE}

# step A
tar_target(
  cache_inventory_dir, 
  cache_inventory_path(),
  format = "file"
),

# step B
tar_target(
  cache_inventory, 
  {
    x <- fst::read_fst(cache_inventory_dir, 
                       as.data.table = TRUE)
  },
),

# step C
tar_target(cache_files,
           get_cache_files(cache_inventory)),

# step D
tar_files(cache_dir, cache_files),

# step E
tar_target(cache, 
           fst::read_fst(path = cache_dir, 
                         as.data.table = TRUE), 
           pattern = map(cache_dir), 
           iteration = "list")
```

The code above illustrates several things. It is divided in steps, being the
last step-- step E--the part of the code in which we create the **branch**
target. Yet, it is important to understand the steps before.

In step A we create target `cache_inventory_dir`, which is merely the path of
the file than contains the cache inventory. Notice that it is returned by a
function and not entered directly into the target. Since it is a file path, we
need to add the argument `format = "file"` to let `{targets}` know that it is
input data. In step B we load the cache inventory file into target
`cache_inventory` by providing the target "path" that we created in step A. This
file has several column. One of them contains the file path of every single
cache file in the PIP network drive. That single column is extracted from the
cache inventory in step C. Now, in step D, each file path is declared as input,
using the convenient function `tar_files()`, creating thus a new target,
`cache_dir`. FInally, we create **branch** target `cache` with all the cache
files by loading each file. To do this iteratively, we parse the `cache_dir`
target to the `path` argument of the function `fst::read_fst()` and to the
`pattern = map()` argument of the `tar_target()` function. Finally, we need to
specify that the output of the iteration is stored as a list, using the argument
`iteration = "list"`.

The basic logic of **branch** targets is that the vector or list to iterate
through should be parsed to the argument of the function and to the
`pattern = map()` argument of the `tar_target()` function. it is very similar to
`purrr::map()`

[Note:]{style="color:red"} If we are iterating through *more than one* vector or
list, you need to (1) separate each of them by commas in the `map()` part of the
argument (See example code below). (2) make sure all the vectors or lists **have
the same length**. This is why we cannot remove NULL or NA values from any
target. (3) make sure you do **NOT** sort any of the output targets as it will
loose its correspondence with other targets.

```{r, eval=FALSE}

# Example of creating branch target using several lists to iterate through.
tar_target(
  name      = dl_dist_stats,
  command   = db_compute_dist_stats(dt       = cache, 
                                    mean     = dl_mean, 
                                    pop      = aux_pop, 
                                    cache_id = cache_ids), 
  pattern   =  map(cache, dl_mean, cache_ids), 
  iteration = "list"
)
```

### Creating the cache files {.unnumbered}

The creation of the cache files is done in the following code,

```{r, eval=FALSE}
tar_target(pipeline_inventory, {
  x <- pipdm::db_filter_inventory(
    dt = pip_inventory,
    pfw_table = aux_pfw)
  
  # Uncomment for specific countries
  # x <- x[country_code == 'IDN' & surveyid_year == 2015]
}
),
tar_target(status_cache_files_creation, 
           pipdm::create_cache_file(
             pipeline_inventory = pipeline_inventory,
             pip_data_dir       = PIP_DATA_DIR,
             tool               = "PC",
             cache_svy_dir      = CACHE_SVY_DIR,
             compress           = FST_COMP_LVL,
             force              = TRUE,
             verbose            = FALSE,
             cpi_dt             = aux_cpi,
             ppp_dt             = aux_ppp)
)
```

It is important to understand this part of the pc pipeline thoroughly because
the cache files used to be created in [Step 2: prepare data](#pipe-prepare-data)
rather than here. Now, it has not only been integrated in the pc pipeline, but
it is also possible to execute the creation of cache files independently from
the rest of the pipeline, by following the instructions in [Executing the
\_targets.R file].

The first target, `pipeline_inventory` is just the inner join of the pip
inventory dataset and the price framework (pfw ) file to make sure we only
include what the **pfw** says. This data set also contains a lot of information
useful for creating the cache files. Notice that the commented line in this
target would filter the pipeline inventory to have only the information for IDN,
2015. [In case you need to update specific cache files, you have to do add the
proper filtering condition in there]{style="color:red"}.

In the second target, `status_cache_files_creation`, you will create the cache
files but notice that the returning value of the function
`pipdm::create_cache_file()` is not the cache file per-se but a list with the
status of the process of creation. If the creation of a particular file fails,
it does not stop the iteration that creates the all cache files. At the end of
the process, it returns a list with the creation status of each cache file.
Notice that function `pipdm::create_cache_file()` requires the CPI and the PPP
auxiliary data. This is so because the variable `welfare_ppp`, which is the
welfare aggregate in 2011 PPP values, is added to the cache files. FInally, and
more importantly, argument `force = TRUE` ensures that even if the cache file
exists already, it should be modified. This is important when you require
additional features in the cache file from the then ones it has now. If set to
`TRUE`, it will replace any file in the network drive that is listed in
`pipeline_inventory`. If set to `FALSE`, only the files that are in
`pipeline_inventory` but not in the cache folder will be created. Use this
option only when you need to add new features to all cache data or when you are
testing and only need a few surveys with the new features.

## Understanding `{pipdm}` functions

The `{pipdm}` package is the backbone of the pc pipeline. It is in charge of
executing the functions in `{wbpip}` and consolidate the new DataBases. This is
why, many of the functions in `{pipdm}` are prefixed with "db\_".

### Internal structure of `{pipdm}` functions

The main objective of `{pipdm}` is to execute the functions in `{wbpip}` to do
the calculations and then build the data frames. As of today, `r Sys.Date()`,
the process is a little intricate.

Let's take the example of estimating distributive measures in the pipeline. The
image [below](#pipdm-structure) shows that there are at least three intermediate
function levels between the `db_compute_dist_stats()` function, which executed
directly in the pc pipeline, and the `wbpip::md_compute_dist_stats()`, which
makes the calculations. Also, notice that the functions are very general in
regards to the output. No higher level function is specific enough to retrieve
only one measure such as the Gini coefficient, or the median, or the quantiles
of the distribution. If you need to add or modify one particular distributive
measure, you must do it in functions inside `wbpip::md_compute_dist_stats()`,
making sure the new output does not mess up the execution of any of the
intermediate functions before the results get to `db_compute_dist_stats()`.

![](img/pipdm_structure.png){#pipdm-structure}

This long chain of functions is inflexible and makes debugging very difficult.
So, if you need to make any modification, identify first the chain of execution
in each `pipdm` function you modify and then make sure your changes do not
affect the output format as it may break the chain of execution. Also, this is a
good example to show why this structure needs to be improved.

### Updating `{pipdm}` (or any other PIP package)

As explained above, if you need to modify any function in `pipdm` or in `wbpip`
you need to make sure that the output does not conflict with the chain of
execution. Additionally, If you update any of the packages developed by the PIP
team, make sure you always increased the version of the package using the
function `usethis::use_version()`. Even if the change in the package is small,
you need to increase the version of the package. Otherwise, `{targets}` won't
execute the sections of the pipeline that run the functions you changed. Finally
as explained in the [Prerequisites], if you are working on a branch different
than master, make sure you install that version of the package before running
the pipeline.

## Executing the `_targets.R` file

The `.Rprofile` in the root of the directory makes sure that both `{targets}`
and `{tarchetypes}` are loaded when the project is started. The execution of the
whole pipeline might be very time consuming because it still needs to load all
the data in the network drive. If you use a desktop remote connection the
execution might be faster than running locally, but it is still very time
consuming. So, my advise is that you only execute the targets that are directly
affected by your changes and manually check that everything looks ok. After
that, you can execute the whole thing confidently and leave it running
overnight.

In order to execute the whole pipeline, you only need to type the directive
`tar_make()` in the console. If you want to execute only one target, then type
the name of the target in the same directive, e.g., `tar_make(dl_dist_stats)`.
Keep in mind that if the inputs of prior targets to the objective target have
changed, those targets will be executed first.

## Debugging

Debugging in targets is not easy. Yet, there are two ways to do it. The first
way is provided in the chapter
[Debugging](https://books.ropensci.org/targets/debugging.html) of the Targets
Manual. It provides clear instruction on how to debug *while still being in the
pipeline*, but it could be the case, as it happened to me, that you don't find
this method flexible enough to dig deep enough into the problem. Alternatively,
you could debug by stepping out of the pipeline a little bit and gain more
flexibility. This is how I do it.

You need to debug in one of two case. One, because you got an error when running
the pipeline with `tar_make()` or, two, because your results are weird. In
either case, you should probably have an idea--though not always--of where the
problem is. If the problem is an error in the execution of the pipeline,
`{targets}` printed messages are usually informative.

### Debugging stem targets {.unnumbered}

Let's see a simple example. Assume the problem is in the target `dt_dist_stats`,
which is created by executing the function `db_create_dist_table` of the
`{pipdm}` package. Since the problem is in there, all the targets and inputs
necessary to create `dt_dist_stats` should be available in the `_targets/` data
store. So, you can load them using `tar_load()` and execute the function in
debugging mode. Like this,

```{r, eval=FALSE}

tar_load(dl_dist_stats)
tar_load(svy_mean_ppp_table)
tar_load(cache_inventory)

debugonce(pipdm::db_create_dist_table)
pipdm::db_create_dist_table(
  dl        = dl_dist_stats,
  dsm_table = svy_mean_ppp_table, 
  crr_inv   = cache_inventory
  )
```

Notice that you need to use the `::` because the environment in which
`{targets}` runs is different from your Global environment in which you might
not have attached all the libraries.

### Debugging branch targets {.unnumbered}

The problem debugging branch targets is that if the problem is in a specific
survey, you can't access the "subtarget" using the survey ID, or something like
that, because the name of the subtarget is created by `{targets}` using a random
number. This requires a little more of work.

Imagine now that the distributive measures of IDN 2015 are wrong. You see the
pipeline and notice that these calculation are executed in target
`dl_dist_stats`, which is branch target created over **all the cache files!** It
looks something like this,

```{r, eval=FALSE}
tar_target(
  name      = dl_dist_stats,
  command   = db_compute_dist_stats(dt       = cache, 
                                    mean     = dl_mean, 
                                    pop      = aux_pop, 
                                    cache_id = cache_ids), 
  pattern   =  map(cache, dl_mean, cache_ids), 
  iteration = "list"
)
```

In order to find the problem in IDN 2015, this what you could do,

```{r, eval=FALSE}
# Load data
dt <- pipload::pip_load_cache("IDN", 2015, "PC")
tar_load(dl_mean)
tar_load(cache_ids)
tar_load(aux_pop)

# Extract corresponding mean and cache ID
idt      <- which(cache_ids == unique(dt$cache_id))
cache_id <- cache_ids[idt]
mean_i   <- dl_mean[[idt]]

# Esecute the function of interest
debugonce(pipdm:::compute_dist_stats)
ds <- pipdm::db_compute_dist_stats(dt       = dt, 
                                   mean     = mean_i, 
                                   pop      = aux_pop, 
                                   cache_id = cache_id)
```

First, you load all the inputs. Since target `dl_mean` is a relatively light
object, we load it directly from the `_targets/` data store. Targets `cache_ids`
and `aux_pop` are data frames, not lists, so we also load them from memory. The
microdata, however, is problematic because target `cache`, which is the one that
is parsed to create the **actual** `dl_dist_stata` target, is a huge list with
all the micro, grouped, and imputed data. The solution is then to load the data
frame of interest, using either `pipload` or `fst`.

Secondly, we need to filter the list `dl_mean` and the data frame `cache_ids` to
parse only the information accepted by `pipdm::db_compute_dist_stats()`
function. This has to be done when debugging because in the actual target this
is done iteratively in `pattern   =  map(cache, dl_mean, cache_ids)`.

Finally, you execute the function of interest. Notice something else. The target
`aux_pop` is parsed as a single data frame because
`pipdm::db_compute_dist_stats()` requires it that way.
[Note:]{style="color:red"} This is also one of the reasons these functions in
`{pipdm}` need some fixing and consistency in the format of the their inputs.

<!--chapter:end:pc_pipeline.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:references.Rmd-->

